<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>4&nbsp; Magnitude tabular data – Handbook on Statistical Disclosure Control</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./05-frequency-tables.html" rel="next">
<link href="./03-microdata.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating slimcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./04-magnitude-tabular-data.html"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Magnitude tabular data</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Handbook on Statistical Disclosure Control</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/sdcTools/HandbookSDC" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <a href="./Handbook-on-Statistical-Disclosure-Control.pdf" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface to the second edition</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01-Introduction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02-regulations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Confidentiality in legal acts and ethical codes</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03-microdata.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Microdata</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04-magnitude-tabular-data.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Magnitude tabular data</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05-frequency-tables.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Frequency tables</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06-remote-access-issues.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Remote access issues</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./07-glossary.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Glossary</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#sec-tabular-introduction" id="toc-sec-tabular-introduction" class="nav-link active" data-scroll-target="#sec-tabular-introduction"><span class="header-section-number">4.1</span> Introduction</a></li>
  <li><a href="#sec-disclosure-control-concepts-tables" id="toc-sec-disclosure-control-concepts-tables" class="nav-link" data-scroll-target="#sec-disclosure-control-concepts-tables"><span class="header-section-number">4.2</span> Disclosure Control Concepts for Magnitude Tabular Data</a>
  <ul class="collapse">
  <li><a href="#sec-sensitive-cells-magnitude-tables" id="toc-sec-sensitive-cells-magnitude-tables" class="nav-link" data-scroll-target="#sec-sensitive-cells-magnitude-tables"><span class="header-section-number">4.2.1</span> Sensitive Cells in Magnitude Tables</a></li>
  <li><a href="#sec-secondary-protection-methods" id="toc-sec-secondary-protection-methods" class="nav-link" data-scroll-target="#sec-secondary-protection-methods"><span class="header-section-number">4.2.2</span> Secondary tabular data protection methods</a></li>
  </ul></li>
  <li><a href="#sec-tau-argus-suppression" id="toc-sec-tau-argus-suppression" class="nav-link" data-scroll-target="#sec-tau-argus-suppression"><span class="header-section-number">4.3</span> The <span class="math inline">\(\tau\)</span>‑ARGUS Implementation of Cell Suppression</a>
  <ul class="collapse">
  <li><a href="#sec-setting-up-a-tabular-data-protection-problem-in-practice" id="toc-sec-setting-up-a-tabular-data-protection-problem-in-practice" class="nav-link" data-scroll-target="#sec-setting-up-a-tabular-data-protection-problem-in-practice"><span class="header-section-number">4.3.1</span> Setting up a Tabular Data Protection Problem in Practice</a></li>
  <li><a href="#sec-evaluation-of-secondary-cell-suppression-algorithms-offered-by-tau-argus" id="toc-sec-evaluation-of-secondary-cell-suppression-algorithms-offered-by-tau-argus" class="nav-link" data-scroll-target="#sec-evaluation-of-secondary-cell-suppression-algorithms-offered-by-tau-argus"><span class="header-section-number">4.3.2</span> Evaluation of Secondary Cell Suppression algorithms offered by <span class="math inline">\(\tau\)</span>‑ARGUS</a></li>
  <li><a href="#sec-efficient-protection-process" id="toc-sec-efficient-protection-process" class="nav-link" data-scroll-target="#sec-efficient-protection-process"><span class="header-section-number">4.3.3</span> Processing table protection efficiently</a></li>
  <li><a href="#sec-introductive-example" id="toc-sec-introductive-example" class="nav-link" data-scroll-target="#sec-introductive-example"><span class="header-section-number">4.3.4</span> Introductive Example</a></li>
  </ul></li>
  <li><a href="#sec-methodology-secondary-suppression-tau-argus" id="toc-sec-methodology-secondary-suppression-tau-argus" class="nav-link" data-scroll-target="#sec-methodology-secondary-suppression-tau-argus"><span class="header-section-number">4.4</span> Methodological concepts of secondary cell suppression algorithms in <span class="math inline">\(\tau\)</span>‑ARGUS</a>
  <ul class="collapse">
  <li><a href="#sec-optimal" id="toc-sec-optimal" class="nav-link" data-scroll-target="#sec-optimal"><span class="header-section-number">4.4.1</span> Optimal</a></li>
  <li><a href="#sec-modular" id="toc-sec-modular" class="nav-link" data-scroll-target="#sec-modular"><span class="header-section-number">4.4.2</span> Modular</a></li>
  <li><a href="#sec-network" id="toc-sec-network" class="nav-link" data-scroll-target="#sec-network"><span class="header-section-number">4.4.3</span> Network</a></li>
  <li><a href="#sec-hypercube" id="toc-sec-hypercube" class="nav-link" data-scroll-target="#sec-hypercube"><span class="header-section-number">4.4.4</span> Hypercube</a></li>
  </ul></li>
  <li><a href="#sec-CTA" id="toc-sec-CTA" class="nav-link" data-scroll-target="#sec-CTA"><span class="header-section-number">4.5</span> Controlled Tabular Adjustment</a></li>
  <li><a href="#sec-CKM_mag" id="toc-sec-CKM_mag" class="nav-link" data-scroll-target="#sec-CKM_mag"><span class="header-section-number">4.6</span> Cell Key Method for Magnitude Tables</a></li>
  <li><a href="#measurement-of-disclosure-risk-and-information-loss" id="toc-measurement-of-disclosure-risk-and-information-loss" class="nav-link" data-scroll-target="#measurement-of-disclosure-risk-and-information-loss"><span class="header-section-number">4.7</span> Measurement of disclosure risk and information loss</a>
  <ul class="collapse">
  <li><a href="#disclosure-risk" id="toc-disclosure-risk" class="nav-link" data-scroll-target="#disclosure-risk"><span class="header-section-number">4.7.1</span> Disclosure risk</a></li>
  <li><a href="#information-loss" id="toc-information-loss" class="nav-link" data-scroll-target="#information-loss"><span class="header-section-number">4.7.2</span> Information loss</a></li>
  </ul></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references"><span class="header-section-number">4.8</span> References</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/sdcTools/HandbookSDC/issues" class="toc-action"><i class="bi bi-github"></i>Report an issue</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Magnitude tabular data</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="sec-tabular-introduction" class="level2" data-number="4.1">
<h2 data-number="4.1" class="anchored" data-anchor-id="sec-tabular-introduction"><span class="header-section-number">4.1</span> Introduction</h2>
<p>Statistical magnitude tables display sums of observations of a quantitative variable where each sum relates to a group of observations defined by categorical variables observed for a set of respondents.</p>
<p>Respondents are typically companies but can also be individuals or households, etc. Grouping variables typically give information on geography or economic activity or size, etc. of the respondents. The <em>“cells”</em> of a table are defined by cross-combinations of the grouping variables.</p>
<p>Each <em>“table cell”</em> presents a sum of a quantitative variable such as income, turnover, expenditure, sales, number of employees, number of animals owned by farms, etc. These sums are the <em>“cell values”</em> (sometimes also referred to as <em>“cell totals”</em>) of a magnitude table. The individual observations of the variable (for each individual respondent) are the <em>“contributions”</em> to the cell value.</p>
<div id="tbl-example-intro-4" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<div aria-describedby="tbl-example-intro-4-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="table">
<colgroup>
<col style="width: 20%">
<col style="width: 25%">
<col style="width: 25%">
<col style="width: 8%">
<col style="width: 12%">
</colgroup>
<tbody>
<tr class="odd">
<td></td>
<td style="text-align: right;">Industry A</td>
<td style="text-align: right;">Industry B</td>
<td>…</td>
<td style="text-align: right;">Total</td>
</tr>
<tr class="even">
<td>Region 1</td>
<td style="text-align: right;">540 (12)</td>
<td style="text-align: right;">231 (15)</td>
<td>…</td>
<td style="text-align: right;"></td>
</tr>
<tr class="odd">
<td>Region 2</td>
<td style="text-align: right;">48 (2)</td>
<td style="text-align: right;">125 (8)</td>
<td>…</td>
<td style="text-align: right;"></td>
</tr>
<tr class="even">
<td>…</td>
<td style="text-align: right;">…</td>
<td style="text-align: right;">…</td>
<td>…</td>
<td style="text-align: right;"></td>
</tr>
<tr class="odd">
<td>Total</td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
<td></td>
<td style="text-align: right;"></td>
</tr>
</tbody>
</table>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-tbl" id="tbl-example-intro-4-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;4.1: Example: Turnover (number of respondents) by Region and Industry
</figcaption>
</figure>
</div>
<p>The <em>“dimension”</em> of a table is given by the number of grouping variables used to specify the table. We say that a table contains <em>“margins”</em> or <em>“marginal cells”</em>, if not all cells of a table are specified by the same number of grouping variables. The smaller the number of grouping variables, the higher the <em>“level”</em> of a marginal cell. A two-dimensional table of some business survey may for instance provide sums of observations grouped by economic activity and company size classes. At the same time it may also display the sums of observations grouped by only economic activity or by only size classes. These are then margins/marginal cells of this table. If a sum across all observations is provided, we refer to it as the <em>“total”</em> or <em>“overall total”</em>.</p>
<p>At first sight, one might find it difficult to understand how the kind of summary information published in magnitude tables presents a disclosure risk at all. However, it often occurs that cells of a table relate to a single or to only a few respondents. The number of this kind of small cells in a table will increase, the more grouping variables are used to specify the table, the higher the amount of detail provided by the grouping variables, and the more uneven the distributions of respondents over the categories of the grouping variables.</p>
<p>If a table cell relates to a small group of respondents (or even only one), then publication of the cell value may imply a disclosure risk. This is the case if these respondents could be identified by an intruder using information displayed in the table.</p>
<div class="callout callout-style-simple callout-note">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-body-container">
<p><strong>Example 1</strong> Let a table cell display the turnover of companies in the mining sector for a particular region X. Let us assume that company A is the only mining company in this region. This is a fact that will be known to certain intruders (think, for instance, of another mining company B in a neighbouring region Y). So, if that table cell is published, company B would be able to disclose the turnover of company A.</p>
</div>
</div>
</div>
<p>In order to establish if a disclosure risk is connected to the publication of a cell value in a table, and in order to protect against this risk, data providers (like, <em>e.g.</em> National Statistical Institutes) should apply tabular data protection methods. In many countries this is a legal obligation to official statistical agencies. It may also be regarded as a necessary requirement in order to maintain the trust of respondents: After all, if in the instance above company A realizes that company B might, by looking into the published table, disclose the value of turnover it has reported, and if it considers this value as confidential information, it may refuse to respond to that survey in the next period, or (if the survey is compulsory) it may choose to provide incorrect or inaccurate information.</p>
<p>Especially for business statistics, the most popular method for tabular data protection is <em>cell suppression</em> . In tables protected by cell suppression, all values of cells for which a disclosure risk has been established are eliminated from the publication. Alternatively, other methods based on cell perturbation etc. may also be used to protect tabular data. While we focus in this chapter on cell suppression, we will also mention alternatives.</p>
<p><a href="#sec-disclosure-control-concepts-tables" class="quarto-xref"><span>Section 4.2</span></a> introduces into the methodological concepts of tabular data protection. In <a href="#sec-sensitive-cells-magnitude-tables" class="quarto-xref"><span>Section 4.2.1</span></a> we present the most common methods for disclosure risk assessment for each individual cell of a table (and for special combinations of individual cells). These methods are called <em>“primary”</em> disclosure control methods. Table cells for which a disclosure risk has been established are called <em>“sensitive”</em>, <em>“confidential”</em>, or <em>“unsafe”</em> cells. Primary disclosure risk is usually assessed by applying certain sensitivity rules. <a href="#sec-sensitive-cells-magnitude-tables" class="quarto-xref"><span>Section 4.2.1</span></a> explains the concept of sensitivity rules and the most common rules.</p>
<p>While detailed tabulated summary information also on smaller groups of statistical objects (companies by subgroups of subsectors at the district level, households by size and income at the neighbourhood level, etc.) might be of interest to certain user groups, it is also a responsibility (maybe the most important one) of official statistics to provide summary information at a high aggregate level by producing summary statistics on large groups of a population (<em>e.g.</em> for <em>all</em> companies of an economy sector). Because of this, it is not enough to have methodologies to protect individual cells. It implies a need for the so-called <em>“secondary”</em> tabular data protection methodologies.</p>
<p>Assume that a table displays the sum of a variable “Production” by three subsectors of an economy sector. Assume that this sum is sensitive for one of the subsectors and that the table is protected by cell suppression, meaning that the confidential cell value is suppressed.</p>
<div class="callout callout-style-simple callout-note">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-body-container">
<p><strong>Example 1a</strong> Production (in mill. Euro)</p>
<table class="table">
<colgroup>
<col style="width: 18%">
<col style="width: 25%">
<col style="width: 25%">
<col style="width: 25%">
</colgroup>
<tbody>
<tr class="odd">
<td>Sector</td>
<td>Subsector I</td>
<td>Subsector II</td>
<td>Subsector III</td>
</tr>
<tr class="even">
<td>56,600</td>
<td>suppressed<br>
(sensitive)</td>
<td>47,600<br>
(non-sensitive)</td>
<td>8,002<br>
(non-sensitive)</td>
</tr>
</tbody>
</table>
<p>With respect to the total production for this sector we distinguish two cases: Either it is foreseen to be published – we then consider it as a cell of the table (<em>e.g.</em> the “total”). If the cell values of the two non-sensitive subsectors and the “total” are displayed, then users of the publication can disclose the cell value for the sensitive subsector by taking the difference between the “total” and the subsector values for the two non-sensitive sectors (56,600 – 47,600 – 8,002 = 998). In order to avoid this, a secondary protection measure for this table has to be taken, <em>e.g.</em> selecting one of the two non-sensitive subsector cells and suppressing it as well. This would be called a “secondary suppression”.</p>
<p>The other option is that the “total” is not foreseen to be displayed / published. Then no secondary protection measure would be needed. In this instance, because the production for one subsector is suppressed, interested users of the table cannot compute the production for the sector on their own – and so the sector-level information is completely lost!</p>
</div>
</div>
</div>
<p>From a general perspective, the purpose of secondary tabular data protection methodologies is chiefly to avoid undesirable effects such as this, ensuring that – while some “small”, primary confidential cells within detailed tables may have to be protected (by cell suppression or by some perturbative method) – sums for larger groups, <em>i.e.</em> the margins of those detailed tables, are preserved to some extent. For cell suppression this means that suppression of marginal cells should be avoided as far as possible. For perturbative methods it means that high level margins should try to be preserved exactly (more or less).</p>
<p>Considering this as the basic idea of secondary protection, after <a href="#sec-sensitive-cells-magnitude-tables" class="quarto-xref"><span>Section 4.2.1</span></a> we assume for the remainder of the chapter margins and overall totals always to be part of a table.</p>
<p>In <a href="#sec-secondary-protection-methods" class="quarto-xref"><span>Section 4.2.2</span></a> we introduce the concepts of secondary tabular data protection methodologies. The focus will be on cell suppression, but we will also mention other methodologies.</p>
<p>The software package <span class="math inline">\(\tau\)</span>‑ARGUS (see <em>e.g.</em> Hundepool et al., 2014) provides software tools for disclosure protection methods for tabular data. The latest version of <span class="math inline">\(\tau\)</span>-ARGUS can be found on GitHub (<a href="https://github.com/sdcTools/tauargus/releases">https://github.com/sdcTools/tauargus/releases</a>). <a href="#sec-tau-argus-suppression" class="quarto-xref"><span>Section 4.3</span></a> is concerned with the practical implementation of secondary cell suppression as offered by <span class="math inline">\(\tau\)</span>‑ARGUS. In <a href="#sec-setting-up-a-tabular-data-protection-problem-in-practice" class="quarto-xref"><span>Section 4.3.1</span></a> we discuss information loss concepts as well as table structures considered by <span class="math inline">\(\tau\)</span>‑ARGUS. We compare the performance of different algorithms for secondary cell suppression in <a href="#sec-evaluation-of-secondary-cell-suppression-algorithms-offered-by-tau-argus" class="quarto-xref"><span>Section 4.3.2</span></a> and give software recommendations. In <a href="#sec-efficient-protection-process" class="quarto-xref"><span>Section 4.3.3</span></a> we explain how to set up procedures for tabular data protection in a practical way and give an introductive example in <a href="#sec-introductive-example" class="quarto-xref"><span>Section 4.3.4</span></a>. In <a href="#sec-methodology-secondary-suppression-tau-argus" class="quarto-xref"><span>Section 4.4</span></a> we briefly introduce the methodological concepts of the secondary cell suppression algorithms provided by <span class="math inline">\(\tau\)</span>-ARGUS. The chapter ends with <a href="#sec-CTA" class="quarto-xref"><span>Section 4.5</span></a>, introducing Controlled Tabular Adjustment as new emerging protection technique for magnitude tables which could become an alternative to the well-established cell suppression methodologies.</p>
</section>
<section id="sec-disclosure-control-concepts-tables" class="level2 page-columns page-full" data-number="4.2">
<h2 data-number="4.2" class="anchored" data-anchor-id="sec-disclosure-control-concepts-tables"><span class="header-section-number">4.2</span> Disclosure Control Concepts for Magnitude Tabular Data</h2>
<p>In this section we explain the main concepts for assessment and control of disclosure risk for magnitude tables.</p>
<p><a href="#sec-sensitive-cells-magnitude-tables" class="quarto-xref"><span>Section 4.2.1</span></a> is concerned with disclosure risk for each individual cell of tables presenting summaries of quantitative variables and will introduce the most common methods used to assess this risk.</p>
<p>In order to preserve the margins of tables to some extent while protecting individual sensitive cells, special disclosure control methodologies have been developed. <a href="#sec-secondary-protection-methods" class="quarto-xref"><span>Section 4.2.2</span></a> presents basic concepts of these methodologies, focusing on secondary cell suppression as the most prominent instance. We finish section methods with a brief, comparative overview of alternative methods for tabular data protection.</p>
<section id="sec-sensitive-cells-magnitude-tables" class="level3 page-columns page-full" data-number="4.2.1">
<h3 data-number="4.2.1" class="anchored" data-anchor-id="sec-sensitive-cells-magnitude-tables"><span class="header-section-number">4.2.1</span> Sensitive Cells in Magnitude Tables</h3>
<p>We begin this section by describing intruder scenarios typically considered by statistical agencies in the context of disclosure control for magnitude tables. Considering these intruder scenarios statistical agencies have developed some <em>‘safety rules’</em> as measures to assess disclosure risks. This section will also introduce the most popular rules using some illustrative examples. Finally, we compare rules and give guidance on making a decision between alternative rules.</p>
<p><br>
<strong><em>Intruder scenarios</em></strong><br>
If a table cell relates to a small group (or even only one) respondent, then publication of the cell value may imply a disclosure risk. This is the case, if these respondents could be identified by an intruder using information displayed in the table. In example 1 of <a href="#sec-tabular-introduction" class="quarto-xref"><span>Section 4.1</span></a>, for the intruder (company B) it is enough to know that the cell value reports the turnover of mining companies in region X. In that example company B is assumed to be able to identify company A as the only mining company in region X. Hence, publication of the cell value implies a disclosure risk: if company B looks into the publication they will be able to disclose the turnover of company A.</p>
<p>But what if a cell value does not relate to one, but to two respondents?</p>
<div class="callout callout-style-simple callout-note">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-body-container">
<p><strong>Example 1b</strong> Let us assume this time that both companies (A and B) are located in region X, and are the only mining companies there. Let us further assume that they both are aware of this fact. Then again publication of the cell value implies a disclosure risk (this time to both companies): if any of the two companies look into the publication and subtract their own contribution to the cell value (<em>i.e.</em> the turnover they reported) from the cell value, they will be able to disclose the turnover of the other company.</p>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-note">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-body-container">
<p><strong>Example 1c</strong> Assume now that the table cell relates to more than two respondents. Imagine this time that four companies (A, B, C and D) are located in region X. Then theoretically three of them (B, C and D, say) could form a <em>coalition</em> to disclose the turnover of company A. Such a coalition might be a rather theoretical construct. An equivalent but perhaps more likely scenario could be that of another party who knows the contributions of companies B, C and D (perhaps a financial advisor working for all three companies) who would then be able to disclose also the turnover of company A by subtracting the contributions of B, C and D from the cell value.</p>
</div>
</div>
</div>
<p>The examples above are based on the intruder scenario typical for business data: it is usually assumed, that the “intruders”, those who might be interested in disclosing individual respondent data, may be “other players in the field”, <em>e.g.</em> competitors of the respondent or other parties who are generally well informed on the situation in the part of the economy to which the particular cell relates. Such intruder scenarios make sense, because, unlike microdata files for researchers, tabular data released by official statistics are accessible to everybody – which means they are accessible in particular to those well informed parties.</p>
<p>In the scenarios of example 1 there is a risk that magnitude information is disclosed exactly. But how about approximate disclosure?</p>
<div class="callout callout-style-simple callout-note">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-body-container">
<p><strong>Example 1d</strong> Let us reconsider the example once more. Assume this time that in region X there are 51 companies that belong to the mining sector, <em>e.g.</em> company A and 50 very small companies S<sub>1</sub> to S<sub>50</sub>. Assume further that 99&nbsp;% of the turnover in mining in region X is contributed by company A. In that scenario, the cell value (turnover in the mining sector for region X) is a pretty close approximation of the turnover of company A. And even though the potential intruder (in our example mining company B of the neighbour region Y) may not be able to identify all 51 mining companies of region X, it is very likely that they will know that there is one very big company in region X and which company that is.</p>
</div>
</div>
</div>
<p><br>
<strong><em>Sensitivity of variables</em></strong><br>
The presumption of the sensitivity of a variable often matters in the choice of a particular protection method. For example, especially in the case of tables presenting business magnitude information many agencies decide that this kind of information must be protected also against the kind of approximate disclosure illustrated by example 1d above, because it is so sensitive.</p>
<p>Considering the above explained intruder scenarios statistical agencies have developed some <em>‘safety rules’</em> (also referred to as <em>‘sensitivity rules’</em> or <em>‘sensitivity measures’</em>), measures to assess disclosure risks. We will now introduce the most popular rules, starting with an overview presenting formal representation of these rules in <a href="#tbl-sensitivity-rules" class="quarto-xref">Table&nbsp;<span>4.2</span></a>. After that, the rules (or rather, classes of rules) will be discussed in detail. We explain in which situations it may make sense to use those rules, using simple examples for illustration where necessary.</p>
<p><br>
<strong><em>Sensitivity rules</em></strong><br>
<a href="#tbl-sensitivity-rules" class="quarto-xref">Table&nbsp;<span>4.2</span></a> briefly presents the most common sensitivity rules. Throughout this chapter we denote <span class="math inline">\(x_{1} \geq x_{2} \geq \cdots \geq x_{N}\)</span> the ordered contributions by respondents <span class="math inline">\(1,2,\ldots,N\)</span>, respectively, to a cell with cell total (or cell value) <span class="math inline">\(X = \sum_{i=1}^N x_{i}\)</span></p>
<div id="tbl-sensitivity-rules" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-tbl figure page-columns page-full">
<div aria-describedby="tbl-sensitivity-rules-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca" class="page-columns page-full">
<table class="table">
<colgroup>
<col style="width: 14%">
<col style="width: 84%">
</colgroup>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>Rule</strong></td>
<td style="text-align: left;"><strong>Definition</strong></td>
</tr>
<tr class="even">
<td style="text-align: left;"></td>
<td style="text-align: left;"><strong>A cell is considered <em>unsafe</em>, when …</strong></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Minimum frequency rule</strong></td>
<td style="text-align: left;">the cell frequency is less than a pre-specified minimum frequency <span class="math inline">\(n\)</span> (the common choice is <span class="math inline">\(n=3\)</span>).</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong><span class="math inline">\((n,k)\)</span>-dominance rule</strong></td>
<td style="text-align: left;">the sum of the <span class="math inline">\(n\)</span> largest contributions exceeds <span class="math inline">\(k\%\)</span> of the cell total, <em>i.e.</em> <span id="eq-dominance-formula"><span class="math display">\[x_{1} + \ldots + x_{n} &gt; \frac{k}{100} X \tag{4.1}\]</span></span></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong><span class="math inline">\(p\%\)</span>-rule</strong></td>
<td style="text-align: left;">the cell total minus the 2 largest contributions <span class="math inline">\(x_{1}\)</span> and <span class="math inline">\(x_{2}\)</span> is less than <span class="math inline">\(p\%\)</span> of the largest contribution, <em>i.e.</em><a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> <span id="eq-p-percent-formula"><span class="math display">\[X - x_{2} - x_{1} &lt; \frac{p}{100}x_{1} \tag{4.2}\]</span></span></td>
</tr>
</tbody>
</table>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;<span class="math inline">\(X − x_{n} − \cdots − x_{2} − x_{1} &lt; \frac{p}{100} x_{1}\)</span> for the case of coalitions of <span class="math inline">\(n-1\)</span> respondents, where <span class="math inline">\(n&gt;2\)</span></p></div></div></div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-tbl" id="tbl-sensitivity-rules-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;4.2: Sensitivity rules
</figcaption>
</figure>
</div>
<p>Note that both the dominance rule and the <span class="math inline">\(p\%\)</span>-rule are meaningful only when all contributions are non-negative. Moreover, the dominance rule does not make sense for <span class="math inline">\(k=100\)</span> and neither does the <span class="math inline">\(p\%\)</span>-rule for <span class="math inline">\(p = 0\)</span>. Both rules are asymptotically equal to minimum frequency rules for <span class="math inline">\(k\rightarrow 100\)</span>, or <span class="math inline">\(p \rightarrow 0\)</span> respectively.</p>
<p>Both, the dominance rule and the <span class="math inline">\(p\%\)</span>-rule belong to a class of rules which are referred to as <em>“concentration rules”</em> below.</p>
<p>When cell suppression is used to protect the table, any aggregate (or: cell in a table) that is indeed <em>‘unsafe’</em>, or <em>‘sensitive’</em> according to the sensitivity rule employed, is subject to what is called <em>‘primary suppression’</em>.</p>
<p>Choice of a particular sensitivity rule is usually based on certain intruder scenarios involving assumptions about additional knowledge available in public or to particular users of the data, and on some (intuitive) notion on the sensitivity of the variable involved.</p>
<p><br>
<strong><em>Minimum frequency rule</em></strong><br>
When the disseminating agency thinks it is enough to prevent exact disclosure, all cells with at least as many respondents as a certain, fixed minimum frequency <span class="math inline">\(n\)</span> are considered safe. Example 1 of <a href="#sec-tabular-introduction" class="quarto-xref"><span>Section 4.1</span></a> (cell value referring to one company in the mining sector of a region) illustrates the disclosure risk for cells with frequency 1. Example 1b above (on two mining companies) shows that there is a similar risk for cells with frequency 2.</p>
<p>Normally the minimum frequency <span class="math inline">\(n\)</span> will be set to 3.</p>
<p>An exception is the case when for some <span class="math inline">\(n_0\)</span> larger than 3 the agency thinks it is realistic to assume that a coalition of <span class="math inline">\(n_0 - 2\)</span> respondents contributing to the same cell may pool their data to disclose the contribution of another respondent. In such a case we set <span class="math inline">\(n\)</span> to <span class="math inline">\(n_0\)</span>. Example 1c above provides an instance for this case with <span class="math inline">\(n_0=5\)</span>. (The intruder knows the pooled data of <span class="math inline">\(5 - 2 = 3\)</span> companies (<em>e.g.</em> B, C and D)).</p>
<p>It should be stressed here that a minimum frequency larger than 3 normally does not make much sense, even though in example 1c, for some cells of a table it may happen that such a ‘pooled data’ situation actually occurs. Usually there is no way for an agency to know for which cell which size to assume for the ‘pool’. Let us assume, for instance, that in a cell with 100 respondents 99 are in the stock market and are therefore obliged to publish data which are also their contributions to that cell value. Then we should consider 99 as the size of the pool, because anybody could add up (<em>i.e.</em> pool) these 99 published data values in order to disclose the (confidential) contribution of company 100 who is not in the stock market. But should the agency really consider all cells with less than 101 respondents as unsafe?</p>
<p><br>
<strong><em>Concentration rules</em></strong><br>
A published cell total is of course always an upper bound for each individual contribution to that cell. This bound is the closer to an individual contribution, the larger the size of the contribution. This fact is the mathematical foundation of the well-known concentration rules. Concentration rules like the dominance and <span class="math inline">\(p\%\)</span>‑rule make sense only if it is assumed specifically that the intruders are able to identify the largest contributors to a cell. The commonly applied sensitivity rules differ in the particular kind and precision of additional knowledge assumed to be around.</p>
<p>When a particular variable is deemed strongly confidential, preventing only exact disclosure may be judged inadequate. In such a case a concentration rule should be specified. For reasons that will be explained below, we recommend use of the so called <span class="math inline">\(p\%\)</span> ‑rule. Another, well known concentration rule is the ‘<span class="math inline">\(n\)</span> respondent, <span class="math inline">\(k\)</span> percent’ dominance rule. Note, that it is absolutely essential to keep the parameters of a concentration rule confidential!</p>
<p>Traditionally, some agencies use a combination of a minimum frequency rule together with a <span class="math inline">\((1,k)\)</span>‑dominance rule. This approach, however, is inadequate, because it ignores the problem that in some cases the contributor with the second largest contribution to a cell which is non-sensitive according to this rule is able to derive a close upper estimate for the contribution of the largest one by subtracting her own contribution from the aggregate total. Example 1 provides an instance.</p>
<div class="callout callout-style-simple callout-note">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-body-container">
<p><strong>Example 1</strong> Application of the (1,90)-rule. Let the total value of a table cell be <span class="math inline">\(X = 100,000\)</span>, let the two largest contributions be <span class="math inline">\(x_{1} = 50,000\)</span> and <span class="math inline">\(x_{2} = 49,\!000\)</span>. Since <span class="math inline">\(50,\!000 &lt; \frac{90}{100} 100,\!000\)</span> the cell is safe according to the <span class="math inline">\((1,90)\)</span>-rule: there seems to be no risk of disclosure. But the second largest contributor is able to derive an upper estimate <span class="math inline">\({\hat{x}}_{1} = 100,\!000 − 49,\!000 = 51,\!000\)</span> for the largest contribution which overestimates the true value of <span class="math inline">\(50,000\)</span> by <span class="math inline">\(2\%\)</span> only: quite a good estimate!</p>
</div>
</div>
</div>
<p>Unlike the <span class="math inline">\((1,k)\)</span>-dominance rule, both the <span class="math inline">\((2,k)\)</span>‑dominance rule and <span class="math inline">\(p\%\)</span>‑rule take the additional knowledge of the second largest contributor into account properly. Of the two, the <span class="math inline">\(p\%\)</span>‑rule should be preferred, because the <span class="math inline">\((2,k)\)</span>-dominance rule has a certain tendency for overprotection, as we will see in the following.</p>
<p><br>
<strong><em><span class="math inline">\(p\%\)</span>-rule and dominance-rule</em></strong><br>
We will show in the following that, according to both types of concentration rules, an aggregate total (<em>i.e.</em> cell value) <span class="math inline">\(X\)</span> is considered as sensitive, if it provides an upper estimate for one of the individual contributions that is relatively close to this contribution.</p>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Expert level
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Assume that there are no coalitions of respondents, <em>i.e.</em> there are no intruders knowing more than one of the contributions. Then the closest upper estimate of any other contribution can be obtained by the second largest contributor, when it subtracts its own contribution <span class="math inline">\(x_{2}\)</span> from the aggregate total (<em>i.e.</em> cell value) <span class="math inline">\(X\)</span> to estimate the largest contribution (<span class="math inline">\({\hat{x}}_{1} = X - x_{2}\)</span>) as seen in example 1. All other scenarios of a contributor subtracting its own value from the total, to estimate another, result in larger relative error. In the, rather unlikely, scenario that <span class="math inline">\(n - 1\)</span> (for <span class="math inline">\(n &gt; 2\)</span>), respondents pool their data in order to disclose the contribution of another, the closest upper estimate of any other contribution can be obtained by the coalition of respondents <span class="math inline">\(2, 3,\ldots, n\)</span> when they estimate <span class="math inline">\({\hat{x}}_{1} = X - x_{2} - x_{3} - \ldots - x_{n}\)</span>.</p>
<p>The question is now, how to determine whether such an estimate is ‘relatively close’.</p>
<p>Application of the <span class="math inline">\(p\%\)</span>-rule yields that the upper estimate <span class="math inline">\({\hat{x}}_{1}\)</span> will overestimate the true value by at least <span class="math inline">\(p\%\)</span> for any non-sensitive cell, <em>i.e.</em> <span class="math inline">\({\hat{x}}_{1} - x_{1} \geq \frac{p}{100} x_{1}\)</span> . That is, the <span class="math inline">\(p\%\)</span>‑rule sets the difference between estimate and true value of the largest contribution in relation to the value of the largest contribution itself.</p>
<p>When we adapt relation (<a href="#eq-dominance-formula" class="quarto-xref"><span>4.1</span></a>) in table 1 (see definition of the <span class="math inline">\((n,k)\)</span>-rule) to the case of <span class="math inline">\(n = 2\)</span>, subtract both sides from <span class="math inline">\(X\)</span> and then divide by <span class="math inline">\(X\)</span> the result is <span id="eq-dominance-relative-error"><span class="math display">\[
(X - x_{2}) - x_{1} &lt; \frac{100 - k}{100} X
\tag{4.3}\]</span></span></p>
<p>In this formulation, the <span class="math inline">\((2,k)\)</span>-rule looks very similar to the formulation of the <span class="math inline">\(p\%\)</span>-rule given by (<a href="#eq-p-percent-formula" class="quarto-xref"><span>4.2</span></a>). Both rules define an aggregate to be sensitive, when the estimate <span class="math inline">\(\hat{x}_{1} = X - x_{2}\)</span> does not overestimate the true value of <span class="math inline">\(x_{1}\)</span> ‘sufficiently’. The difference between both rules is in how they determine this ‘sufficiency’. According to the <span class="math inline">\(p\%\)</span>-rule, it is expressed as a rate (<em>i.e.</em> <span class="math inline">\(p\%\)</span>) of the true value of the largest contribution <span class="math inline">\(x_{1}\)</span>, while according to the <span class="math inline">\((2,k)\)</span>‑rule, it is expressed as a rate (<em>i.e.</em> <span class="math inline">\((100-k)\%\)</span>) of the aggregate total <span class="math inline">\(X\)</span>. Considering this, the concept of the <span class="math inline">\(p\%\)</span>-rule seems to be more natural than that of the <span class="math inline">\((2,k)\)</span>-rule.</p>
<p><span class="math inline">\((2,k)\)</span>-rules correspond to <span class="math inline">\(p\%\)</span>-rules in the following way:</p>
<p>If <span class="math inline">\(k\)</span> is set to <span class="math inline">\(100 \frac{100}{100+p}\)</span> then</p>
<ol type="A">
<li><p>any aggregate, which is safe according to the <span class="math inline">\((2,k)\)</span>‑rule, is also safe according to the <span class="math inline">\(p\%\)</span>-rule (this will be proven below), but</p></li>
<li><p>not any aggregate, which is safe according to the <span class="math inline">\(p\%\)</span>-rule, is also safe according to this <span class="math inline">\((2,k)\)</span>-rule. An example is given below (example 2). In these cases the aggregate could be published according to the <span class="math inline">\(p\%\)</span>-rule, but would have to be suppressed according to the <span class="math inline">\((2,k)\)</span>-rule.</p></li>
</ol>
<p>Based on the above explained idea, that the concept of the <span class="math inline">\(p\%\)</span>-rule is more natural than that of the <span class="math inline">\((2,k)\)</span>-rule, we interpret this as a tendency for over-protection in the <span class="math inline">\((2,k)\)</span>-rule. Example 2 below is an instance for this kind of over-protection.</p>
<p>We therefore recommend use of the <span class="math inline">\(p\%\)</span>‑rule instead of a <span class="math inline">\((2,k)\)</span>‑dominance rule.</p>
<p><br>
<strong><em>How to obtain the <span class="math inline">\(p\)</span> parameter?</em></strong><br>
When we replace a <span class="math inline">\((2,k)\)</span>‑dominance rule, by a <span class="math inline">\(p\%\)</span>‑rule, the natural choice is to derive the parameter <span class="math inline">\(p\)</span> from <span class="math inline">\(k = 100 \frac{100}{100 + p}\)</span> , <em>e.g.</em> to set <span class="math inline">\(p = 100 \frac{100 - k}{k}\)</span></p>
<p>Thus, a <span class="math inline">\((2,80)\)</span>‑dominance rule would be replaced by a <span class="math inline">\(p\%\)</span>‑rule with <span class="math inline">\(p = 25\)</span>, a <span class="math inline">\((2,95)\)</span>‑dominance rule by a <span class="math inline">\(p\%\)</span>‑rule with <span class="math inline">\(p = 5.26\)</span> .</p>
<p>If we also derive <span class="math inline">\(p\)</span> from this formula, when replacing a <span class="math inline">\((1,k)\)</span>‑dominance rule, we will obtain a much larger number of sensitive cells. In addition to the cells which are unsafe according to the <span class="math inline">\((1,k)\)</span>-dominance rule which will then also be unsafe according to the <span class="math inline">\(p\%\)</span>‑rule, there will be cells which were safe according to the <span class="math inline">\((1,k)\)</span>‑dominance rule, but are not safe according to the <span class="math inline">\(p\%\)</span>‑rule, because the rule correctly considers the insider knowledge of a large second largest contributor. We could then put up with this increase in the number of sensitive cells. Alternatively, we could consider the number of sensitive cells that we used to assign (with the <span class="math inline">\((1,k)\)</span>-dominance rule) as a kind of a maximum-prize we are prepared to ‘pay’ for data protection. In that case we will reduce the parameter <span class="math inline">\(p\)</span>. The effect will be that some of the cells we used to consider as sensitive according to the <span class="math inline">\((1,k)\)</span>-dominance rule will now not be sensitive. But this would be justified because those cells are less sensitive as the cells which are unsafe according to the <span class="math inline">\(p\%\)</span>-rule, but are not according to the former <span class="math inline">\((1,k)\)</span>-dominance rule, as illustrated above by Example 1.</p>
<div class="callout callout-style-simple callout-note">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-body-container">
<p><strong>Example 2</strong> Let <span class="math inline">\(p = 10\)</span>, then <span class="math inline">\(k = 100 \frac{100}{100 + p} = 90.9\)</span>, let the total value of a table cell be <span class="math inline">\(X = 110,\!000\)</span>, let the largest two contributions be <span class="math inline">\(x_{1} = 52,\!000\)</span>, and <span class="math inline">\(x_{2} = 50,\!000\)</span>.</p>
<p>Then <span class="math display">\[
\hat{x_1} = X - x_{2} = 110,\!000 − 50,\!000 = 60,\!000
\]</span> and <span class="math display">\[
100 \frac{\hat{x_1} - x_1}{x_1}=100 \frac{60,\!000 - 52,\!000}{52,\!000}=15.4
\]</span> <em>i.e.</em> the upper estimate <span class="math inline">\(\hat{x_1} = X - x_{2}\)</span> will overestimate the true value by <span class="math inline">\(15.4\%\)</span>. So the aggregate is safe according to the <span class="math inline">\(p\%\)</span>-rule at <span class="math inline">\(p = 10\)</span>.</p>
<p>On the other hand the two largest contributions are <span class="math inline">\(x_{1} + x_{2} = 102,\!000\)</span>. As <span class="math inline">\(102,000 &gt; \frac{100} {100 + p } X = 100,\!000\)</span> the aggregate is not safe according to the <span class="math inline">\((2,k)\)</span>-rule.</p>
</div>
</div>
</div>
<p><br>
<em>Proof of A.</em><br>
For an aggregate which is safe according to the <span class="math inline">\((2,k)\)</span>‑rule with <span class="math inline">\(k = 100 \cdot \frac{100}{100 + p}\)</span> the following will hold: <span id="eq-i"><span class="math display">\[
x_{1} \leq \frac{k}{100} \cdot X = \frac{100}{100 + p} X
\tag{4.4}\]</span></span> and <span id="eq-ii"><span class="math display">\[\frac{ \left( X - x_{2} \right) - x_{1} }{ X } \geq 1 - \frac{k}{100} = 1 - \frac{100}{100 + p} = \frac{p} {100 + p}
\tag{4.5}\]</span></span> (c.f. (<a href="#eq-dominance-relative-error" class="quarto-xref"><span>4.3</span></a>) ).<br>
This is equivalent to <span id="eq-iii"><span class="math display">\[
\frac{ \left( X - x_{2} \right) - x_{1} }{ x_1 } \geq \frac{p}{100+p}\frac{X}{x1}
\tag{4.6}\]</span></span> From (<a href="#eq-i" class="quarto-xref"><span>4.4</span></a>) it follows that <span class="math display">\[
\frac{ p }{ 100 + p } \frac{ X }{ x_{1} } \geq \frac{p}{100}
\]</span> And hence from (<a href="#eq-iii" class="quarto-xref"><span>4.6</span></a>) that <span class="math display">\[\frac{ \left( X - x_{2} \right) -  x_{1} } { x_{1} } \geq \frac{p}{ 100 + p } \cdot \frac{ X }{ x_{ 1 } } \geq \frac{ p }{ 100 }
\]</span></p>
</div>
</div>
</div>
<p><br>
<strong><em>The <span class="math inline">\((p,q)\)</span>-rule</em></strong><br>
A well known extension of the <span class="math inline">\(p\%\)</span>-rule is the so called prior‑posterior <span class="math inline">\((p,q)\)</span>‑rule. With the extended rule, one can formally account for general knowledge about individual contributions assumed to be around <em>prior</em> to the publication, in particular that the second largest contributor can estimate the smaller contributions <span class="math inline">\(X_{R} = \sum_{ i &gt; 2 } x_{1}\)</span> to within <span class="math inline">\(q\%\)</span>. An aggregate is then considered unsafe when the second largest respondent could estimate the largest contribution <span class="math inline">\(x_{1}\)</span> to within <span class="math inline">\(p\)</span> percent of <span class="math inline">\(x_{1}\)</span> , by subtracting her own contribution and this estimate <span class="math inline">\({\hat{X}}_{R}\)</span> from the cell total, <em>i.e.</em> when <span class="math inline">\(|\left( X - x_{2} \right) - x_{1} - {\hat{X}}_{R}| &lt; \frac{p}{100} x_{1}\)</span>. Because <span class="math inline">\(\left( X - x_{2} \right) - x_{1} = X_{R}\)</span>, the left hand side is assumed to be less than <span class="math inline">\(\frac{q}{100} X_{R}\)</span>. So the aggregate is considered to be sensitive, if <span class="math inline">\(X_{R} &lt; \frac{p}{q} x_{1}\)</span>. Evidently, it is actually the ratio <span class="math inline">\(\frac{p}{q}\)</span> which determines which cells are considered safe, or unsafe. Therefore, any <span class="math inline">\((p,q)\)</span>‑rule with <span class="math inline">\(q &lt; 100\)</span> can also be expressed as <span class="math inline">\(( p^*, q^*)\)</span>‑rule, with <span class="math inline">\(q^* = 100\)</span> and <span id="eq-p-star"><span class="math display">\[
p^* := 100  \frac{p}{q}
\tag{4.7}\]</span></span></p>
<p>Of course we can also adapt the <span class="math inline">\((n,k)\)</span>-dominance rule to account for <span class="math inline">\(q\%\)</span> relative a priori bounds: Let <em>e.g.</em> <span class="math inline">\(n = 2\)</span>. According to (<a href="#eq-dominance-relative-error" class="quarto-xref"><span>4.3</span></a>) above, an aggregate should then be considered unsafe when the second largest respondent could estimate the largest contribution <span class="math inline">\(x_{1}\)</span> to within <span class="math inline">\((100 - k)\)</span> percent of <span class="math inline">\(X\)</span> , by subtracting her own contribution and the estimate <span class="math inline">\({\hat{X}}_{R}\)</span> from the cell total, <em>i.e.</em> when <span class="math inline">\(|\left( X - x_{2} \right) - x_{1} - {\hat{X}}_{R}| &lt; (100 - k)/100  X\)</span>. Just as in the case of the <span class="math inline">\(p\%\)</span>-rule, we see that the aggregate is sensitive, when <span class="math inline">\(X_{R} &lt; \frac{100 - k}{q} X\)</span>, and that for given parameters <span class="math inline">\(k\)</span> and <span class="math inline">\(q\)</span> the parameter <span class="math inline">\(k^*\)</span> corresponding to <span class="math inline">\(q^* = 100\)</span> should be chosen such that <span id="eq-k-star"><span class="math display">\[
\frac{100-k^*}{100} = \frac{100-k}{q}
\tag{4.8}\]</span></span></p>
<p>For a more analytical discussion of sensitivity rules the interested reader is referred to (Cox, 2001), for more generalized formulations considering coalitions to (Loeve, 2001).</p>
<p><br>
<strong><em>Negative contributions</em></strong><br>
When disclosure risk has to be assessed for a variable that can take not only positive, but also negative values, we suggest to reduce the value of <span class="math inline">\(p\)</span> (or increase <span class="math inline">\(k\)</span>, for the dominance-rule, resp.). It may even be adequate to take that reduction even to the extent of replacing a concentration rule by a minimum frequency rule. This recommendation is motivated by the following consideration. Above, we have explained that the <span class="math inline">\(p\%\)</span>‑rule is equivalent to a <span class="math inline">\((p,q)\)</span>-rule with <span class="math inline">\(q=100\)</span>. When contributions may take negative, as well as positive values, it makes sense to assume that the bound <span class="math inline">\(q_{-}\)</span> for the relative deviation of <em>a priori</em> estimates <span class="math inline">\({\hat{X}}_{R}\)</span> exceeds <span class="math inline">\(100\%\)</span>. This can be expressed as <span class="math inline">\(q_{-} = 100 f\)</span>, with <span class="math inline">\(f &gt; 1\)</span>. According to (<a href="#eq-p-star" class="quarto-xref"><span>4.7</span></a>) the <span class="math inline">\(p\)</span> parameter <span class="math inline">\(p_f\)</span> for the case of negative/positive data should be chosen as <span class="math inline">\(p_{f} = 100 \frac{p}{q_{-}} = 100 \frac{p}{100 f} = \frac{p}{f} &lt; p\)</span>. That means particularly that for large <span class="math inline">\(f\)</span> the <span class="math inline">\(p\%\)</span>-rule with corresponding parameter <span class="math inline">\(p_{f}\)</span> is asymptotically equal to the minimum frequency rule (c.f. the remark just below <a href="#tbl-sensitivity-rules" class="quarto-xref">Table&nbsp;<span>4.2</span></a>).</p>
<p>In case of the dominance rule, because of (<a href="#eq-k-star" class="quarto-xref"><span>4.8</span></a>), <span class="math inline">\(k_{f}\)</span> can be determined by <span class="math inline">\(\frac{100 - k_{f}}{100} = \frac{100 - k}{100 f}\)</span>, and because <span class="math inline">\(f &gt; 1\)</span> this means that <span class="math inline">\(k_{ f } &gt; k\)</span>. For large <span class="math inline">\(f\)</span> the dominance rule with parameter <span class="math inline">\(k_{ f }\)</span> will be asymptotically equal to the minimum frequency rule.</p>
<p><br>
<strong><em>Waivers</em></strong><br>
Sometimes, respondents authorize publication of an aggregate even if this publication might cause a risk of disclosure for their contributions. Such authorizations are also referred to as <em>‘waivers’</em>. When <span class="math inline">\(s\)</span> is the largest respondent from which no such waiver has been obtained, and <span class="math inline">\(r\)</span> is the largest respondent except for <span class="math inline">\(s\)</span> then for any pair <span class="math inline">\((i,j)\)</span>, <span class="math inline">\(i \neq j\)</span> of respondents, where no waiver has been obtained from <span class="math inline">\(j\)</span>, it holds <span class="math inline">\(x_{i} + x_{j} \leq x_{r} + x_{s}\)</span>. Therefore, we will be able to deal with such a situation properly, if we reformulate the concentration rules as<br>
<br>
<span class="math inline">\(X - x_{s} - x_{r} &lt; \frac{p}{100} x_{s},\)</span> for the <span class="math inline">\(p\%\)</span>-rule,<br>
<br>
and<br>
<br>
<span class="math inline">\(x_{r} + x_{s} &gt; \frac{k}{100} X,\)</span> for the <span class="math inline">\((2,k)\)</span>-dominance rule.</p>
<p><br>
<strong><em>Foreign trade rules</em></strong><br>
In foreign trade statistics traditionally a special rule is applied. Only for those enterprises that have actively asked for protection of their information special sensitivity rules are applied. This implies that if the largest contributor to a cell asks for protection and contributes over a certain percentage to the cell total, that cell is considered a primarily unsafe cell.</p>
<p>Given the normal level of detail in the foreign trade tables the application of the standard sensitivity rules would imply that a very large proportion of the table should have been suppressed. This is considered not to be a desirable situation and for a long time there have not been serious complaints. This has led to this special rule. And this rule has a legal basis in the European regulation 638/2004 <a href="https://eur-lex.europa.eu/LexUriServ/LexUriServ.do?uri=CELEX:32004R0638:EN:HTML">https://eur-lex.europa.eu/LexUriServ/LexUriServ.do?uri=CELEX:32004R0638:EN:HTML</a>.</p>
<p>In <span class="math inline">\(\tau\)</span>‑ARGUS special options (<em>the request rule</em>) have been introduced to apply this rule. The secondary cell suppression will be done in the standard way.</p>
<p><br>
<strong><em>Holdings</em></strong><br>
In many datasets, especially economic datasets, the reporting unit is different from the entities we want to protect. Often companies have several branches in various regions. So the marginal cell may have several contributions but from one company only. Therefore it might be a mistake to think that such a cell is safe if we look only at the largest two contributors, while the largest three might belong to only one company.</p>
<p>We need to group the contributions from one company together to one contribution before we can apply the sensitivity rules.</p>
<p>There is no problem in making the cell totals because we are only interested in the sum of the contributions.</p>
<p>In <span class="math inline">\(\tau\)</span>‑ARGUS it is possible to take this into account by using the holding option.</p>
<p><br>
<strong><em>Sampling weights</em></strong><br>
Often tables are created from datasets based on sample surveys and NSIs collect a lot of their information this way. There are two reasons why disclosure risk assessment may also be necessary for a sample survey: especially in business surveys the common approach is to sample with unequal probabilities. The first is that large companies are often sampled with probability 1. Typically it will be the case for some of the aggregates foreseen for a publication that all respondents have been sampled with probability 1. In the absence of non-response, for such an aggregate, the survey data are data for the full population. Secondly even, if sampling probabilities are smaller than 1, if an aggregate relates to a strongly skewed population, and the sampling error is small, then the probability is high that the survey estimate for the aggregate total may also be a close estimate for the largest unit in that population. It makes sense therefore to assess the disclosure risk for survey sample estimates, if the sampling errors are small but the variables are strongly sensitive. The question is then, how to determine technically, if a sample survey aggregate should be considered safe, or unsafe. The following procedure is used in <span class="math inline">\(\tau\)</span>‑ARGUS:</p>
<p>For sample survey data each record has a sampling weight associated with it. These sampling weights are constructed such that the tables produced from this datasets will resemble as much as possible the population, as if the table had been based on a full census.</p>
<p>Making a table one has to take into account these sampling weights. If the data file has a sampling weight, specified in the metadata, the table can be computed taking these weights into account. For making a cell total this is a straightforward procedure, however the sensitivity rules have to be adapted to the situation where we do not know the units of the population with the largest values.</p>
<p>One option is the following approximation:</p>
<div class="callout callout-style-simple callout-note">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-body-container">
<p><strong>Example</strong> A cell with two contributions:</p>
<p><em>100, weight 4</em></p>
<p><em>10, weight 7</em></p>
<p>The weighted cell value then equals (4 <span class="math inline">\(\times\)</span> 100) + (7 <span class="math inline">\(\times\)</span> 10) = 470. Without considering the weights there are only two contributors to the cell: one contributing 100 and another contributing 10. However by taking account of the sampling weights the individual contributions can be approximated as 100, 100, 100, 100, 10, 10, 10, 10, 10, 10 and 10. The largest two contributors are now 100 and 100. These are regarded as the largest two values for application of the safety rules. If the weights are not integers, a simple extension is applied.</p>
</div>
</div>
</div>
<p><em>Note:</em><br>
This procedure cannot be applied in combination with the holding concept, because naturally for the approximate contributions it is unknown which holding they belong to.</p>
<p>It should also be noted here that with the above procedure it may happen that a cell is considered safe, even though the sample estimate of the cell total provides a very close estimate of the unit in the population with the largest value. Assume <em>e.g.</em> the sequence of values in the full population to be 900, 100, 100, and seven times 5. Then the population total would be 1135. Assume that we sampled two units, both with a value of 100, but with different probabilities so that the sampling weights for the two are 1, and 9, respectively, yielding a population estimate of 1000 which will not be sensitive according to the above procedure. If the second largest unit as an intruder subtracts her own contribution (= 100) from the population estimate she will obtain an estimate of the value of the largest unit which exactly matches the true value of 900. But there is of course a sampling error connected to this estimate and this fact should be considered by the intruder.</p>
<p>On the other hand, there is a certain tendency for overprotection associated to the above procedure, <em>i.e.</em> there is a chance that according to the above procedure an aggregate is unsafe, when in fact it is not. Assume <em>e.g.</em> the sequence of values in the full population to be 100, 50, 50, 1, 1. Assume we have sampled (with unequal probabilities) the unit with value 100 with a sampling weight of 2 associated to it, and one of the units with value 1 with a sampling weight of 3. According to the above procedure two values of 100 would be considered as largest contribution for an estimated cell total of 203 which according to for instance a <span class="math inline">\(p\%\)</span>-rule would be considered as sensitive for any <span class="math inline">\(p&gt;3\)</span> . But in fact, even the second largest unit in the population with a value of 50 will overestimate the value 100 of the largest unit by about 50% when it subtracts her own contribution from the population estimate of 203.</p>
<p>This tendency for overprotection can be avoided, when, instead of the above procedure, in the case of sample surveys we replace for instance for the <span class="math inline">\(p\%\)</span>‑rule the formulation (<a href="#eq-p-percent-formula" class="quarto-xref"><span>4.2</span></a>) of <a href="#tbl-sensitivity-rules" class="quarto-xref">Table&nbsp;<span>4.2</span></a> by the following <span id="eq-p-percent-weighted"><span class="math display">\[
\hat{X} - x_{2}^{s} - x_{1}^{s} &lt; \frac{p}{100} x_{1}^{s}
\tag{4.9}\]</span></span> where <span class="math inline">\(\hat{X}\)</span> denote the estimated population total, and <span class="math inline">\(x_{i}^{s}\)</span>, <span class="math inline">\(i = 1,2\)</span>, the largest two observations from the sample.</p>
<p>The difference between this, and the above procedure is that with (<a href="#eq-p-percent-weighted" class="quarto-xref"><span>4.9</span></a>) we consider as intruder the respondent with the second largest contribution observed in the sample, whereas according to the above procedure, whenever the sampling weight of the largest respondent is 2 or more, an ‘artificial’ intruder is assumed who contributes as much as the largest observation from the sample.</p>
<p>Considering formulation (<a href="#eq-dominance-relative-error" class="quarto-xref"><span>4.3</span></a>) of the dominance-rule, it is straightforward to see that we can adapt (<a href="#eq-p-percent-weighted" class="quarto-xref"><span>4.9</span></a>) to the case of dominance‑rules by <span id="eq-dominance-weighted"><span class="math display">\[
x_{1}^{s} + \ldots + x_{n}^{s} &gt; \frac{k}{100} \hat{X}
\tag{4.10}\]</span></span></p>
<p>It is also important to note in this context that sampling weight should be kept confidential, because otherwise we must replace (<a href="#eq-p-percent-weighted" class="quarto-xref"><span>4.9</span></a>) by <span id="eq-p-percent-weighted-alt"><span class="math display">\[
\hat{X} - w_{2} x_{2}^{s} - w_{1} x_{1}^{s} &lt; \frac{p}{100} x_{1}^{s}
\tag{4.11}\]</span></span></p>
<p>where <span class="math inline">\(w_{i}\)</span>, <span class="math inline">\(i=1,2\)</span>, denote the sampling weights. Obviously, according to (<a href="#eq-p-percent-weighted-alt" class="quarto-xref"><span>4.11</span></a>) more aggregates will be sensitive.</p>
<p>Considering this as the basic idea of secondary protection, after <a href="#sec-sensitive-cells-magnitude-tables" class="quarto-xref"><span>Section 4.2.1</span></a> we assume for the remainder of the chapter margins and overall totals always to be part of a table.</p>
<p>In <a href="#sec-secondary-protection-methods" class="quarto-xref"><span>Section 4.2.2</span></a> we introduce into the concepts of secondary tabular data protection methodologies.</p>
</section>
<section id="sec-secondary-protection-methods" class="level3" data-number="4.2.2">
<h3 data-number="4.2.2" class="anchored" data-anchor-id="sec-secondary-protection-methods"><span class="header-section-number">4.2.2</span> Secondary tabular data protection methods</h3>
<p>For reasons explained in the introduction of this chapter, we assume here that tables always include margins and overall totals along with their ‘inner’ cells. Thus, there is always a linear relationship between cells of a table. (Consider for instance example 1a of the introduction to this chapter: The sector result (say: <span class="math inline">\(X_T\)</span>) is the sum of the three sub-sector results (<span class="math inline">\(X_1\)</span> to <span class="math inline">\(X_3\)</span>), <em>i.e.</em> the linear relationship between these four cells is given by the relation <span class="math inline">\(X_T = X_1 + X_2 + X_3\)</span>. As we have seen in example 1a, if it has been established that a disclosure risk is connected to the release of certain cells of a table, then it is not enough to prevent publication of these cells. Other cells must be suppressed (so called ‘complementary’ or ‘secondary’ suppressions), or be otherwise manipulated in order to prevent the value of the protected sensitive cell being recalculated through f.i. differencing.</p>
<p>Within this section we explain the methodological background of secondary tabular data protection methods.</p>
<p>At the end of the section we give a brief comparison of secondary cell suppression, partial suppression and controlled tabular adjustment as alternative disclosure limitation techniques.</p>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Expert level
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>When a table is protected by cell suppression, by making use of the linear relation between published and suppressed cell values in a table with suppressed entries, it is always possible for any particular suppressed cell of a table to derive upper and lower bounds for its true value. This holds for either tables with non-negative values, and those tables containing negative values as well, when it is assumed that instead of zero, some other (possibly tight) lower bound for any cell is available to data users in advance of publication. The interval given by these bounds is called the <em>‘feasibility interval’</em>.</p>
<div class="callout callout-style-simple callout-note">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-body-container">
<p><strong>Example 3</strong> This example (Geurts, 1992, Table 10, p 20) illustrates the computation of the feasibility interval in the case of a simple two-dimensional table where all cells may only assume non-negative values:</p>
<div id="tbl-example-3" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<div aria-describedby="tbl-example-3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="table">
<thead>
<tr class="header">
<th></th>
<th style="text-align: center;">1</th>
<th style="text-align: center;">2</th>
<th style="text-align: center;">Total</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>1</strong></td>
<td style="text-align: center;"><span class="math inline">\(X_{11}\)</span></td>
<td style="text-align: center;"><span class="math inline">\(X_{12}\)</span></td>
<td style="text-align: center;">7</td>
</tr>
<tr class="even">
<td><strong>2</strong></td>
<td style="text-align: center;"><span class="math inline">\(X_{21}\)</span></td>
<td style="text-align: center;"><span class="math inline">\(X_{22}\)</span></td>
<td style="text-align: center;">3</td>
</tr>
<tr class="odd">
<td><strong>3</strong></td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">6</td>
</tr>
<tr class="even">
<td><strong>Total</strong></td>
<td style="text-align: center;">9</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">16</td>
</tr>
</tbody>
</table>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-tbl" id="tbl-example-3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;4.3: Example 3
</figcaption>
</figure>
</div>
<p>For this table the following linear relations hold:</p>
<p><span class="math display">\[\begin{align}
X_{11} + X_{12} &amp;= 7 \quad \text{(R1)} \\
X_{21} + X_{22} &amp;= 3 \quad \text{(R2)} \\
X_{11} + X_{21} &amp;= 6 \quad \text{(C1)} \\
X_{12} + X_{22} &amp;= 4 \quad \text{(C2)} \\
\end{align}\]</span></p>
<p>with <span class="math inline">\(X_{ij} \geq 0, \forall (i,j)\)</span>. Using linear programming methodology, it is possible to derive systematically for any suppressed cell in a table an upper bound <span class="math inline">\(\left(X^{\max} \right)\)</span> and a lower bound <span class="math inline">\(\left( X^{\min} \right)\)</span> for the set of feasible values. In the example above, for cell (1,1) these bounds are <span class="math inline">\(\left( X_{11}^{\min} \right)\)</span> = 3 and <span class="math inline">\(X_{11}^{\max}\)</span> = 6 .</p>
<p>In this simple instance, however, we do not need linear programming technology to derive this result: Because of the first column relation (C1) <span class="math inline">\(X_{11}\)</span> must be less or equal 6, and because of the second row relation (R2) <span class="math inline">\(X_{21}\)</span> must be less or equal 3. Therefore, and because of the first column relation (C1) <span class="math inline">\(X_{11}\)</span> must be at least 3.</p>
</div>
</div>
</div>
<p>A general mathematical statement for the linear programming problem to compute upper and lower bounds for the suppressed entries of a table is given in Fischetti and Salazar (2000).</p>
<p>In principle, a set of suppressions (the <em>‘suppression pattern’</em>) should only be considered valid, if the bounds for the feasibility interval of any sensitive cell cannot be used to deduce bounds on an individual respondent contribution contributing to that cell that are too close according to the sensitivity rule employed. For a mathematical statement of that condition, we determine safety bounds for primary suppressions. We call the deviation between those safety bounds and the true cell value <em>‘upper and lower protection levels’</em>. The formulas of <a href="#tbl-upper-protection-levels" class="quarto-xref">Table&nbsp;<span>4.4</span></a> can be used to compute upper protection levels. Out of symmetry considerations the lower protection level is often set identical to the upper protection level.</p>
<div id="tbl-upper-protection-levels" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<div aria-describedby="tbl-upper-protection-levels-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="table">
<colgroup>
<col style="width: 22%">
<col style="width: 77%">
</colgroup>
<thead>
<tr class="header">
<th>Sensitivity rule</th>
<th style="text-align: center;">Upper protection level</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\((1,k)\)</span>-rule</td>
<td style="text-align: center;"><span class="math inline">\(\frac{100}{k}x_1 - X\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\((n,k)\)</span>-rule</td>
<td style="text-align: center;"><span class="math inline">\(\frac{100}{k} \cdot (x_{1} + x_{2} + \cdots + x_{n}) − X\)</span></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(p\%\)</span>-rule</td>
<td style="text-align: center;"><span class="math inline">\(\frac{p}{100} \cdot x_{1} − (X − x_{1} − x_{2})\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\((p,q)\)</span>-rule</td>
<td style="text-align: center;"><span class="math inline">\(\frac{p}{q} \cdot x_{1} − (X − x_{1} − x_{2})\)</span></td>
</tr>
</tbody>
</table>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-tbl" id="tbl-upper-protection-levels-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;4.4: Upper protection levels
</figcaption>
</figure>
</div>
<p>Note that we recommend using protection levels of zero when instead of a concentration rule only a minimum frequency rule has been used for primary disclosure risk assessment. As explained in <a href="#sec-sensitive-cells-magnitude-tables" class="quarto-xref"><span>4.2.1</span></a>, minimum frequency rules (instead of concentration rules) should only be used, if it is enough to prevent exact disclosure only. And in such a case, a protection level of zero should be enough. Using <span class="math inline">\(\tau\)</span>-ARGUS this can be achieved by setting the parameter ‘minimum frequency range’ to zero.</p>
<p>If the distance between upper bound of the feasibility interval and true value of a sensitive cell is below the upper protection level computed according to the formulas of <a href="#tbl-upper-protection-levels" class="quarto-xref">Table&nbsp;<span>4.4</span></a>, then this upper bound could be used to derive an estimate for individual contributions of the sensitive cell that is too close according to the safety rule employed, which can easily be proven along the lines of Cox (1981).</p>
<div class="callout callout-style-simple callout-note">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-body-container">
<p><strong>Example 4</strong> Cell <span class="math inline">\(X = 330\)</span> with 3 contributions of distinct respondents <span class="math inline">\(x_{1}=300,\ x_{2}=20,\ x_{3}=10\)</span> is confidential (or ‘unsafe’) according to the <span class="math inline">\((1,85)\)</span>- dominance rule. If the feasible upper bound <span class="math inline">\(X^{\max}\)</span> for this confidential cell value is less than <span class="math inline">\(\frac{100}{85} \cdot x_{1} = 352.94\)</span> , then it will provide an upper estimate for the largest contribution <span class="math inline">\(x_{1}\)</span> which is too close according to the <span class="math inline">\((1,85)\)</span>-dominance rule.</p>
</div>
</div>
</div>
<p>Example 5 proves the formula given in <a href="#tbl-upper-protection-levels" class="quarto-xref">Table&nbsp;<span>4.4</span></a> for the case of the <span class="math inline">\(p\%\)</span>-rule:</p>
<div class="callout callout-style-simple callout-note">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-body-container">
<p><strong>Example 5</strong> Let <span class="math inline">\(X + U\)</span> be the upper bound of the feasibility interval for a cell with cell value <span class="math inline">\(X\)</span> . The second largest respondent can then deduce an upper bound <span class="math inline">\(x_{1}^{U}\)</span>by subtracting its own contribution from that upper bound: <span class="math inline">\(x_{1}^{U} = X + U - x_{2}\)</span>. According to the definition of the <span class="math inline">\(p\%\)</span>-rule proper protection means that <span class="math inline">\(x_{1}^{U} \geq \left( 1 + \frac{p}{100} \right) \cdot x_{1}\)</span>. So, if the feasibility interval provides upper protection, it must hold that <span class="math inline">\(U \geq \left( 1 + \frac{p}{100} \right) \cdot x_{1} - X + x_{2} = p/100 \cdot x_{1} - \left( X - x_{1} - x_{2} \right)\)</span>.</p>
</div>
</div>
</div>
<p>The distance between upper bound of the feasibility interval and true value of a sensitive cell must exceed the upper protection level; otherwise the sensitive cell is not properly protected. This safety criterion is a necessary, but not always sufficient criterion for proper protection! It is a sufficient criterion, when the largest respondent makes the same contribution also within the combination of suppressed cells within the same aggregation (a row or column relation of the table, for instance), and when no individual contribution of any respondent (or coalition of respondents) to such a combination of suppressed cells is larger than the second largest respondent’s (or coalition’s) contribution.</p>
<p>Cases where the criterion is not sufficient arise typically, when the only other suppressed cell within the same aggregation is a sensitive cell too (and not the marginal cell of the aggregation), or when the same respondent can contribute to more than one cell within the same aggregation. In these cases, it may turn out that the individual contribution of a respondent may still be disclosed, even though the upper bound of the feasibility interval is well away from the value of the sensitive cell to which this respondent contributes. The most prominent case is that of two cells with only one single contributor (a.k.a. <em>‘singletons’</em>) within the same aggregation. No matter how large the cell values, because they both know their own contribution of course, both can use this additional knowledge to disclose the other’s contribution. For an analytical discussion of these issues see Cox(2001). In principle, a suppression pattern should not be considered as valid, when certain respondents can use their insider knowledge (on their own contribution to a cell) to disclose individual contributions of other respondents.</p>
</div>
</div>
</div>
<p>The problem of finding an optimum set of suppressions known as the ‘secondary cell suppression problem’ is to find a valid set of secondary suppressions with a minimum loss of information connected to it. For a mathematical statement of the secondary cell suppression problem see <em>e.g.</em> Fischetti and Salazar (2000).</p>
<p>However, cell suppression is not the only option to protect magnitude tables. As one alternative to cell suppression, <em>‘Partial Suppression’</em> has been suggested in Salazar (2003). The partial suppression problem is to find a valid suppression pattern, where the size of the feasibility intervals is minimal. The idea of the partial suppression method is to publish the feasibility intervals of the resulting suppression pattern. Because there is a perception that the majority of users of the publications prefer the statistical agencies to provide actual figures rather than intervals, and partial suppression tends to affect more cells than cell suppression, this method has not yet been implemented.</p>
<p>Another alternative method for magnitude table protection is known as <em>‘Controlled Tabular Adjustment’</em> (CTA) suggested for instance in Cox and Dandekar (2002), Castro (2003), or Castro and Giessing (2006). CTA methods attempt to find the closest table consistent with the constraints imposed by the set of table equations and by the protection levels for the sensitive cells, of course taking into account also bounds on cell values available to data users in advance of publication, like non-negativity. This adjusted table would then be released instead of the original table.</p>
</section>
</section>
<section id="sec-tau-argus-suppression" class="level2 page-columns page-full" data-number="4.3">
<h2 data-number="4.3" class="anchored" data-anchor-id="sec-tau-argus-suppression"><span class="header-section-number">4.3</span> The <span class="math inline">\(\tau\)</span>‑ARGUS Implementation of Cell Suppression</h2>
<p>The software package <span class="math inline">\(\tau\)</span>‑ARGUS provides software tools for disclosure protection methods for tabular data. This can be achieved by modifying the table so that it contains less, or less detailed, information. <span class="math inline">\(\tau\)</span>‑ARGUS allows for several modifications of a table: a table can be redesigned, meaning that rows and columns can be combined; sensitive cells can be suppressed and additional cells to protect these can be found in some optimal way (secondary cell suppression). Several alternative algorithms for the selection of secondary suppressions are available in <span class="math inline">\(\tau\)</span>‑ARGUS, <em>e.g.</em> the Hypercube/GHMiter method, a Modular and a Full optimal solution, and a method based on a network flow algorithm. Instead of cell suppression one could also use other methods for disclosure control of tables. One of these alternative methods is Controlled Rounding. However, use of Controlled Rounding is more common for frequency tables.</p>
<div id="fig-tau-argus-overview" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-tau-argus-overview-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Images/media/tau_argus_overview.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-tau-argus-overview-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4.1: Overview of the <span class="math inline">\(\tau\)</span>‑ARGUS architecture
</figcaption>
</figure>
</div>
<p>We start this section by introducing the basic issues related to the question of how to properly set up a tabular data protection problem for a given set of tables that are intended to be published, discussing tabular data structures, and particularly tables with hierarchical structures. <a href="#sec-evaluation-of-secondary-cell-suppression-algorithms-offered-by-tau-argus" class="quarto-xref"><span>Section 4.3.2</span></a> provides an evaluation of algorithms for secondary cell suppression offered by <span class="math inline">\(\tau\)</span>-ARGUS, including a recommendation on which tool to use in which situation. After that, in <a href="#sec-efficient-protection-process" class="quarto-xref"><span>Section 4.3.3</span></a> we give some guidance on processing table protection efficiently, especially in the case of linked tables. We finish the section with an introductive example in <a href="#sec-introductive-example" class="quarto-xref"><span>Section 4.3.4</span></a>.</p>
<p>The latest version of <span class="math inline">\(\tau\)</span>-ARGUS can be found on GitHub (<a href="https://github.com/sdcTools/tauargus/releases">https://github.com/sdcTools/tauargus/releases</a>).</p>
<section id="sec-setting-up-a-tabular-data-protection-problem-in-practice" class="level3 page-columns page-full" data-number="4.3.1">
<h3 data-number="4.3.1" class="anchored" data-anchor-id="sec-setting-up-a-tabular-data-protection-problem-in-practice"><span class="header-section-number">4.3.1</span> Setting up a Tabular Data Protection Problem in Practice</h3>
<p>At the beginning of this section we explain the basic concept of tables in the context of tabular data protection, and complex structures like hierarchical and linked tables. In order to find a good balance between protection of individual response data and provision of information – in other words, to take control of the loss of information that obviously cannot be avoided completely because of the requirements of disclosure control, it is necessary to somehow rate the information loss connected to a cell that is suppressed. We therefore complete this section by explaining the concept of cell costs to rate information loss.</p>
<p><br>
<strong><em>Table specification</em></strong><br>
For setting up the mathematical formulation of the protection problems connected to the release of tabulations for a certain data set, all the linear relations between the cells of those tabulations have to be considered. This leads us to a crucial question: What is a table anyway? In the absence of confidentiality concerns, a statistician creates a table in order to show certain properties of a data set, or to enhance comparison between different variables. So a single table might literally mix apples and oranges. Secondly, statisticians may wish to present a number of those ‘properties’, publishing multiple tables from a particular data set. Where does one table end and the next start? Is the ideal table one that fits nicely on a standard-size sheet of paper? With respect to tabular data protection, we have to think of tables in a different way:</p>
<p>Magnitude tables display sums of observations of a quantitative variable, the so-called ‘<em>response variable’</em>. The sums are displayed across all observations and/or within groups of observations. These groups can be formed by grouping the respondents according to certain criteria such as their economic activity, region, size class of turnover, legal form, etc. In that case the grouping of observations is defined by categorical variables observed for each individual respondent, the ’explanatory variables’. It also occurs that observations are grouped by categories of the response variable, for instance fruit production into apples, pears, cherries, etc.</p>
<p>The “<em>dimension</em>” of a table is given by the number of grouping variables used to specify the table. “<em>Margins</em>” or “<em>marginal cells</em>” of a table are those cells, which are specified by a smaller number of grouping variables. The smaller the number of grouping variables, the higher the “<em>level</em>” of a marginal cell. A two-dimensional table of some business survey may for instance provide sums of observations grouped by economic activity and company size classes. At the same time it may also display the sums of observations grouped by only economic activity or by only size classes. These are then margins/marginal cells of this table. If a sum across all observations is provided, we refer to it as the “<em>total</em>” or “<em>overall total</em>”.</p>
<p>Note that tables presenting ratios or indexes typically do not define a tabular data protection problem, because there are no additive structures between the cells of such a table, and neither between a cell value, and the values of the contributing units. Think, for instance, of mean wages, where the cell values would be a sum of wages divided by a sum over the number of employees of several companies. However, there may be exceptions: if, for instance, it is realistic to assume that the denominators (say, the number of employees) can be estimated quite closely, both on the company level and on the cell level, then it might indeed be adequate to apply disclosure control methods based, for example, on the enumerator variable (in our instance: the wages).</p>
<p><br>
<strong><em>Hierachical, linked tables</em></strong><br>
Data collected within government statistical systems must meet the requirements of many users, who differ widely in the particular interest they take in the data. Some may need community-level data, while others need detailed data on a particular branch of the economy but no regional detail. As statisticians, we try to cope with this range of interest in our data by providing the data at several levels of detail. We usually combine explanatory variables in multiple ways, when creating tables for publication. If two tables presenting data on the same response variable share some categories of at least one explanatory variable, there will be cells which are presented in both tables – those tables are said to be <em>linked</em> by the cells they have in common. In order to offer a range of statistical detail, we use elaborate classification schemes to categorize respondents. Thus, a respondent will often belong to various categories of the same classification scheme ‑ for instance a particular community within a particular county within a particular state ‑ and may thus fall into three categories of the regional classification.</p>
<p>The structure between the categories of hierarchical variables also implies sub-structure for the table. When, in the following, we talk about sub-tables without substructure, we mean a table constructed in the following way:</p>
<p>For any explanatory variable we pick one particular non-bottom-level category (the ‘food production sector’ for instance). Then we construct a ‘sub-variable’. This sub-variable consists only of the category picked in the first step and those categories of the level immediately below belonging to this category (bakers, butchers, etc.). After doing that for each explanatory variable the table specified through a set of these sub-variables is free from substructure then, and is a sub-table of the original one. Any cell within the sub-table does also belong to the original table. Many cells of the original table will appear in more than one sub-table: The sub-tables are linked. Example 6 provides a simple instance of a one-dimensional table with hierarchical structure.</p>
<div class="callout callout-style-simple callout-note">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-body-container">
<p><strong>Example 6</strong> Assume the categories A, AA, AB, AB1, AB2, and AB3 of some variable EXP resemble a hierarchical structure as depicted in <a href="#fig-hierarchical-structure" class="quarto-xref">Figure&nbsp;<span>4.2</span></a> below:</p>
<div id="fig-hierarchical-structure" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-hierarchical-structure-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Images/media/chap4_hierarchical_structure.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-hierarchical-structure-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4.2: Hierarchical structure
</figcaption>
</figure>
</div>
<p>Let the full table present turnover by the categories of EXP. Then the two subtables of this table will be:</p>
<div>

</div>
<div class="quarto-layout-panel" data-layout="[[-1,1,-1,1,-1], [1]]">
<div class="quarto-layout-row">
<div class="quarto-figure-spacer quarto-layout-cell" style="flex-basis: 20.0%;justify-content: flex-start;">
<p>&nbsp;</p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 20.0%;justify-content: center;">
<table class="table">
<tbody>
<tr class="odd">
<td style="text-align: left;">EXP</td>
<td style="text-align: left;"></td>
<td style="text-align: left;">Turnover</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>A</strong></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;"></td>
<td style="text-align: left;"><strong>AA</strong></td>
<td style="text-align: left;"></td>
</tr>
<tr class="even">
<td style="text-align: left;"></td>
<td style="text-align: left;"><strong>AB</strong></td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
</div>
<div class="quarto-figure-spacer quarto-layout-cell" style="flex-basis: 20.0%;justify-content: flex-start;">
<p>&nbsp;</p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 20.0%;justify-content: center;">
<table class="table">
<tbody>
<tr class="odd">
<td style="text-align: left;">EXP</td>
<td style="text-align: left;"></td>
<td style="text-align: left;">Turnover</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>AB</strong></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;"></td>
<td style="text-align: left;"><strong>AB1</strong></td>
<td style="text-align: left;"></td>
</tr>
<tr class="even">
<td style="text-align: left;"></td>
<td style="text-align: left;"><strong>AB2</strong></td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
</div>
<div class="quarto-figure-spacer quarto-layout-cell" style="flex-basis: 20.0%;justify-content: flex-start;">
<p>&nbsp;</p>
</div>
</div>
</div>
<p>Note that cell <strong>AB</strong> appears in both subtables</p>
</div>
</div>
</div>
<p>Of course, when using cell suppression methods to protect a set of linked tables, or sub-tables, it is not enough to select secondary suppressions for each table (or sub-table) separately. Otherwise it might for instance happen that the same cell is suppressed in one table because it is used as secondary suppression, while within another table it remains unsuppressed. A user comparing the two tables would then be able to disclose confidential cells in the first table. A common approach is to protect tables separately, but note any complementary suppression belonging also to one of the other tables; suppress it in this table as well, and repeat the cell suppression procedure for this table. This approach is called a ‘<em>backtracking procedure</em>’. Although within a backtracking process for a hierarchical table the cell-suppression procedure will usually be repeated several times for each sub-table, the number of computations required for the process will be much smaller than when the entire table is protected all at once.</p>
<p>It must however be stressed, that a backtracking procedure is not global according to the denotation in Cox (2001). See Cox (2001) for discussion of problems related to non-global methods for secondary cell suppression.</p>
<p>For mathematical formulation of linked tables structures see de Wolf (2007) and de Wolf, Giessing (2008).</p>
<p><br>
<strong><em>Cell costs</em></strong><br>
The challenge of tabular data protection is to preserve as much information in the table as possible, while creating the required uncertainty about the true values of the sensitive cells, as explained in the previous section. It is necessary to somehow rate the information content of data, in order to be able to express the task of selecting an ‘optimal set’ of secondary suppressions, or, alternatively, of adjusting a table in an optimal way, as mathematical programming problem. Information loss is expressed in these mathematical models as the sum of costs associated to the secondary suppressions, or non-sensitive cells subject to adjustment.</p>
<p>For cell suppression, the idea of equating a minimum loss of information with the smallest number of suppressions is probably the most natural concept. This would be implemented technically by assigning identical costs to each cell. Yet experience has shown that this concept often yields a suppression pattern in which many large cells are suppressed, which is undesirable. In practice other cost functions, based on cell values, or power transformations thereof, or cell frequencies yield better results. Note that several criteria, other than the numeric value, may also have an impact on a user’s perception of a particular cells importance, such as its situation within the table (totals and sub-totals are often rated as highly important), or its category (certain categories of variables are often considered to be of secondary importance). <span class="math inline">\(\tau\)</span>-ARGUS offers special facilities to choose a cost function and to carry out power transformations of the cost function:</p>
<p><br>
</p>
<p><strong><em>Cost function</em></strong><br>
The cost function indicates the relative importance of a cell. In principle the user could attach a separate cost to each individual cell, but that will become a rather cumbersome operation. Therefore in <span class="math inline">\(\tau\)</span>‑ARGUS a few standard cost functions are available<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>:</p>
<div class="no-row-height column-margin column-container"><div id="fn2"><p><sup>2</sup>&nbsp;One of the secondary cell suppression methods of <span class="math inline">\(\tau\)</span>-ARGUS, the hypercube method, uses an elaborate internal weighting scheme in order to avoid to some extent the suppression of totals and subtotals which is essential for the performance of the method. In order to avoid that this internal weighting scheme is thrown out of balance, it has been decided to make the cost function facilities of t-ARGUS inoperational for this method.</p></div></div><ul>
<li>Unity, <em>i.e.</em> all cells have equal weight (just minimising the number of suppressions)</li>
<li>The number of contributors (minimising the number of contributors to be hidden)</li>
<li>The cell value itself (preserving as much as possible the largest – usually most important – cells)</li>
<li>The cell value of another variable in the dataset (this will be the indication of the important cells).</li>
</ul>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Expert level
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Especially in the latter two cases the cell value could give too much preference to the larger cells. Is a 10 times bigger cell really 10 times as important? Sometimes there is a need to be a bit more moderate. Some transformation of the cost function is desired. Popular transformations are square-root or a log-transformation. Box and Cox (1960) have proposed a system of power transformations:</p>
<p><span class="math display">\[
y = \frac{x^{\lambda} -1}{\lambda}
\]</span></p>
<p>In this formula <span class="math inline">\(\lambda = 0\)</span> yields to a log-transformation and the <span class="math inline">\(-1\)</span> is needed for making this formula continuous in <span class="math inline">\(\lambda\)</span>.</p>
<p>This last aspect and the fact that this could lead to negative values we have introduced a simplified version of the lamda-transformation in <span class="math inline">\(\tau\)</span>‑ARGUS</p>
<p><span class="math display">\[
y = \begin{cases}
x^{\lambda} \quad &amp;\text{for }\lambda\neq0 \\
\text{log}(x)\quad &amp;\text{for }\lambda=0
\end{cases}
\]</span></p>
<p>So choosing <span class="math inline">\(\lambda = \frac{1}{2}\)</span> will give the square-root.</p>
</div>
</div>
</div>
</section>
<section id="sec-evaluation-of-secondary-cell-suppression-algorithms-offered-by-tau-argus" class="level3 page-columns page-full" data-number="4.3.2">
<h3 data-number="4.3.2" class="anchored" data-anchor-id="sec-evaluation-of-secondary-cell-suppression-algorithms-offered-by-tau-argus"><span class="header-section-number">4.3.2</span> Evaluation of Secondary Cell Suppression algorithms offered by <span class="math inline">\(\tau\)</span>‑ARGUS</h3>
<p>The software package <span class="math inline">\(\tau\)</span>−ARGUS offers a variety of algorithms to assign secondary suppressions which will be described in detail in <a href="#sec-methodology-secondary-suppression-tau-argus" class="quarto-xref"><span>Section 4.4</span></a>. While some of those algorithms are not yet fully developed, or not yet fully integrated into the package, respectively, two are indeed powerful tools for automated tabular data protection. In this section we focus on those latter algorithms. We compare them with respect to quality aspects of the protected tables, while considering also some practical issues. Based on this comparison, we finally give some recommendation on the use of those algorithms.</p>
<p>We begin this section by introduction of certain minimum protection standards PS1, and PS2. We explain why it is a ‘must’ for an algorithm to satisfy those standards in order to be useable in practice. For those algorithms that qualify ‘useable’ according to these criteria, we report some results of studies comparing algorithm performance with regard to information loss, and disclosure risk.</p>
<p>In <a href="#sec-secondary-protection-methods" class="quarto-xref"><span>Section 4.2.2</span></a> we have defined criteria for a safe suppression pattern. It is of course possible to design algorithms that will ensure that no suppression pattern is considered feasible unless all of these criteria are satisfied. In practice, however, such an algorithm would either require too much computer resource (in particular: CPU time) when applied to the large, detailed tabulations of official statistics, or the user would have to put up with a strong tendency for oversuppression. In the following, we therefore define minimum standards for a suppression pattern that is safe w.r.t. the safety rule adopted except for a residual disclosure risk that might be acceptable to an agency. It should be noted that those standards are definitely superior, or at least equal to what could be achieved by the traditional manual procedures for cell suppression, even if carried out with utmost care! For the evaluation, we consider only those algorithms that ensure validity of the suppression pattern at least with respect to those relaxed standards.</p>
<p><br>
<strong><em>Minimum protection standards</em></strong><br>
For a table with hierarchical substructure, feasibility intervals computed on basis of the set of equations for the full table normally tend to be much closer than those computed on basis of separate sets of equations corresponding to sub-tables without any substructure. Moreover, making use of the additional knowledge of a respondent, who is the single respondent to a cell (a so called <em>‘singleton’</em>), it is possible to derive intervals that are much closer than without this knowledge<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a>.</p>
<div class="no-row-height column-margin column-container"><div id="fn3"><p><sup>3</sup>&nbsp;Consider for instance the table of example 3 in <a href="#sec-secondary-protection-methods" class="quarto-xref"><span>Section 4.2.2</span></a>. Without additional knowledge the bounds for cell (1,1) are <span class="math inline">\(X_{11}^\text{min} = 3\)</span> and <span class="math inline">\(X_{11}\text{max} = 6\)</span>. However, if cell (1,2) were a singleton-cell (a cell with only a single contributor) then this single contributor could of course exactly compute the cell value of cell (1,1) by subtracting her own contribution from the row total of row 1.</p></div></div><p>Based on the assumption of a simple but probably more realistic disclosure scenario where intruders will deduce feasibility intervals separately for each sub-table, rather than taking the effort to consider the full table, and that single respondents, using their special additional knowledge, are more likely attempting to reveal other suppressed cell values when they are within the same row or column (more general: relation), the following minimum protection standards (PS) make sense:</p>
<blockquote class="blockquote">
<p>(PS1) <em>Protection against exact external disclosure</em>:<br>
With a valid suppression pattern, it is not possible to disclose the value of a sensitive cell exactly, if no additional knowledge (like that of a singleton) is considered, and if subsets of table equations are considered separately, <em>i.e.</em> when feasibility intervals are computed separately for each sub-table.</p>
<p>(PS2) <em>Protection against singleton disclosure</em>:<br>
A suppression pattern, with only two suppressed cells within a row (or column) of a table is not valid, if each of the two corresponds to a single respondent who are not identical.</p>
<p>(PS1*) extension of (PS1) for inferential (instead of exact) disclosure,</p>
<p>(PS2*) extension of (PS2), covering the more general case where a single respondent can disclose another single respondent cell, not necessarily located within the same row (or column) of the table.</p>
</blockquote>
<p><span class="math inline">\(\tau\)</span>-ARGUS offers basically two algorithms, Modular and Hypercube, both satisfying the above-mentioned minimum standards regarding disclosure risk. Both take a backtracking approach to protect hierarchical tables within an iterative procedure: they subdivide hierarchical tables into sets of linked, unstructured tables. The cell suppression problem is solved for each subtable separately. Secondary suppressions are coordinated for overlapping subtables.</p>
<p>The Modular approach, a.k.a. HiTaS algorithm (see de Wolf, 2002), is based on the optimal solution as implemented by Fischetti and Salazar-González. By breaking down the hierarchical table in smaller non-structured subtables, protecting these subtables and linking the subtables together by a backtracking procedure the whole table is protected. For a description of the modular approach see <a href="#sec-modular" class="quarto-xref"><span>Section 4.4.2</span></a>, the optimal Fischetti/Salazar-González method is described in section <a href="#sec-optimal" class="quarto-xref"><span>Section 4.4.1</span></a>. See also Fischetti and Salazar-González (2000).</p>
<p>The Hypercube is based on a hypercube heuristic implemented by Repsilber (2002). See also the descriptions of this method in <a href="#sec-hypercube" class="quarto-xref"><span>Section 4.4.4</span></a>.</p>
<p>Note that in the current implementation of <span class="math inline">\(\tau\)</span>-ARGUS, use of the optimization tools requires a license for additional commercial software (LP-solver), whereas use of the hypercube method is free. While Modular is only available for the Windows platform, of Hypercube there are also versions for Unix, IBM (OS 390) and SNI (BS2000).</p>
<p>Both Modular and Hypercube provide sufficient protection according to standards PS1* (protection against inferential disclosure) and PS2 above. Regarding singleton disclosure, Hypercube even satisfies the extended standard PS2*. However, simplifications of the heuristic approach of Hypercube cause a certain tendency for oversuppression. This evaluation therefore includes a relaxed variant that, instead of PS1*, satisfies only the reduced standard PS1, <em>i.e.</em> zero protection against inferential disclosure. We therefore refer to this method as Hyper0 in the following. Hyper0 processing can be achieved simply by deactivating the option “Protection against inferential disclosure required” when running Hypercube out of <span class="math inline">\(\tau\)</span>‑ARGUS.</p>
<div id="tbl-algorithms" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<div aria-describedby="tbl-algorithms-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="table">
<colgroup>
<col style="width: 17%">
<col style="width: 22%">
<col style="width: 26%">
<col style="width: 16%">
<col style="width: 13%">
</colgroup>
<tbody>
<tr class="odd">
<td colspan="2" style="text-align: left;"><strong>Algorithm</strong></td>
<td style="text-align: left;"><strong>Modular</strong></td>
<td style="text-align: left;"><strong>Hypercube</strong></td>
<td><strong>Hyper0</strong></td>
</tr>
<tr class="even">
<td colspan="2" style="text-align: left;"><strong>Procedure for secondary</strong> <strong>suppression</strong></td>
<td style="text-align: left;"><strong>Fischetti/Salazar</strong> <strong>optimization</strong></td>
<td colspan="2" style="text-align: left;"><strong>Hypercube</strong> <strong>heuristic</strong></td>
</tr>
<tr class="odd">
<td rowspan="2" style="text-align: left;"><strong>Protection</strong> <strong>standard</strong></td>
<td style="text-align: left;"><strong>Interval/Exact</strong> <strong>disclosure</strong></td>
<td style="text-align: left;">PS1*</td>
<td style="text-align: left;">PS1*</td>
<td>PS1</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Singleton</strong></td>
<td style="text-align: left;">PS2</td>
<td style="text-align: left;">PS2*</td>
<td>PS2*</td>
</tr>
</tbody>
</table>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-tbl" id="tbl-algorithms-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;4.5: Algorithms for practical use
</figcaption>
</figure>
</div>
<p>In the following, we briefly report results of two evaluations studies, in the following referred to as study A, and study B. For further detail on these studies see Giessing (2004) for study A, and Giessing et al.&nbsp;(2006) for study B. Both studies are based on tabulations of a strongly skewed magnitude variable.</p>
<p>For study A, we used a variety of hierarchical tables generated from the synthetic micro‑data set supplied as CASC deliverable 3-D3. In particular, we generated 2- and 3-dimensional tables, where one of the dimensions had a hierarchical structure. Manipulation of the depth of this hierarchy resulted in 7 different tables, with a total number of cells varying between 460 and 150,000 cells. Note that the Hyper0 method was not included in study A.</p>
<p>For study B, which was carried out on behalf of the German federal and state statistical offices, we used 2-dimensional tables of the German turnover tax statistics. Results have been derived for tables with 7-level hierarchical structure (given by the NACE economy classification) of the first dimension, and 4‑level structure of the second dimension given by the variable Region.</p>
<p>We compare the loss of information due to secondary suppression in terms of the number and cell values of secondary suppression, and analyze the disclosure risk of the protected tables. <a href="#tbl-results-study-A" class="quarto-xref">Table&nbsp;<span>4.6</span></a> reports the results regarding the loss of information obtained in study A.</p>
<div id="tbl-results-study-A" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<div aria-describedby="tbl-results-study-A-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="table">
<colgroup>
<col style="width: 9%">
<col style="width: 9%">
<col style="width: 16%">
<col style="width: 16%">
<col style="width: 13%">
<col style="width: 16%">
<col style="width: 13%">
</colgroup>
<thead>
<tr class="header">
<th colspan="2" rowspan="2">Table Hier. levels</th>
<th rowspan="2">No.&nbsp;Cells</th>
<th colspan="2">No.&nbsp;Suppressions (%)</th>
<th colspan="2">Added Value of Suppressions (%)</th>
</tr>
<tr class="odd">
<th>Hypercube</th>
<th>Modular</th>
<th>Hypercube</th>
<th>Modular</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td colspan="7"><strong>2-dimensional tables</strong></td>
</tr>
<tr class="even">
<td>1</td>
<td>3</td>
<td>460</td>
<td>6.96</td>
<td>4.35</td>
<td>0.18</td>
<td>0.05</td>
</tr>
<tr class="odd">
<td>2</td>
<td>4</td>
<td>1,050</td>
<td>10.95</td>
<td>8.29</td>
<td>0.98</td>
<td>0.62</td>
</tr>
<tr class="even">
<td>3</td>
<td>6</td>
<td>8,230</td>
<td>14.92</td>
<td>11.48</td>
<td>6.78</td>
<td>1.51</td>
</tr>
<tr class="odd">
<td>4</td>
<td>7</td>
<td>16,530</td>
<td>14.97</td>
<td>11.13</td>
<td>8.24</td>
<td>2.12</td>
</tr>
<tr class="even">
<td colspan="7"><strong>3-dimensional tables</strong></td>
</tr>
<tr class="odd">
<td>5</td>
<td>3</td>
<td>8,280</td>
<td>14.63</td>
<td>10.72</td>
<td>6.92</td>
<td>1.41</td>
</tr>
<tr class="even">
<td>6</td>
<td>4</td>
<td>18,900</td>
<td>17.31</td>
<td>15.41</td>
<td>12.57</td>
<td>3.55</td>
</tr>
<tr class="odd">
<td>7</td>
<td>6</td>
<td>148,140</td>
<td>15.99</td>
<td>10.63</td>
<td>23.16</td>
<td>3.91</td>
</tr>
</tbody>
</table>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-tbl" id="tbl-results-study-A-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;4.6: Results of study A - Number and value of secondary suppressions
</figcaption>
</figure>
</div>
<p><a href="#tbl-res-B" class="quarto-xref">Table&nbsp;<span>4.7</span></a> presents information loss results from study B on the two topmost levels of the second dimension variable Region, <em>e.g.</em> on the national, and state level.</p>
<div id="tbl-res-B" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<div aria-describedby="tbl-res-B-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="table">
<colgroup>
<col style="width: 25%">
<col style="width: 11%">
<col style="width: 9%">
<col style="width: 12%">
<col style="width: 11%">
<col style="width: 19%">
</colgroup>
<thead>
<tr class="header">
<th rowspan="3">Algorithm for secondary suppression</th>
<th colspan="4">Number</th>
<th>Added Value</th>
</tr>
<tr class="odd">
<th colspan="2">State level</th>
<th colspan="2">National level</th>
<th>overall</th>
</tr>
<tr class="header">
<th>abs</th>
<th>%</th>
<th>abs</th>
<th>%</th>
<th>%</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Modular</strong></td>
<td>1,675</td>
<td>10.0</td>
<td>7</td>
<td>0.6</td>
<td>2.73</td>
</tr>
<tr class="even">
<td><strong>Hyper0</strong></td>
<td>2,369</td>
<td>14.1</td>
<td>8</td>
<td>0.7</td>
<td>2.40</td>
</tr>
<tr class="odd">
<td><strong>Hypercube</strong></td>
<td>2,930</td>
<td>17.4</td>
<td>22</td>
<td>1.9</td>
<td>5.69</td>
</tr>
</tbody>
</table>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-tbl" id="tbl-res-B-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;4.7: Results of study B - Number and value of secondary suppressions
</figcaption>
</figure>
</div>
<p>Both studies show that best results are achieved by the modular method, while using either variant of the hypercube method leads to an often quite considerable increase in the amount of secondary suppression compared to the result of the modular method. The size of this increase varies between hierarchical levels of the aggregation: on the higher levels of a table the increase tends to be even larger than on the lower levels. Considering the results of study B presented in <a href="#tbl-res-B" class="quarto-xref">Table&nbsp;<span>4.7</span></a>, on the national level we observe an increase of about 214% for Hypercube (14% for Hyper0), compared to an increase of 75% (41%) on the state level. In Giessing et al.&nbsp;(2006) we report also results on the district level. On that level the increase was about 28% on the higher NACE levels for Hypercube (9% for Hyper0), and 20 % for Hypercube (14% for Hyper0) on the lower levels. In study A we observed increases mostly around 30% for Hypercube, for the smallest 2‑di­mensional, and the largest 3-dimensional instance even of 50% and 60%, respectively. In particular, our impression is that the hypercube method tends to behave especially badly (compared to Modular), when tabulations involve many short equations, with only a few positions.</p>
<p>A first conclusion from the results of study B reported above was, because of the massive oversuppression, to exclude the Hypercube method in the variant that prevents inferential disclosure from any further experimentation. Although clearly only second-best performer, testing with Hyper0 was to be continued, because of technical and cost advantages mentioned above.</p>
<p>In experiments with state tables where the variable Region was taken down to the community level, we found that this additional detail in the table caused an increase in suppression at the district level of about 70 – 80% with Hyper0 and about 10 – 30% with Modular.</p>
<p><br>
<strong><em>Disclosure risk</em></strong><br>
As explained in <a href="#sec-secondary-protection-methods" class="quarto-xref"><span>Section 4.2.2</span></a>, in principle, there is a risk of inferential disclosure, when the bounds of the feasibility interval of a sensitive cell could be used to deduce bounds on an individual respondent’s contribution that are too close according to the method employed for primary disclosure risk assessment. However, for large, complex structured tables, this risk is rather a risk potential, not comparable to the disclosure risk that would result from a publication of that cell. After all, the effort connected to the computation of feasibility intervals based on the full set of table equations is quite considerable. Moreover, in a part of the disclosure risk cases only insiders (other respondents) would actually be able to disclose the individual contribution.</p>
<p>Anyway, with this definition, and considering the full set of table equations, in study B we found between about 4% (protection by Modular) and about 6% (protection by Hyper0)&nbsp;of sensitive cells at risk in an audit step where we computed the feasibility intervals for a tabulation by NACE and state, and for two more detailed tables (down to the district level) audited separately for two of the states. In study A, we found up to about 5.5% (1.3%) cells at risk for the 2-dimensional tables protected by Modular (Hypercube, resp.), and for the 3-dimensional tables up to 2.6% (Modular) and 5.6% (Hypercube).</p>
<p>Of course, the risk potential is much higher for at-risk cells when the audit is carried out separately for each subtable without substructure, because the effort for this kind of analysis is much lower. Because Modular protects according to PS1* standard, there are no such cells in tables protected by Modular. For Hyper0, in study B we found about 0.4% cells at risk, when computing feasibility intervals for several subtables of a detailed tabulation.</p>
<p>For those subtables, when protected by Modular, for about 0.08% of the singletons, it turned out that another singleton would be able to disclose its contribution. Hyper0 satisfies PS2* standard, therefore of course no such case was found, when the table was protected by Hyper0.</p>
<p><br>
<strong><em>Recommendation</em></strong><br>
The methods Modular and Hypercube of <span class="math inline">\(\tau\)</span>‑ARGUS are powerful tools for secondary cell suppression. Concerning protection against external disclosure both satisfy the same protection standard. However, Modular gives much better results regarding information loss. Even compared to a variant of Hypercube (Hyper0) with relaxed protection standard, Modular performs clearly better. Although longer computation times for this method (compared to the fast hypercube method) can be a nuisance in practice, the results clearly justify this additional effort – after all, it is possible, for instance, to protect a table with detailed hierarchical structure in two dimensions and more than 800,000 cells within just about an hour. Considering the total costs involved in processing a survey, it would neither be justified to use Hypercube (or Hyper0) only in order to avoid the costs for the commercial license which is necessary to run the optimization tools of Modular.</p>
<p>We also recommend use of Modular to assign secondary suppressions in 3-dimensional tables. However, when those tables are given by huge classifications, long computation times may become an actual obstacle. It took us, for instance, about 11 hours to run a 823 x 18 x 10 table (first dimension hierarchical). The same table was protected by Hyper0 within about 2 minutes. Unfortunately, replacing Modular by Hyper0 in that instance leads to about 28% increase in the number of secondary suppressions.</p>
<p>According to our findings, we would expect the Hypercube method (<em>i.e.</em> Hyper0) to be good alternative to Modular, if</p>
<ul>
<li>Tabulations involve more than 2 dimensions and are very large, and</li>
<li>the majority of table equations (<em>e.g.</em> rows, columns, …) are long, involving many positions, and</li>
<li>the distribution of the variable which is tabulated is rather even, not skewed, resulting in a lower rate of sensitive cells, and</li>
<li>protection against inferential disclosure is not an important issue, for instance when a minimum frequency rule instead of a concentration rule is employed to assess the primary disclosure risk.</li>
</ul>
<p>In the next section we will give some hints on what to do, if according to these conditions use of the Modular method seems to not be an option, because table sizes are too large, in order to avoid being confined to the use of Hyper or Hyper0.</p>
<p>Concerning protection against singleton (insider) disclosure, the Hypercube method fulfils a higher standard. However, our empirical results confirm that in practice there are very few singleton cells left unprotected by Modular against this kind of disclosure risk. We are therefore confident that the problem can be fixed easily by a specialized audit step that would not even involve linear programming methods.</p>
</section>
<section id="sec-efficient-protection-process" class="level3" data-number="4.3.3">
<h3 data-number="4.3.3" class="anchored" data-anchor-id="sec-efficient-protection-process"><span class="header-section-number">4.3.3</span> Processing table protection efficiently</h3>
<p>Within this section we give some guidance for how to facilitate disclosure analysis, and how to use the methods of <span class="math inline">\(\tau\)</span>-ARGUS in an efficient way. For more tips and tricks to improve practical performance, see van der Meijden (2006).</p>
<p>Especially in large tables with detailed hierarchical structure we have observed that the choice of the parameter lambda (<span class="math inline">\(\lambda\)</span>) may substantially affect the suppression pattern – and the results may be unexpected. Normally we would expect that constant cell costs will lead to a solution with a (relatively) small number of secondary suppressions. In hierarchical tables, however, it may turn out that given this cost function the Modular method tends to suppress cells in the margins of subtables rather than inner cells which in turn require additional suppressions in a (linked) subtable. This effect may be so strong that in the end, a <span class="math inline">\(\lambda\)</span> close to zero yielding approximately constant costs may lead to a larger number of suppressions as if a larger <span class="math inline">\(\lambda\)</span> (say: 0.5) had been chosen. For larger tables it makes therefore sense to test the outcome with different values of <span class="math inline">\(\lambda\)</span>.</p>
<p><br>
<strong><em>Table design</em></strong><br>
In <a href="#sec-setting-up-a-tabular-data-protection-problem-in-practice" class="quarto-xref"><span>Section 4.3.1</span></a> we already discussed how to set up tables for tabular data protection. If the concept for the publication is not yet fixed, some table-redesign may help to reduce the complexity of the problem.</p>
<p><br>
<strong><em>Table redesign</em></strong><br>
If a large number of sensitive cells are present in a table, it might be an indication that the spanning variables are too detailed. In that case one could consider combining certain rows and columns in the table. (This might not always be possible because of publication policy.) Otherwise the number of secondary cell suppressions might just be too enormous. It is a property of the sensitivity rules that a joint cell is safer than any of the individual cells. So as a result of this operation the number of unsafe cells is reduced. One can try to eliminate all unsafe combinations in this way, but that might lead to an unacceptably high information loss. Instead, one could stop at some point, and eliminate the remaining unsafe combinations by using other techniques such as cell suppression.</p>
<p>In practice, it may also often be considered impractical and also not adequate to do the disclosure analysis for each table of the entire publication separately. A well-known simplification is to focus the analysis on certain lead variables. These lead tables are protected with <span class="math inline">\(\tau\)</span>‑ARGUS as described above and the pattern found is then used for all the other tables. This strategy has the advantage that it is efficient and prevents the recalculation of suppressed cells by using the inherent relations between the different tables in the publication. The fact that for certain cells in the other tables not always all cells that should be considered unsafe according to the concentration rules will actually be considered unsafe, is considered a minor problem.</p>
<p>A compromise between the copying of the suppression pattern and protecting each table individually is the coordination of the primary suppressions only. This strategy is supported by the concept of shadow variables in <span class="math inline">\(\tau\)</span>‑ARGUS. The shadow variable is used for the lead-table and only for finding the primary unsafe cells. After that the pattern is used for each individual table itself and the secondary cell suppression is done in the traditional way.</p>
<p><br>
<strong><em>Shadow variables</em></strong><br>
The whole theory of primary unsafe cells is based on the idea that certain larger contributors in a cell might be at risk and need protection. The largest contributors therefore play an important role in the sensitivity rules.</p>
<p>Often the cell value itself is a very good estimate of the size of the contributors. But sometimes the cell value of a table is not always the best indicator of the size of the company. <em>E.g.</em> if the cell value is the investment in some commodity, the largest companies do not always have the largest values. Simply applying the sensitivity rules could yield a situation in which a rather small company will dominate the cell and force it to become unsafe. If we assume that this smaller company is not very well known and visible there is no reason to protect this cell.</p>
<p>If we want to protect the real large companies it could be a good idea to apply the sensitivity rules on another table (<em>shadow variable</em>) with the same spanning variables (<em>e.g.</em> turnover) and use the pattern of primary unsafe cells. Copy the pattern to the current table and compute the secondary unsafe cells in the normal way.</p>
<p>In <span class="math inline">\(\tau\)</span>‑ARGUS we call the table where the primary unsafe cells are computed the shadow table.</p>
<p>Another reason for using a shadow variable could be the coordination of the pattern in different tables with the same spanning variables based on one dataset. See the remarks on periodical datasets below.</p>
<p><br>
<strong><em>Ignore response variable relations</em></strong><br>
A commonly used strategy to simplify disclosure analysis is not to reflect the full linear relationship between published tabulation cells in the table protection procedure. A typical case, where this would be justified is the following: Assume we publish salaries of seasonal workers from foreign countries (<span class="math inline">\(S_{\text{foreigners}}\)</span>), and also salaries of all seasonal workers (<span class="math inline">\(S_{\text{S-W}}\)</span>), by region, for instance. Normally, for table protection we should consider the relation</p>
<p><span class="math display">\[
S_{\text{S-W}} = S_{\text{foreigners}} + S_{\text{non-foreigners}} \ , \qquad \text{(R1)}
\]</span></p>
<p>even if we are not interested in publishing <span class="math inline">\(S_{\text{non-foreigners}}\)</span> data, and may not even have collected them. Typically <span class="math inline">\(S_{\text{S-W}}\)</span> data would also appear in other relations, for instance together with data for permanent workers as</p>
<p><span class="math display">\[
S_{\text{all}}= S_{\text{S-W}} + S_{\text{permanent}}  \ , \qquad \text{(R2)}
\]</span></p>
<p>so we would have to protect a set of linked tables. From a practical point of view this is of course not desirable. We assume now that <span class="math inline">\(S_{\text{non-foreigners}}\)</span> figures are for all regions much larger as <span class="math inline">\(S_{\text{non-foreigners}}\)</span> figures and that the <span class="math inline">\(S_{\text{non-foreigners}}\)</span> are not published. Then the <span class="math inline">\(S_{\text{S-W}}\)</span> figures cannot be used to estimate a sensitive <span class="math inline">\(S_{\text{foreigners}}\)</span> cell value. It is thus not necessary to coordinate the suppression pattern for <span class="math inline">\(S_{\text{S-W}}\)</span> and <span class="math inline">\(S_{\text{foreigners}}\)</span>. This means that it is not necessary to carry out tabular data protection for the (R1) by Region tabulation. It is enough to protect two separate tables: <span class="math inline">\(S_{\text{S-W}}\)</span> or (R2) by Region, and <span class="math inline">\(S_{\text{foreigners}}\)</span> by Region.</p>
<p>This has been an instance of how to avoid a linked-tables structure. Those structures can, on the other hand, be very useful to make tabular data protection problems tractable in practice.</p>
<p><br>
<strong><em>Splitting detailed tables</em></strong><br>
Assume we have collected data on labour costs, including information on NACE category, Region, and Size class of the number of employees. We may want to release a 3-dimensional tabulation of labour costs by NACE, Region and Size Class. Looking at this table however, we may realize that while for a certain section (say: NACE<span class="math inline">\(_1\)</span>) of the economy this amount of detail may be adequate, for the others (NACE<span class="math inline">\(_2\)</span>, NACE<span class="math inline">\(_3\)</span>, <span class="math inline">\(\ldots\)</span>, NACE<span class="math inline">\(_n\)</span>) it is not: In those sections many of the lower-level cells are either empty, or sensitive. As pointed out at the end of the section on information loss of <a href="#sec-evaluation-of-secondary-cell-suppression-algorithms-offered-by-tau-argus" class="quarto-xref"><span>Section 4.3.2</span></a>, more detail in a tabulation generally leads to more suppressions on the higher levels. This kind of oversuppression can be avoided by taking a splitting approach, splitting the table into a set of linked tables: Instead of one table, we protect three: 1.) Labour costs by Region, Size Class and NACE<span class="math inline">\(_1\)</span>, 2.) Labour costs by Region and NACE, and 3.) Labour costs by size class and NACE. Even though we cannot then release any labour cost data for a particular NACE category, in a particular Region and in a particular Size Class unless the NACE category belongs to NACE<span class="math inline">\(_1\)</span>, the additional unsuppressed information gained by means of the splitting approach (as compared to the simple approach dealing with a single 3-dimensional table only) is usually rated much higher.</p>
<p><br>
<strong><em>Use of hierarchical structures</em></strong><br>
Especially when working with the Modular method for secondary cell suppression, using hierarchically structured variables usually helps a lot to reduce computational complexity, and hence computing time. Smaller numbers of categories per equation will make the program run faster. For the hypercube method, this is less of an issue. Here it is generally good to avoid structures with very few categories within a relation. The method has been developed for large tables, where the number of categories per relation is usually less than 20.</p>
<p><br>
<strong><em>How to process linked tables</em></strong><br>
We explain now a way how to process a set of linked tables, for instance <span class="math inline">\(T_1\)</span>, <span class="math inline">\(T_2\)</span>, <span class="math inline">\(T_3\)</span> using Modular for secondary cell suppression. This process is normally an iterative process. The method described here is referred to as “<em>traditional method</em>” in de Wolf, Giessing (2008). De Wolf, Giessing (2008) suggest an extension of the <span class="math inline">\(\tau\)</span>‑ARGUS Modular (referred to as “<em>adapted Modular</em>”) method as a much more practical approach to deal with linked tables. Once this extension is available the method described in the following should be used only in cases where the adapted Modular method cannot be applied to the full set of linked tables (<em>e.g.</em> if the tables involve too many dimensions).</p>
<p>The question is then, which protection levels (cf. <a href="#sec-secondary-protection-methods" class="quarto-xref"><span>Section 4.2.2</span></a>) to assign to those ‘manually unsafe cells’ (in the <span class="math inline">\(\tau\)</span>‑ARGUS terminology). A proper method would consist of using a CTA method (c.f. <a href="#sec-secondary-protection-methods" class="quarto-xref"><span>Section 4.2.2</span></a>) to find the closest adjusted table <span class="math inline">\(T_1^*\)</span> to <span class="math inline">\(T_1\)</span> that is consistent with the suppression pattern <span class="math inline">\(S_{11}\)</span> (that is, all cell values of <span class="math inline">\(T_1^*\)</span> cells that are not in <span class="math inline">\(S_{11}\)</span> must be identical to the original <span class="math inline">\(T_1\)</span> cell values), and the constraints imposed by the protection levels for the sensitive cells. We could then compute protection levels for <span class="math inline">\(S_{11}\)</span> cells on basis of the distances between adjusted and original cell value. In practice a much simpler approach is usually taken, by assigning a fixed percentage (the ‘manual safety range’ <span class="math inline">\(\tau\)</span>-ARGUS terminology) of the cell value as protection level. There is, however, no way to tell which percentage would be suitable: when a small cell is a secondary suppression for a large, strongly sensitive cell, a protection level even of 100% may be required. On the other hand, when a large cell is a secondary suppression for a very small sensitive cell, even 1% may cause overprotection.</p>
<p>After processing <span class="math inline">\(T_2\)</span> in this fashion, <span class="math inline">\(T_3\)</span> is processed. In addition to the <span class="math inline">\(S_{113}\)</span> cells, here we also have to consider as additional suppressions <span class="math inline">\(S_{213}\)</span> cells, <em>i.e.</em> the subset of secondary suppressions selected for Table <span class="math inline">\(T_2\)</span> during the first iteration which are identical to a cell in <span class="math inline">\(T_3\)</span>.</p>
<p>In this way, when processing table <span class="math inline">\(T_i,\ (i = 1,2,3)\)</span> in iteration step <span class="math inline">\(k_0\)</span>, we consider as additional suppressions <span class="math inline">\(\bigcup\limits_{k &lt; k_{0}}{\left( S_{1\text{ki}} \cup S_{2\text{ki}} \cup S_{3\text{ki}} \right)} \cup \bigcup\limits_{j &lt; i}{S_{\text{jk}_{0}i}}\)</span>, while we may try to avoid (by weighting, or by setting cell status to ‘protected’) that those cells that the tables have in common, <em>i.e.</em> the cells of <span class="math inline">\(T_{i_{j}} \quad (j \neq i)\)</span>, and that are not already contained in the set of additional suppressions are selected as secondary suppressions. Note that we assign 0% protection levels to cells in <span class="math inline">\(S_{iki}\)</span>, the set of secondary suppression which have been assigned to the current table in a previous step <span class="math inline">\(k\)</span> of the iteration. The process is continued until for each of the tables the set of additional suppressions that has to be considered when processing the table in this step is identical to the set considered in the previous step.</p>
<p>The necessary modifications of cell properties like cell status, or cell costs can be achieved relatively easily using the a priori file facility of <span class="math inline">\(\tau\)</span>-ARGUS. The general concept of this file is outlined in the following.</p>
<p><br>
<strong><em>A-priori file</em></strong><br>
Through the standard sensitivity rules and other options we can apply all kinds of SDC-methods. Nevertheless there are situations which still require ‘manual’ intervention. <em>E.g.</em> for some other outside reason a cell must be suppressed or contrarily must be excluded from secondary suppression. Examples are the exports of certain sensitive goods (military equipment) to certain countries. These cells must be suppressed. Sometimes a cell value is known already to outsiders (maybe by other publications etc.) and therefore cannot be selected as a secondary suppression.</p>
<p>Also the preferences for the selection of the secondary suppressions via the cost function could be used as an option to influence the secondary suppression pattern. This might be the result of the year-to-year periodical data issue (see below).</p>
<p>Specifying all this information manually during a <span class="math inline">\(\tau\)</span>‑ARGUS run for a large table can be rather cumbersome and error-prone. In the batch version of <span class="math inline">\(\tau\)</span>‑ARGUS it is even impossible to do this. So the a-priori file has been introduced.</p>
<p>In this file the user can specify for each cell (by giving the values of the spanning variables):</p>
<ul>
<li><p>cells that cannot be selected as secondary suppressions</p></li>
<li><p>cells that must be suppressed</p></li>
<li><p>specify a new value of the cost function for a cell; this can be either high or low. This will influence the chance of a cell as a candidate for sec.&nbsp;suppression.<br>
Note that this option is not valid for the hypercube, as the hypercube has its own cost function that cannot be influenced.</p></li>
</ul>
<p>Further details of this file can be found in the <span class="math inline">\(\tau\)</span>‑ARGUS manual.</p>
<p><br>
<strong><em>Periodical data</em></strong><br>
If tables are provided regularly (say monthly, quarterly or annually) certain problems may be connected to disclosure control: Firstly, it may seem to be too much effort. Secondly, if we do disclosure analysis independently each time, suppression patterns may differ. We will have the situation that we publish a particular cell value for one period, but use it as secondary suppression in the next period. Also with a cell published in one period, but becoming a primary cell in the second period, there is a risk of a very narrow estimate based on the previous period. Especially if the observations tend to be rather constant over time the pre-period value might be a close estimate of the suppressed value for the current period, which then might be used to disclose a current-period sensitive cell.</p>
<p>An alternative might be simply to copy the suppression pattern of the previous period, and add some (primary and secondary – if required) suppressions for those cases where cells were not sensitive in the last period, but are sensitive in the current period. Another option would be to assign low costs to previous-period suppressions when assigning secondary suppressions to the current data tables. However, both these strategies will cause an increase in information loss, which will be stronger the more changes there are in cell sensitivity between periods.</p>
<p><br>
<strong><em>Tables with negative values</em></strong><br>
In <a href="#sec-sensitive-cells-magnitude-tables" class="quarto-xref"><span>Section 4.2.1</span></a> we already discussed the problem of negative contributions, presenting an idea on how to solve the problem with respect to primary protection. However, when there are negative contributions, it may also happen that some of the table cell values turn out to be negative. The current version of <span class="math inline">\(\tau\)</span>-ARGUS does not yet support the processing of such a table, but a little pre-processing may solve the problem:</p>
<p>If the microdata is available, one should simply add another variable to the dataset presenting the absolute values for the variable of interest. Cell suppression should then be based on this (positive) variable. As explained in <a href="#sec-sensitive-cells-magnitude-tables" class="quarto-xref"><span>Section 4.2.1</span></a>, we recommend use of a minimum frequency rule, rather than a concentration rule in such a case. Hence, taking absolute values would not affect the results of primary suppression. As explained in <a href="#sec-sensitive-cells-magnitude-tables" class="quarto-xref"><span>Section 4.2.1</span></a>, we recommend using protection levels of zero in the case of a minimum frequency rule. So the fact that some cell values will be larger for the shadow variable than for the original variable will have no effect on the feasibility checks within the process of secondary cell suppression. It may still have an effect on the selection of secondary suppressions, if we use this shadow variable also as cost variable, because in that way we assign too large costs to cells with many and/or large negative contributions. However, there are options to modify cell costs for individual cells in <span class="math inline">\(\tau\)</span>‑ARGUS.</p>
<p>In the more difficult case that the microdata is not available, we could make a new table file, selecting only bottom-level cells. In this new file, we could replace the original cell values by their absolutes, and then use <span class="math inline">\(\tau\)</span>‑ARGUS facilities to add up the table again.</p>
</section>
<section id="sec-introductive-example" class="level3" data-number="4.3.4">
<h3 data-number="4.3.4" class="anchored" data-anchor-id="sec-introductive-example"><span class="header-section-number">4.3.4</span> Introductive Example</h3>
<p>The German census of horticulture is a decennial survey. The main publication of 2005 data involves about 70 response variables grouped by geography and size class of horticultural area. Many of the response variables are frequencies which were not considered as sensitive. Magnitude variables are horticultural or agricultural area, classified by type of establishment, or by production type, etc., and number of labourers classified in similar ways.</p>
<p>Because the publication presents data on low geographic detail, the experts from agriculture statistics thought it would not be adequate to base disclosure analysis simply on a few lead variables, like horticultural area and copy the resulting suppression pattern to other variables: The argument was that, even if there are many units falling into a particular size class/geography cell, and this cell is not dominated by any large units, it might still happen, for instance, that only a single unit falls into a particular subgroup of, say, the type of production, and this unit could be identifiable by this property (given the size class and geography information). In such a case the subgroup cell should be suppressed to avoid that sensitive information would be published about this unit. Consequently, disclosure risk assessment had to be carried out for each of the variables separately.</p>
<p>The main approach taken to simplify the original extremely complex linked tables structure resulting from linear relationship between the response variables was to ignore response variable relations like we discussed it in 4.3.3: A typical instance is fruit production. Fruit can be produced by farm companies focussing on fruit production (which was one of the categories for ‘type of production’), but also by other companies. The publication presents the overall fruit production area, and also fruit production area of farms focussing on fruit production. In order to avoid disclosure-by-differencing problems, in principal we should have processed fruit production area by both categories (<em>e.g.</em> ‘farms focussing on fruit production’ and ‘other farms’). However, as the ‘other farms’ category corresponds to a much larger group, it seemed to be justified not to include this variable into the disclosure control process, treating ‘fruit production area (by size class and geography)’ and ‘fruit production area of farms focussing on fruit production (by size class and geography)’ as separate 2-dimensional tables.</p>
<p>In this way, the disclosure control procedure finally involved 23 tables. 16 were 2-dimensional, the others were 3-dimensional. Most of the tables could be processed independently, but unfortunately, 3 of the 3‑dimensional tables were linked.</p>
<p>The explanatory variables were Geography (at 358 categories and 4 levels, down to the department level), size class (at 9 categories), and various ‘type of production’ classifications, the largest involving 10 categories with hierarchical substructure.</p>
<p>Data for the tabulations was delivered by the German state statistical offices on an aggregate level. Some SAS procedures were developed to organize data import from those 16 files, <em>e.g.</em> to build suitable <span class="math inline">\(\tau\)</span>‑ARGUS table format files, and also read the results of disclosure processing (<em>e.g.</em> cell status: safe/unsafe) back into the 16 files. Each line of that table format file (corresponding to a particular table cell) contained the respective codes for the spanning variables, the cell value, the cell frequency, the value of the largest and second-largest contributor, and a cell status (safe/unsafe). A special <span class="math inline">\(\tau\)</span>‑ARGUS facility was used to build federal-level aggregates from the 16 corresponding state-level cells.</p>
<p>For processing the tables we used <span class="math inline">\(\tau\)</span>‑ARGUS. The <span class="math inline">\(p\%\)</span>-rule was employed for primary protection. For secondary cell suppression we ran the Modular method of <span class="math inline">\(\tau\)</span>‑ARGUS. For the 20 tables that could be protected independently from each other, these were straightforward applications of the software, which all finished successfully after at most a few minutes.</p>
<p>The only problem was the processing of the 3 linked tables. We used some little SAS programs to organise the import of secondary suppressions selected in one table into the others in the fashion described in <a href="#sec-efficient-protection-process" class="quarto-xref"><span>Section 4.3.3</span></a>, making suitable a-priori files for the next <span class="math inline">\(\tau\)</span>-ARGUS run. However, it turned out that this procedure did not work well for that instance: the tables were extremely detailed with few non-zero, non-sensitive cells on the lower levels. As a result, quite a lot of secondary suppressions were assigned, also on the higher levels of the table, in particular those parts that were also contained in one of the other 2 tables. The number of suppressions in the overlap sections did not decrease substantially from one iteration step to the next.</p>
<p>After it was explained to the experts from agricultural statistics that the tables were probably too detailed, it became clear that they had not really been interested in publishing data for those tables on the district level. What they were actually interested in for those 3 tables were 3-dimensional tabulations by size class and geography, down to the level ‘Region’ (just above the district level), and 2-dimensional tabulations by geography down to the district level, but without size classes. Processing the 3 linked, 3-dimensional tables down to the level ‘Region’ was a straightforward application of the linked tables processing described in 4.3.3, finished after one iteration step. We then copied secondary suppressions of those three 3-dimensional tables into the corresponding three 2-dimensional district level tables and again applied the linked tables processing which also ended successfully after a single iteration.</p>
</section>
</section>
<section id="sec-methodology-secondary-suppression-tau-argus" class="level2" data-number="4.4">
<h2 data-number="4.4" class="anchored" data-anchor-id="sec-methodology-secondary-suppression-tau-argus"><span class="header-section-number">4.4</span> Methodological concepts of secondary cell suppression algorithms in <span class="math inline">\(\tau\)</span>‑ARGUS</h2>
<p>Within this section we briefly explain the methodological concepts of the secondary cell suppression algorithms offered by <span class="math inline">\(\tau\)</span>-ARGUS.</p>
<section id="sec-optimal" class="level3" data-number="4.4.1">
<h3 data-number="4.4.1" class="anchored" data-anchor-id="sec-optimal"><span class="header-section-number">4.4.1</span> Optimal</h3>
<p>The optimal approach is based on a Mathematical Programming technique which consists of solving Integer Linear Programming programs modelling the combinatorial problems under different methodologies (Cell Suppression and Controlled Rounding). The main characteristic of these models is that they share the same structure, thus based only on a 0-1 variable for each cell. In the Cell Suppression methodology, the variable is 1 if and only if the cell value must be suppressed. In the Controlled Rounding methodology, the variable is 1 if and only if the cell value must be rounded up. No other variables are necessary, so the number of variables in the model is exactly the number of cells in the table to be protected. In addition, the model also imposes the protection level requirements (upper, lower and sliding) in the same way for the different methodologies (Cell Suppression and Controlled Rounding). These requirements ask for a guarantee that an attacker will not get too narrow an interval of potential values for a sensitive cell, which he/she will compute by solving two linear programming programs (called attacker problems). Even if a first model containing this two-attacker problem would lead to a bi-level programming model, complex to be solved in practice, a Benders' decomposition approach allows us to convert the attacker problems into a set of linear inequalities. This conversion provides a second model for each methodology that can be efficiently solved by a modern cutting-plane approach. Since the variables are 0-1, a branching phase can be necessary, and the whole approach is named "branch-and-cut algorithm".</p>
<p>Branch-and-cut algorithms are modern techniques in Operations Research that provide excellent results when solving larger and complicated combinatorial problems arising in many applied fields (like routing, scheduling, planning, telecommunications, etc.).The idea is to solve a compact 0-1 model containing a large number of linear inequalities (as the ones above mentioned for the Cell Suppression and for the Controlled Rounding) through an iterative procedure that does not consider all the inequalities at the same time, but generates the important ones when needed. This dynamic procedure of dealing with large models allows the program to replace the resolution of a huge large model by a short sequence of small models, which is termed a "decomposition approach". The on-line generation of the linear inequalities (rows) was also extended in this work to the variables (columns), thus the algorithm can also works on tables with a large number of cells, and the overall algorithm is named "branch-and-cut-and-price" in the Operations Research literature.</p>
<p>To obtain good performance, the implementation has also considered many other ingredients, standard in branch-and-cut-and-price approaches. For example, it is fundamentally the implementation of a pre-processing approach where redundant equations defining the table are eliminated, where variables associated to non-relevant cells are removed, and where dominated protection levels are detected. The pre-processing is fundamental to make the problem as small as possible before starting the optimization phase. Another fundamental ingredient is the heuristic routine, which allows the algorithm to start with an upper bound of the optimal loss of information. This heuristic routine ensures the production of a protected pattern if the algorithm is interrupted by the user before the end. In other words, thanks to the heuristic routine, the implemented algorithm provide a near-optimal solution if the execution is cancelled before having a proof of optimality. During the implicit enumeration approach (<em>i.e.</em> the branch-and-cut-and-price) the heuristic routine is called several times, thus providing different protected patterns, and the best one will be the optimal solution if its loss of information is equal to the lower bound. This lower bound is computed by solving a relaxed model, which consists of removing the integrability condition on the integer model. Since the relaxed model is a linear program, a linear programming solver must be called.</p>
<p>We have not implemented our own linear programming solver, but used a commercial solver which is already tested by other programmers for many years. A robust linear programming solver is a guarantee that no numerical trouble will appear during the computation.</p>
<p>That is the reason to require either CPLEX (from ILOG) or XPRESS (from DashOptimization). Because the model to be solved can be applied to all type of table structures (2-dim, 3-dim, 4-dim, etc), including hierarchical and linked tables, we cannot use special simplex algorithm implementations, like the min-cost flow computation which would require to work with tables that can be modelled as a network (<em>e.g.</em>, 2-dimensional tables or collections of 2-dim tables linked by one link). On this special table, ad-hoc approaches (solving network flows or short path problems) could be implemented to avoid using general linear programming solvers.</p>
<p>The optimal solution is the result of solving this complex optimisation problem. Although the computing power of modern PCs has been increased considerably during the past period, nevertheless this method will not be applicable for very large tables, which are not uncommon in NSIs. Another drawback of the current implementation is the lack of facilities to protect against the singleton problem. This might be included in a future release.</p>
<p>However this method is used very successfully in the modular approach described in the next section.</p>
<p><strong>Reference:</strong></p>
<p>Fischetti, M. and J.J. Salazar-González (1998). <em>Models and Algorithms for Optimizing Cell Suppression in Tabular Data with Linear Constraints.</em> Technical Paper, University of La Laguna, Tenerife.</p>
</section>
<section id="sec-modular" class="level3" data-number="4.4.2">
<h3 data-number="4.4.2" class="anchored" data-anchor-id="sec-modular"><span class="header-section-number">4.4.2</span> Modular</h3>
<p>The modular (HiTaS) solution is a heuristic approach to cell suppression in hierarchical tables. Hierarchical tables are specially linked tables: at least one of the spanning variables exhibits a hierarchical structure, <em>i.e</em>. contains (many) sub-totals.</p>
<p>In Fischetti and Salazar (1998) a theoretical framework is presented that should be able to deal with hierarchical and generally linked tables. In what follows, this will be called the mixed integer approach. In this framework, additional constraints to a linear programming problem are generated. The number of added constraints however, grows rapidly when dealing with hierarchical tables, since many dependencies exist between all possible (sub-)tables containing many (sub-)totals. The implemented heuristic approach (HiTaS) deals with a large set of (sub)-tables in a particular order. A non hierarchical table can be considered to be a hierarchical table with just one level. In that case, the approach reduces to the original mixed integer approach and hence provides the optimal solution. In case of a hierarchical table, the approach will provide a sub-optimal solution that minimises the information loss per sub-table, but not necessarily the global information loss of the complete set of hierarchically linked tables.</p>
<p>In the following section, a short description of the approach is given. For a more detailed description of the method, including some examples, see <em>e.g</em>., De Wolf (2002).</p>
<p>HiTaS deals with cell suppression in hierarchical tables using a top-down approach. The first step is to determine the primary unsafe cells in the base-table consisting of all the cells that appear when crossing the hierarchical spanning variables. This way all cells, whether representing a (sub-)total or not, are checked for primary suppression. Knowing all primary unsafe cells, the secondary cell suppressions have to be found in such a way that each (sub-)table of the base-table is protected and that the different tables cannot be combined to undo the protection of any of the other (sub-)tables. The basic idea behind the top-down approach is to start with the highest levels of the variables and calculate the secondary suppressions for the resulting table. The suppressions in the interior of the protected table are then transported to the corresponding marginal cells of the tables that appear when crossing lower levels of the two variables. All marginal cells, both suppressed and not suppressed, are then ‘fixed’ in the calculation of the secondary suppressions of that lower level table, <em>i.e.</em> they are not allowed to be (secondarily) suppressed. This procedure is then repeated until the tables that are constructed by crossing the lowest levels of the spanning variables are dealt with.</p>
<p>A suppression pattern at a higher level only introduces restrictions on the marginal cells of lower level tables. Calculating secondary suppressions in the interior while keeping the marginal cells fixed, is then independent between the tables on that lower level, <em>i.e.</em> all these (sub)-tables can be dealt with independently of each other. Moreover, added primary suppressions in the interior of a lower level table are dealt with at that same level: secondary suppressions can only occur in the same interior, since the marginal cells are kept fixed.</p>
<p>However, when several empty cells are apparent in a low level table, it might be the case that no solution can be found if one is restricted to suppress interior cells only. Unfortunately, backtracking is then needed.</p>
<p>Obviously, all possible (sub)tables should be dealt with in a particular order, such that the marginal cells of the table under consideration have been protected as the interior of a previously considered table. To that end, certain groups of tables are formed in a specific way (see De Wolf (2002)). All tables within such a group are dealt separately, using the mixed integer approach.</p>
<p>The number of tables within a group is determined by the number of parent-categories the variables have one level up in the hierarchy. A parent-category is defined as a category that has one or more sub-categories. Note that the total number of (sub)-tables that have to be considered thus grows rapidly.</p>
<p>For the singleton problem there is a procedure included in the modular solution. At least on each row/column of a (sub-)table it is guaranteed that no disclosure by the singleton is possible.</p>
</section>
<section id="sec-network" class="level3" data-number="4.4.3">
<h3 data-number="4.4.3" class="anchored" data-anchor-id="sec-network"><span class="header-section-number">4.4.3</span> Network</h3>
<p>The network flows package for cell suppression (NF CSP) provided by the Dept. of Statistics and Operations Research of the Universitat Politècnica de Catalunya implements a fast heuristic for the protection of statistical data in two dimensional tables with one hierarchical dimension. The method sensibly combines and improves ideas of previous approaches for the secondary cell suppression problem in two-dimensional tables. Details about the heuristic can be found in (Castro, 2003a) and (Castro, 2003b)</p>
<p>Network flow heuristics for secondary cell suppression build on the fact, that a suppressed cell in a two-dimensional table is safe from exact disclosure if, and only if, the cell is contained in a ‘cycle’, or ‘alternating path’ of suppressed cells, where a cycle is defined to be a sequence of non-zero cells with indices <span class="math inline">\(\{ (i_0,j_0), (i_1,j_0), (i_1, j_1), \ldots,(i_n, j_n), (i_0, j_n) \}\)</span>, where all <span class="math inline">\(i_{k}\)</span> and <span class="math inline">\(j_{l}\)</span> for <span class="math inline">\(k = 0,1,\ldots,n\)</span> , and <span class="math inline">\(l = 0,1,\ldots,n\)</span> respectively (<span class="math inline">\(n\)</span>: length of the path), are different, <em>e.g.</em> <span class="math inline">\(i_{k_{l}} \neq i_{k_{2}}\)</span> unless <span class="math inline">\(k_{1}\)</span> = <span class="math inline">\(k_{2}\)</span> , and <span class="math inline">\(j_{l_{1}} \neq \ j_{l_{2}}\)</span> unless <span class="math inline">\(l_{1}\)</span> = <span class="math inline">\(l_{2}\)</span> .</p>
<p>The NF CSP heuristic is based on the solution of a sequence of shortest-path subproblems that guarantee a feasible pattern of suppressions (<em>i.e.</em> one that satisfies the protection levels of sensitive cells). Hopefully, this feasible pattern will be close to the optimal one. In the ARGUS implementation solutions of the shortest-path subproblems are computed by either of two network flows optimization solvers also implemented by Universitat Politècnica de Catalunya.</p>
<p>However the current implementation of the network flows do (not yet) protect against the singleton problem.</p>
</section>
<section id="sec-hypercube" class="level3" data-number="4.4.4">
<h3 data-number="4.4.4" class="anchored" data-anchor-id="sec-hypercube"><span class="header-section-number">4.4.4</span> Hypercube</h3>
<p>In order to ensure tractability also of big applications, <span class="math inline">\(\tau\)</span>−ARGUS interfaces with the GHM<em>ITER</em> hypercube method of R. D. Repsilber of the Landesamt für Datenverarbeitung und Statistik in Nordrhein-Westfalen/Germany, offering a quick heuristic solution. The method has been described in depth in (Repsilber, 1994), or (Repsilber, 2002). For a briefer description see (Giessing and Repsilber, 2002).<br>
</p>
<p><strong><em>The method</em></strong><br>
The approach builds on the fact that a suppressed cell in a simple n‑dimensional table without substructure cannot be disclosed exactly if that cell is contained in a pattern of suppressed, nonzero cells, forming the corner points of a hypercube.</p>
<p>The algorithm subdivides n-dimensional tables with hierarchical structure into a set of <span class="math inline">\(n\)</span>-dimensional sub-tables without substructure. These sub-tables are then protected successively in an iterative procedure that starts from the highest level. Successively for any primary suppression in the current sub-table, all possible hypercubes with this cell as one of the corner points are constructed.</p>
<p>If protection against inferential disclosure is requested, for each hypercube, a lower bound is calculated for the width of the feasibility interval for the primary suppression that would result from the suppression of all corner points of the particular hypercube. To compute that bound, it is not necessary to implement the time consuming solution to the Linear Programming problem and it is possible to consider bounds on cell values that are assumed to be known to an intruder. If it turns out that the bound for the feasibility interval width is sufficiently large, the hypercube becomes a feasible solution. For any of the feasible hypercubes, the loss of information associated with the suppression of its corner points is calculated. The particular hypercube that leads to minimum information loss is selected, and all its corner points are suppressed. If several hypercubes are considered feasible, the method selects the one which requires the smallest number of additional cells to be suppressed. If there are several feasible hypercubes that would all lead to the same number of additional secondary suppressions, the method picks the one with the smallest total of weighted cell values of those additional suppressions. The weights are determined automatically, considering the hierarchical level of cells. Cells on a higher level get larger weights. The weights are determined temporarily, only the subtable which is currently processed is considered.</p>
<p>After all sub-tables have been protected once, the procedure is repeated in an iterative fashion. Within this procedure, when cells belonging to more than one sub-table are chosen as secondary suppressions in one of these sub-tables, in further processing they will be treated like sensitive cells in the other sub-tables they belong to. The same iterative approach is used for sets of linked tables.</p>
<p>The ‘hypercube criterion’ is a sufficient but not a necessary criterion for a ‘safe’ suppression pattern. Thus, for particular subtables the ‘best’ suppression pattern may not be a set of hypercubes – in which case, of course, the hypercube method will miss the best solution and lead to some overprotection. Other simplifications of the heuristic approach that add to this tendency for over‑suppression are the following: when assessing the feasibility of a hypercube to protect specific target suppressions against inferential disclosure, the method</p>
<ul>
<li><p>is not able to consider protection maybe already provided by other cell suppressions (suppressed cells that are not corner points of this hypercube) within the same sub‑table,</p></li>
<li><p>does not consider the sensitivity of multi‑contributor primary suppressions properly, that is, it does not consider the protection already provided to the individual respondent contributions in advance of cell suppression through aggregation of these contributions.<br>
</p></li>
</ul>
<p><strong><em>ARGUS settings</em></strong><br>
In the implementation offered by <span class="math inline">\(\tau\)</span>‑ARGUS, GHMITER makes sure that a single respondent cell will never appear to be corner point of one hypercube only, but of two hypercubes at least. Otherwise it could happen that a single respondent, who often can be reasonably assumed to know that he is the only respondent, could use his knowledge on the amount of his own contribution to recalculate the value of any other suppressed corner point of this hypercube. Because of this strategy, the method yields PS2* standard (c.f. <a href="#sec-evaluation-of-secondary-cell-suppression-algorithms-offered-by-tau-argus" class="quarto-xref"><span>Section 4.3.2</span></a>).</p>
<p>When protection against inferential disclosure is requested, the settings of the method are determined on basis of the assumptions that an intruder could estimate each cell value to within bounds of <span class="math inline">\(\pm q\%\)</span> prior to the publication, where the percentage of <span class="math inline">\(q\)</span> for those <em>a priori</em> bounds is selected by the <span class="math inline">\(\tau\)</span>‑ARGUS user. Considering these <em>a&nbsp;priori</em> bounds, <span class="math inline">\(\tau\)</span>‑ARGUS determines the settings for the hypercube method in such a way, that it yields PS1* protection standard (c.f. <a href="#sec-evaluation-of-secondary-cell-suppression-algorithms-offered-by-tau-argus" class="quarto-xref"><span>Section 4.3.2</span></a>). Because the method is unable to ‘add’ the protection given by multiple hypercubes, in certain situations it will be unable to confirm that a cell has been protected properly according to PS1* standard and will attempt to select the hypercube that provides the largest amount of protection to the target suppression in that situation.</p>
</section>
</section>
<section id="sec-CTA" class="level2" data-number="4.5">
<h2 data-number="4.5" class="anchored" data-anchor-id="sec-CTA"><span class="header-section-number">4.5</span> Controlled Tabular Adjustment</h2>
<p><em>‘Controlled Tabular Adjustment’</em> (CTA) suggested for instance in Cox and Dandekar (2002), Castro (2003), or Castro and Giessing (2006). CTA is a new emerging protection method for magnitude tabular data. Unlike cell suppression, it is a perturbative method.</p>
<p>The starting point of the cell suppression methodology presented in 4.3 is that suppressing cells in a table results in theory in replacing the confidential cell value by a feasibility interval that could principally be computed by any user of a table published with suppressions. As explained in 4.2.2., in case the proper protection is given to the cells, the feasibility interval covers the protection interval (<em>i.e.</em> the interval that is needed to protect the individual contribution). CTA methodology on the other hand aims at finding the closest additive table to the original table ensuring that adjusted values of all confidential cells are safely away from their original value (considering the protection intervals) and that the adjusted values are within a certain range of the real values.<br>
</p>
<p><strong><em>CTA variants</em></strong><br>
Several variants of CTA have been discussed in the literature (Dandekar and Cox, 2002), (Cox et al., 2004), (Castro, 2006) etc., suggesting to obtain CTA by using (mixed integer) linear programming methodology. The main differences between those alternatives are on one hand the way in which the deviation sense of the primary suppressions is determined (heuristically vs.&nbsp;optimal, <em>i.e.</em> through solution of integer programming problems). On the other hand, the definition of constraints matters (forcing the adjusted values to be within a “certain range” of the real values). And finally there is the issue of information loss/cell costs, <em>e.g.</em> the distance metric used to determine what is “close” to the original table. Typically, weighted metrics are used. Implementations usually offer a choice of cost functions.</p>
<p>As an alternative to the above linear programming based approaches, (Cox et al., 2006) and (Ichim and Franconi, 2006) suggest methology to achieve CTA using statistical methods like Iterative Proportional Fitting or Calibration techniques.<br>
</p>
<p><strong><em>Information loss measures</em></strong><br>
While for cell suppression, the effect of SDC on the data is obvious to the users of the data, for CTA this is not the case. Many information loss measures proposed in the literature that should help – eventually both parties: data providers and data users – to judge the quality of an adjusted table are global measures. A typical global measure would be statistics on the relative deviations between true and adjusted cell values. (Cox et al., 2004) hint at measuring the preservation of univariate properties like mean, variance, correlation, etc. in subsets of cells. (Cox et al., 2006) employ Goodness-of-fit statistics like the Kolmogorov Smirnov test to compare adjusted vs.&nbsp;original data.</p>
<p>In contrast to a global measure, a local measure will inform the data user on the reliability of each particular (adjusted) cell value. (Castro and Giessing, 2006b) discuss criteria that could be used by data providers to decide whether an adjustment is so small that there is no need for the cell to be flagged as “adjusted”.</p>
<p>In order to be able to publish a single value in the tables whilst at the same time clarifying to the users that these values are not the original values, (Giessing, Hundepool and Castro, 2007) suggest to round the values resulting from CTA. The basis for rounding would be chosen in a way that the rounding interval includes both the true and the adjusted value. This strategy requires however some occasional post-processing to ensure that the protection of the sensitive cells is sufficient.<br>
</p>
<p><strong><em>Software</em></strong><br>
An implementation based on a simple heuristic to fix deviation senses for the adjustment of sensitive cells is available in the R-package <em>sdcTable</em> (Meindl, 2009). An algorithm for optimal CTA has been implemented on behalf of Eurostat by (Castro and Gonzalez, 2009).</p>
</section>
<section id="sec-CKM_mag" class="level2" data-number="4.6">
<h2 data-number="4.6" class="anchored" data-anchor-id="sec-CKM_mag"><span class="header-section-number">4.6</span> Cell Key Method for Magnitude Tables</h2>
<p>The ‘<em>Cell Key Method</em>’ (CKM) was initially developed by the Australian Bureau of Statistics (Fraser/ Wooton (2016); Thompson et al.&nbsp;(2013)) and hence is sometimes referred to as ‘<em>ABS method</em>’. It is a post tabular perturbative method that maintains consistency between all tables that use the same microdata and configuration. This method adds sufficient ‘<em>noise</em>’ to each cell so if an intruder tried to gather information by differencing, they would not be able to obtain the real data. It is one of the SDC methods recommended by Eurostat for the population and housing censuses 2021. In order to keep consistency between tables every record of the underlying microdata is equipped with a randomly generated number, the so called ‘<em>Record Key</em>’. This step is performed just once such that afterwards the Record Key is a fixed component of the microdata that is to be used for every subsequent evaluation. Now whenever the microdata are aggregated to form a table cell, the corresponding Record Keys are aggregated as well, forming the eponymous ‘<em>Cell Key</em>’. Using said Cell Key and a predefined table that encodes a probability distribution, which is tailored to the data/purpose, the corresponding noise can be looked up and added to the original cell value, before dissemination.</p>
<p>For more general information about the Cell Key Method, see <a href="05-frequency-tables.html#sec-CKM_freq" class="quarto-xref"><span>Section 5.4</span></a>.</p>
<p>The Cell Key Method was initially developed to protect frequency tables. By adding a controlled integer noise <span class="math inline">\(v\)</span> to a table cell <span class="math inline">\(n\)</span> a perturbed value <span class="math inline">\(\hat{n}=n+v\)</span> is generated. By applying this procedure to each table cell uncertainty about the real cell value is created. In this respect, this method has similar effects as simple deterministic rounding, but is unbiased and provides a higher level of protection when compared to a variant with similar level of information loss. A short comparison can be found in Enderle et al.&nbsp;(2018).</p>
<p>When adapting this method to magnitude tables, it must be noted that the amount of the noise <span class="math inline">\(v\)</span> needed, depends on the magnitude of the values in the microdata. Whereas in the case of frequencies it is a matter of protecting low counts and in particular individual cases, in the case of magnitude tables the magnitude of the individual values can vary a lot, and it is not sufficient, for example, to add a noise term of magnitude 5 when it is a matter of protecting a value that is one million. On the other hand, adding a noise term of one thousand to an original value of one hundred might be a little exaggerated. Since sufficient protection is particularly important in cases of dominance of one (or few) respondent(s), for the Cell Key Method for magnitude tables the amount of noise should correlate with the size of the largest contribution(s) to a cell value. For simplicity, we assume in the following that only the largest contribution is considered.</p>
<p>So, if <span class="math inline">\(x\)</span> is the actual value of a cell, <span class="math inline">\(x_{max}\)</span> is the corresponding largest contribution and <span class="math inline">\(v\)</span> is a limited random noise variable with a discrete distribution, then the perturbed value <span class="math inline">\(\hat{x}\)</span> computes as <span class="math inline">\(\hat{x}=x+\delta\cdot x_{max} \cdot v\)</span>, where <span class="math inline">\(\delta\)</span> is an additional control parameter. Since CKM derives its protective effect from uncertainty, all published values must of course be subject to some perturbation (with some probability). But since the amount of perturbation depends on the magnitude of the largest contribution, the deviations obtained are relatively small. Especially for cells without dominance issues, by adding (or subtracting) a fraction of the largest contribution value doesn’t affect the cell value too much. Whereas in table cells with a single strongly dominating contribution, the change in value is correspondingly more significant. But this is precisely what is desired.</p>
<p>The calculations for the Cell Key Method are normally carried out by appropriate software, such as <span class="math inline">\(\tau\)</span>-ARGUS, so that the user does not have to perform them themselves. Nevertheless, in the following, we give a brief, rough overview of how CKM is applied to continuous data, for interested readers. More detailed explanations, along with ideas for how to obtain control parameters such as <span class="math inline">\(\delta\)</span> can be found in Gießing and Tent (2019).</p>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-4-contents" aria-controls="callout-4" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Expert level
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-4" class="callout-4-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Since magnitudes usually aren’t integers but continuous, it is not possible to represent all values in a finite table. Hence an interpolation technique is used, while the p-table generally maintains the same form as depicted in <a href="05-frequency-tables.html#tbl-ckm_attacker" class="quarto-xref">Table&nbsp;<span>5.24</span></a>, in <a href="05-frequency-tables.html#sec-CKM_freq" class="quarto-xref"><span>Section 5.4</span></a> about CKM for frequency tables. Since we don’t want for a positive value <span class="math inline">\(x\)</span> to become negative after perturbation we require <span class="math inline">\(0\leq\hat{x}=x+\delta\cdot x_{max} \cdot v\)</span> which is equivalent to <span class="math inline">\(v \geq (-x)/( \delta\cdot x_{max})\)</span>. So <span class="math inline">\(v\)</span> has to be chosen with respect to <span class="math inline">\(x/( \delta\cdot x_{max})\)</span> and hence implicitly depends on both the cell value <span class="math inline">\(x\)</span> and the largest contributor <span class="math inline">\(x_{max}\)</span>. Note, that in the case of frequency tables, the distribution of the noise <span class="math inline">\(v\)</span> depended only on the count <span class="math inline">\(n\)</span> to be perturbed, therefore, this value was used to look up the noise in the perturbation table. Accordingly, for the continuous case, the value <span class="math inline">\(x/( \delta\cdot x_{max})\)</span> is now used in the lookup step.</p>
<div class="callout callout-style-simple callout-note">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-body-container">
<p><strong>Example</strong> We now use <a href="05-frequency-tables.html#tbl-ckm_attacker" class="quarto-xref">Table&nbsp;<span>5.24</span></a> again, to illustrate such a lookup step. If <span class="math inline">\(x/( \delta\cdot x_{max})\)</span> is one of the numbers 0, 1, 2, 3 or any value larger than 3, the lookup step works just as is the case of frequency tables. So if said value is 3, for example, and if the corresponding Cell Key is 0.8, again we have to look for that row in <a href="05-frequency-tables.html#tbl-ckm_attacker" class="quarto-xref">Table&nbsp;<span>5.24</span></a>, for which <span class="math inline">\(\textit{'orig. value'}\)</span> is 3 and for which <span class="math inline">\(\textit{'lower bound'} &lt; 0.8 \leq \textit{'upper bound'}\)</span>. Which again is met in the last table row, for which the noise is given as 1. Hence the perturbed value computes as <span class="math inline">\(\hat{x}=x+\delta\cdot x_{max} \cdot 1\)</span>. Imagine now a true cell value <span class="math inline">\(x=300\)</span>, with largest contributor <span class="math inline">\(x_{max}=200\)</span> and an additional control parameter of <span class="math inline">\(\delta=0.5\)</span>. The corresponding result is then <span class="math inline">\(\hat{x}=300+0.5\cdot 200 \cdot 1 = 400\)</span>. Please keep in mind that in this calculation example all values (including the p-table) are chosen in such a way that the calculation can be carried out as easily as possible and that the amount of noise is not anyway typical for practical use cases.</p>
<p>In case <span class="math inline">\(x/( \delta\cdot x_{max})\)</span> is a non-integer value smaller than 3, this value cannot be found in <a href="05-frequency-tables.html#tbl-ckm_attacker" class="quarto-xref">Table&nbsp;<span>5.24</span></a>. To solve this issue interpolation is used, <em>i.e.</em> if <span class="math inline">\(2 &lt; x/( \delta\cdot x_{max}) &lt; 3\)</span>, there exists a unique pair of positive numbers <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> such that <span class="math inline">\(x/( \delta\cdot x_{max}) = 2\cdot a + 3\cdot b\)</span> and <span class="math inline">\(a+b=1\)</span>. The algorithm, as implemented in <span class="math inline">\(\tau\)</span>-ARGUS, for example, then computes both the noise <span class="math inline">\(v_3\)</span> for the case <span class="math inline">\(x/( \delta\cdot x_{max})=3\)</span> and the noise <span class="math inline">\(v_2\)</span> for the case <span class="math inline">\(x/( \delta\cdot x_{max})=2\)</span>. The correct noise then results as <span class="math inline">\(v_2\cdot a +  v_3\cdot b\)</span>. We already looked up the noise <span class="math inline">\(v_3=1\)</span>, so we still need <span class="math inline">\(v_2\)</span>. So once again we need to search <a href="05-frequency-tables.html#tbl-ckm_attacker" class="quarto-xref">Table&nbsp;<span>5.24</span></a>. After we spotted the row for which ‘<span class="math inline">\(\textit{orig. value}\)</span>’ equals 2 and for which <span class="math inline">\(\textit{`lower bound'} &lt; 0.8 \leq \textit{`upper bound'}\)</span>, we obtain a noise value of <span class="math inline">\(v_2=0\)</span>. So if we consider the case where x = 300, <span class="math inline">\(x_{max} = 200\)</span> and <span class="math inline">\(\delta = 0.6\)</span>, then <span class="math inline">\(x/( \delta\cdot x_{max}) = 2.5\)</span> and this can be written as <span class="math inline">\(2\cdot 0.5 + 3\cdot 0.5\)</span>, <em>i.e.</em> in our instance <span class="math inline">\(a=b=0.5\)</span>. The perturbed value is therefore calculated as <span class="math inline">\(\hat{x}=300+0.6\cdot 200 \cdot (v_2\cdot 0.5+v_3\cdot 0.5)=300+0.6\cdot 200 \cdot (0\cdot 0.5+1\cdot 0.5) = 360\)</span> .</p>
</div>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="measurement-of-disclosure-risk-and-information-loss" class="level2" data-number="4.7">
<h2 data-number="4.7" class="anchored" data-anchor-id="measurement-of-disclosure-risk-and-information-loss"><span class="header-section-number">4.7</span> Measurement of disclosure risk and information loss</h2>
<section id="disclosure-risk" class="level3" data-number="4.7.1">
<h3 data-number="4.7.1" class="anchored" data-anchor-id="disclosure-risk"><span class="header-section-number">4.7.1</span> Disclosure risk</h3>
<p>The disclosure risk for tabular data (especially magnitude) includes the assessment of:</p>
<ul>
<li><p>primary risk,</p></li>
<li><p>secondary risk.</p></li>
</ul>
<p>The primary risk concerns the threat for direct identification of an unit resulting from too low frequency or existence of outliers according to the presented magnitude in the cell. The secondary risk assessment is necessary due to the fact that primary confidential cells in detailed tables may still not ensure sufficient protection against re-identification: together with single cells also sums for larger groups, <em>i.e.</em> the margins are computed. Then the protection of the primary sensitive cells can be undone, by some differencing. Therefore the risk of such an event should also be assessed.</p>
<p>The key measure for assessing primary disclosure risk in the case of magnitude tables is based on the <span class="math inline">\((n,k)\)</span>-dominance or <span class="math inline">\(p\%\)</span> rules (c.f. <a href="#sec-sensitive-cells-magnitude-tables" class="quarto-xref"><span>Section 4.2.1</span></a>, <a href="#tbl-sensitivity-rules" class="quarto-xref">Table&nbsp;<span>4.2</span></a>). In some formal regulations or recommendations it is assumed <span class="math inline">\(n=1\)</span> and <span class="math inline">\(k=75\)</span>. Hovever, in <a href="#sec-sensitive-cells-magnitude-tables" class="quarto-xref"><span>Section 4.2.1</span></a>, following the reach practical experience, it is recommended to use <span class="math inline">\(n&gt;1\)</span> in this context. Quite commonly the <span class="math inline">\((n,k)\)</span>-dominance is combined with <span class="math inline">\(k\)</span>-anonymity: a cell is regarded as unsafe if it violates the <span class="math inline">\(k\)</span>-anonymity or the <span class="math inline">\((n,k)\)</span>-dominance. Therefore the dislosure risk can be measured as the share of cells violating the finally assumed principle. A broader discussion on an application of these rules to assess the disclosure risk is performed in <a href="#sec-disclosure-control-concepts-tables" class="quarto-xref"><span>Section 4.2</span></a>. See also Hundepool et al.&nbsp;(2012).</p>
<p>However, this assessment concerns risk at the level of individual table cells. When designing tables, a risk measure at table level might be convenient. For frequncy tables, Antal, Shlomo and Elliot (2014) formulate four fundamental properties for a disclosure risk measure at table level:</p>
<ul>
<li><p>small cell values should have higher disclosure risk than larger,</p></li>
<li><p>uniformly distributed frequencies imply low disclosure risk ,</p></li>
<li><p>the more zero cells in the census table, the higher the disclosure risk ,</p></li>
<li><p>the risk measure should be bounded by 0 and 1.</p></li>
</ul>
<p>In the currently investigated case of magnitude table one should add one more condition concerning the specificity of this type of data presentation. That is, the larger is the share of the largest contributors in a cell the higher is the disclosure risk.</p>
<p>Shlomo, Antal and Elliot (2015) proposed in this context a measure based on the entropy. According to their approach, a high entropy indicates that the distribution across cells is uniform and a low entropy indicates mainly zeros in a row/column or table with a few non-zero cells. The fewer the number of non-zero cells, the more likely that attribute disclosure occurs. The entropy is given as <span id="eq-entropy"><span class="math display">\[
H=-\sum_{i=1}^{k}{\frac{c_i}{n}\cdot\log{\frac{c_i}{n}}},
\tag{4.12}\]</span></span> where <span class="math inline">\(c_i\)</span> is the number of units contained in <span class="math inline">\(i\)</span>-th cell, <span class="math inline">\(k\)</span> is the number of cell in the tables and <span class="math inline">\(n\)</span> is the total number of units covered by the table. The measure (<a href="#eq-entropy" class="quarto-xref"><span>4.12</span></a>) is next normalized to the form <span id="eq-entropy-normalized"><span class="math display">\[
1-\frac{H}{\log n}.
\tag{4.13}\]</span></span></p>
<p>This approach, however, doesn’t take our fifth condition into account. So, one can extend (<a href="#eq-entropy-normalized" class="quarto-xref"><span>4.13</span></a>) to the form combining both aspects, <em>e.g.</em> in the following way: <span class="math display">\[
r=1-\frac{1}{2}\left(\frac{H}{\log n}+\frac{1}{n}\sum_{i=1}^n{\frac{x_{i(\text{max})}}{\sum_{j\in C_i}{x_j}}}\right),
\]</span> where <span class="math inline">\(x_{i(\text{max})}\)</span> is the share of the largest contributors to the cell <span class="math inline">\(C_i\)</span> and <span class="math inline">\(x_j\)</span> is the contribution of the <span class="math inline">\(j\)</span>-th unit to it. This measure takes value from <span class="math inline">\([0,1]\)</span> and is easily interpretable.</p>
<p>The disclosure risk assessment can be also adjusted to the specificity of currently used SDC method. For instance, Enderle, Giessing and Tent (2020) have proposed an original risk measure for continous data perturbed using the Cell Key approach. It assumes that an intruder can know the amount of noise and the maximum deviation parameter. Then he/she can determine the feasibility intervals for original internal and margin cells using the noisy values. The risk estimate is then a weighted sum of risk probabilities, <em>e.g.</em> risk probabilites computed by summing up certain probabilities defined by the p-table relating to “risky” constellations, weighted by the empirical probability of the respective constellation to occur in a given test dataset.</p>
</section>
<section id="information-loss" class="level3" data-number="4.7.2">
<h3 data-number="4.7.2" class="anchored" data-anchor-id="information-loss"><span class="header-section-number">4.7.2</span> Information loss</h3>
<p>The basis of assessment of information loss for tables are differences between values of cells determined using microdata after application of SDC methods (or perturbed/hidden during creation of tables) and relevant values obtained on the basis of the original data. Let <span class="math inline">\(T_0\)</span> denote a table generated using original microdata and <span class="math inline">\(T_1\)</span> - analogous table created on the basis of perturbed relevant microdata (or in which original cell vaules were pretrurbed - post-tabular pertrurbation). Denote by <span class="math inline">\(T_l(c)\)</span> the value of cell <span class="math inline">\(c\)</span> of table <span class="math inline">\(T_l\)</span>, <span class="math inline">\(l=0,1\)</span>.</p>
<p>The following metrics are commonly recommended for determining measures of information loss due to application of SDC process in magnitude tables:</p>
<ul>
<li><p>absolute deviation: <span class="math inline">\(|T_1(c)-T_0(c)|\)</span>,</p></li>
<li><p>relative absolute deviation: <span class="math inline">\(\frac{|T_1(c)-T_0(c)|}{T_0(c)}\)</span>,</p></li>
<li><p>absolute deviation of square roots: <span class="math inline">\(|\sqrt{T_1(c)}-\sqrt{T_0(c)}|\)</span>.</p></li>
</ul>
<p>for any <span class="math inline">\(c\)</span>.</p>
<p>As one can observe, the absolute deviation of square roots has a sense in practice only if the cell values are nonnegative. However, this is the most common situation. Otherwise, <span class="math inline">\(\sqrt{T_l(c)}\)</span> can be replaced with <span class="math inline">\(-\sqrt{|T_l|}\)</span>, <span class="math inline">\(l=0,1\)</span>.</p>
<p>Using the metrics given above one can define complex measures of information loss for a given aggregate <span class="math inline">\(A\)</span> (it can be the whole table or a specific subset of table cells, referring to, for example, the same spatial unit - such as population numbers by age groups for a LAU 1 unit). The first of these measures is the average absolute deviation - the mean of absolute differences between cell values in original and modified tables: <span class="math display">\[
\text{AD}(A)=\frac{\sum_{c\in A}{|T_1(c)-T_0(c)|}}{n_A},
\]</span> where <span class="math inline">\(n_A\)</span> is the number of cells included in the aggregate <span class="math inline">\(A\)</span>.</p>
<p>One can also propose to use in this context the sum of relative absolute deviations, <em>i.e.</em> the the sum of relative differences between cell values in both tables: <span class="math display">\[
\text{RAD}(A)=\sum_{c\in A}{\frac{|T_1(c)-T_0(c)|}{T_0(c)}}.
\]</span></p>
<p>The last - but not least - measure which we would like to present here is the measure based on absolute differences between square roots from cell values in original nad perturbed tables using the formula of Hellinger’s distance (proposed in 1909 by German mathematician Ernst Hellinger (1883-1950)): <span class="math display">\[
\text{HD}(A)=\sqrt{\frac{1}{2}\sum_{c\in A}{\left(\sqrt{T_1(c)}-\sqrt{T_0(c)}\right)^2}}.
\]</span></p>
<p>However, there may be missing data in the tables. When perturbative SDC methods are applied, these gaps will result mainly from missing data occurring in the original microdata being a basis of construction of tables. In this case, during computation of cell values one can omit these gaps or earlier make an imputation of them. The only inconvenience may appear when there are no data concerning the categories defined by the cell. Then one must either resign from a given structure of the table (<em>e.g.</em> by combining some of the original categories into another, more coarse one), or the measure of loss should be based on the measurement of loss at the level of microdata corresponding to this cell, performed in the manner described in <a href="03-microdata.html">Chapter 3</a>.</p>
<p>More difficult is the situation where non-perturbative post-tabular SDC methods are used. For assessing information loss from the perspective of the data user, one can make a comparison of tables before and after the SDC process. For this, one should simulate a table, in which for missing cells their values are imputed. On the other hand, for controlling the process of selection of secondary cell suppression , information loss will be expressed as the sum of costs incurred due to it. The problem is whether the weight of each cell in the table is the same, or whether cells with a higher value have a higher weight (cf.&nbsp;the discussion in <a href="#sec-setting-up-a-tabular-data-protection-problem-in-practice" class="quarto-xref"><span>Section 4.3.1</span></a>). In practice, suppression of too many cells with high values can significantly decrease the utility of disseminated data. The issue of information loss can be variously expressed, depending on preferences and needs of a user. In this way, it is possible to influence the operation of the algorithm for selecting cells for secondary suppression. In <a href="#sec-setting-up-a-tabular-data-protection-problem-in-practice" class="quarto-xref"><span>Section 4.3.1</span></a> (see also Hundepool et al.&nbsp;(2012)) we indicate the most common criteria taken into account when the cost function for cell suppression is defined:</p>
<ul>
<li><p>the same weights for all cells - for minimalization of the number of secondary suppressed cells,</p></li>
<li><p>number of units in aggregate represented by a cell - leads to looking for possibilities of suppression of only such cells which will represent jointly as small number of units as possible,</p></li>
<li><p>cell value - an optimal solution is leaving in publication as many cells with higher values as possible.</p></li>
</ul>
<p>For more details, see discussion on cell costs and cost functions in <a href="#sec-setting-up-a-tabular-data-protection-problem-in-practice" class="quarto-xref"><span>Section 4.3.1</span></a>.</p>
</section>
</section>
<section id="references" class="level2" data-number="4.8">
<h2 data-number="4.8" class="anchored" data-anchor-id="references"><span class="header-section-number">4.8</span> References</h2>
<p>Antal, L., Shlomo, N., &amp; Elliot, M. (2014). Measuring disclosure risk with entropy in population based frequency tables. In <em>Privacy in Statistical Databases: UNESCO Chair in Data Privacy, International Conference, PSD 2014, Ibiza, Spain, September 17-19, 2014. Proceedings</em> (pp.&nbsp;62-78). Springer International Publishing.</p>
<p>Box, G. E. P. and Cox, D. R. (1964) <em>An analysis of transformations</em>, Journal of Royal Statistical Society, Series B, vol.&nbsp;26, pp.&nbsp;211—252.</p>
<p>Castro, J.(2002), <em>Network flows heuristics for complementary cell suppression: an empirical evaluation and extensions</em>, in LNCS 2316, Inference Control in Statistical Databases, J. Domingo-Ferrer (Ed), (2002) 59–73.</p>
<p>Castro, J. (2003 a), <em>Minimum-Distance Controlled Perturbation Methods for Large-Scale Tabular Data Protection</em>, accepted subject to revision to European Journal of Operational Research, 2003.</p>
<p>Castro, J. (2003 b) <em>User’s and programmer’s manual of the network flows heuristics package for cell suppression in 2D tables</em>. Technical Report DR 2003-07, Dept. of Statistics and Operations Research, Universitat Politècnica de Catalunya, Barcelona, Spain,2003; See <a href="https://research.cbs.nl/casc/deliv/41D6_NF1H2D-Tau-Argus.pdf">https://research.cbs.nl/casc/deliv/41D6_NF1H2D-Tau-Argus.pdf</a>.</p>
<p>Castro, J. (2006). <em>Minimum-distance controlled perturbation methods for large-scale tabular data protection</em>, European Journal of Operational Research, 171, 39–52.</p>
<p>Castro, J., Giessing S. (2006a). <em>Testing variants of minimum distance controlled tabular adjustment</em>, in Monographs of Official Statistics. Work session on Statistical Data Confidentiality, Eurostat-Office for Official Publications of the European Communities, Luxembourg, 2006, pp.&nbsp;333-343.</p>
<p>Catro, J., Giessing S. (2006b). <em>Quality issues minimum distance controlled tabular adjustment</em>, paper presented at the European Conference on Quality in Survey Statistics (Q2006), 24.-26. April 2006 in Cardiff.</p>
<p>Catro, J., Gonzalez J.A. (2009). <em>A Package for L1 Controlled Tabular Adjustment</em>, paper presented at the Joint UNECE/Eurostat Work Session on Statistical Data Confidentiality (Bilbao, 2-4 December 2009).</p>
<p>Cox, L. (1981), <em>Linear Sensitivity Measures in Statistical Disclosure Control</em>, Journal of Planning and Inference, 5, 153 - 164, 1981.</p>
<p>Cox, L. (2001), <em>Disclosure Risk for Tabular Economic Data</em>, In: Doyle, Lane, Theeuwes, Zayatz (Eds) Confidentiality, Disclosure, and Data Access: Theory and Practical Applications for Statistical Agencies, North-Holland.</p>
<p>Cox, L., Dandekar, R.H., (2002), <em>Synthetic Tabular Data – an Alternative to Complementary Cell Suppression</em>, unpublished manuscript.</p>
<p>Cox, L. H., Kelly, J. P., and Patil, R. (2004), <em>Balancing quality and confidentiality for multivariate tabular data</em>, Lecture Notes in Computer Science, 3050, 87–98.</p>
<p>Cox, L., Orelien, J. G., Shah, B. V. (2006), <em>A Method for Preserving Statistica Distributions Subject to Controlled Tabular Adjustment</em>, In: Domingo-Ferrer, Franconi (Eds.): Privacy in Statistical Databases 2006, Lecture Notes in Computer Science, Vol. 4302, Springer, Heidelberg (2006), p.1-11.</p>
<p>De Wolf, P.P. (2002), <em>HiTaS: A Heustic Approach to Cell Suppression in Hierarchical Tables</em>, In: Domingo-Ferrer (Ed.) Inference Control in Statistical Databases, Springer (Lecture notes in computer science; Vol. 2316).</p>
<p>De Wolf, P.P. (2007), <em>Cell suppression in a special class of linked tables</em>, paper presented at the Joint ECE/Eurostat Worksession on Statistical Confidentiality in Manchester, December 2007, available at <a href="https://unece.org/sites/default/files/datastore/fileadmin/DAM/stats/documents/ece/ces/2007/12/confidentiality/wp.21.e.pdf">https://unece.org/sites/default/files/datastore/fileadmin/DAM/stats/documents/ece/ces/2007/12/confidentiality/wp.21.e.pdf</a>.</p>
<p>De Wolf, P.P., Giessing, S. (2008) <em>How to make the <span class="math inline">\(\tau\)</span>‑ARGUS Modular Method Applicable to Linked Tables</em>. In: Domingo-Ferrer, Josep; Saygin, Yücel (Eds.): Privacy in Statistical Databases 2008, Lecture Notes in Computer Science , Vol. 5262, Springer, Heidelberg (2008), p.227-238.</p>
<p>Enderle, T., Giessing, S., Tent, R. (2018), <em>Designing Confidentiality on the Fly Methodology – Three Aspects</em>, In: Domingo-Ferrer, J., Montes, F. (eds) Privacy in Statistical Databases. PSD 2018. Lecture Notes in Computer Science, vol 11126. Springer, Cham. <a href="https://doi.org/10.1007/978-3-319-99771-1_3">https://doi.org/10.1007/978-3-319-99771-1_3</a>.</p>
<p>Enderle, T., Giessing, S., Tent, R. (2020), <em>Calculation of risk probabilities for the cell key method</em>, In: Doming-Ferre, J., Muralidhar, K. (eds) Privacy in Statistical Databases. PSD 2020. Lecture Notes in Computer Science, vol 12276. Springer, Cham. <a href="https://doi.org/10.1007/978-3-030-57521-2_11">https://doi.org/10.1007/978-3-030-57521-2_11</a>.</p>
<p>EU (2004), <em>Community statistics relating to the trading of goods between Member States</em>, <a href="https://eur-lex.europa.eu/LexUriServ/LexUriServ.do?uri=CELEX:32004R0638:EN:HTML">https://eur-lex.europa.eu/LexUriServ/LexUriServ.do?uri=CELEX:32004R0638:EN:HTML</a>.</p>
<p>Fraser, B. and Wooton, J. (2005), <em>A proposed method for confidentialising tabular output to protect against differencing</em> Work session on Statistical Data Confidentiality, <a href="https://www.unece.org/fileadmin/DAM/stats/documents/ece/ces/ge.46/2005/wp.35.e.pdf">https://www.unece.org/fileadmin/DAM/stats/documents/ece/ces/ge.46/2005/wp.35.e.pdf</a>.</p>
<p>Geurts, J. (1992), <em>Heuristics for Cell Suppression in Tables</em>, working paper, Netherlands Central Bureau of Statistics.</p>
<p>Giessing, S. and Repsilber, D. (2002), <em>Tools and Strategies to Protect Multiple Tables with the GHQUAR Cell Suppression Engine</em>, In: Domingo-Ferrer (Ed.) Inference Control in Statistical Databases, Springer Lecture Notes in Computer Science Vol. 2316.</p>
<p>Giessing, S. (2004), <em>Survey on methods for tabular protection in ARGUS</em>, Lecture Notes in Computer Science. Volume Privacy in statistical databases 3050, 1­13, J. Domingo-Ferrer and V. Torra, Springer, Berlin.</p>
<p>Giessing, S., Dittrich, S., Gehrling, D., Krüger, A., Merz, F.J., Wirtz, H. (2006), <em>Bericht der Arbeitsgruppe „Geheimhaltungskonzept des statistischen Verbundes, Pilotanwen­dung: Umsatzsteuerstatistik”</em>, document for the meeting of the „Ausschuss für Organisation und Umsetzung”, Mai 2006, in German.</p>
<p>Giessing, S., Hundepool, A., Castro, J. (2007) <em>Rounding Methods for Protecting EU-aggregates</em>, proceedings of the Joint UNECE/Eurostat Worksession on Statistical Confidentiality in Manchester, December 2007.</p>
<p>Fischetti, M, Salazar Gonzales, J.J. (2000), <em>Models and Algorithms for Optimizing Cell Suppression Problem in Tabular Data with Linear Constraints</em>, in Journal of the American Statistical Association, Vol. 95, pp 916.</p>
<p>Hundepool. A., Domingo-Ferrer. J., Franconi, L., Giessing S., Nordholt, E.S., Spicer, K. and de Wolf, P-P. (2012), <em>Statistical Disclosure Control</em>, series: Wiley Series in Survey Methodology, John Wiley &amp; Sons, Ltd., Chichester. ISBN: 978-1-119-97815-2.</p>
<p>Hundepool, A, Wetering, A van de, Ramaswamy, R, Wolf, P.P. de, Giessing, S, Fischetti, M, Salazar, JJ, Castro, J, Lowthian, P, (2014), <em><span class="math inline">\(\tau\)</span>-ARGUS 4.1 user manual</em>, Statistics Netherlands, Voorburg NL, Feb.&nbsp;2014. <a href="https://research.cbs.nl/casc/Software/TauManualV4.1.pdf">https://research.cbs.nl/casc/Software/TauManualV4.1.pdf</a>.</p>
<p>Ichim, D., Franconi, L. (2006), <em>Calibration estimator for magnitude tabular data protection</em>, Proceedings of the conference Privacy in Statistical Databases 2006, December 2006, Rome, Italy.</p>
<p>Loeve, A. (2001), <em>Notes on sensitivity measures and protection levels</em>, Research paper, Statistics Netherlands, available at <a href="https://research.cbs.nl/casc/Related/marges.pdf">https://research.cbs.nl/casc/Related/marges.pdf</a>.</p>
<p>Meindl, B. <em>Linking Complementary Cell Suppression and the Software R</em>, paper presented at the New Techniques and Technologies (NTTS) Conference, 18.-20. Feb.&nbsp;2009 in Brussels.</p>
<p>Salazar, J.J. (2003) <em>Partial Cell Suppression: a New Methodology for Statistical Disclosure Control</em>, Statistics and Computing, 13, 13-21.</p>
<p>Shlomo, N., Antal, L., &amp; Elliot, M. (2015). <em>Measuring disclosure risk and data utility for flexible table generators</em>. Journal of Official Statistics, 31(2), 305-324.</p>
<p>Repsilber, R. D. (1994), <em>Preservation of Confidentiality in Aggregated data</em>, paper presented at the Second International Seminar on Statistical Confidentiality, Luxembourg, 1994.</p>
<p>Repsilber, D. (2002), <em>Sicherung persönlicher Angaben in Tabellendaten</em>,&nbsp;In: Statistische Analysen und Studien Nordrhein-Westfalen, Landesamt für Datenverarbeitung und Statistik NRW, Ausgabe 1/2002 (in German).</p>
<p>Thompson, Gwenda/Broadfoot, Stephen/Elazar, Daniel (2013), <em>Methodology for the Automatic Confidentialisation of Statistical Outputs from Remote Servers at the Australian Bureau of Statistics</em>, Paper presented at the Joint UNECE/Eurostat Work Session on Statistical Data Confidentiality. Ottawa 2013. See <a href="https://www.unece.org/fileadmin/DAM/stats/documents/ece/ces/ge.46/2013/Topic_1_ABS.pdf">https://www.unece.org/fileadmin/DAM/stats/documents/ece/ces/ge.46/2013/Topic_1_ABS.pdf</a>.</p>
<p>Van der Meijden, R., (2006)‚ <em>Improving confidentiality with <span class="math inline">\(\tau\)</span>‑ARGUS by focussing on clever usage of microdata</em>, In: Monographs of Official Statistics. Work session on Statistical Data Confidentiality, Eurostat-Office for Official Publications of the European Communities, Luxembourg, 2006, pp.&nbsp;363-370.</p>
<p>Wolf, P.P. de (2002). <em>HiTaS: a heuristic approach to cell suppression in hierarchical tables</em>, In: Domingo-Ferrer (Ed.) Inference control in statistical databases, from theory to practice, Springer, LNCS 2316, pp.&nbsp;74–82.</p>


</section>


</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./03-microdata.html" class="pagination-link" aria-label="Microdata">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Microdata</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./05-frequency-tables.html" class="pagination-link" aria-label="Frequency tables">
        <span class="nav-page-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Frequency tables</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




<footer class="footer"><div class="nav-footer"><div class="nav-footer-center"><div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/sdcTools/HandbookSDC/issues" class="toc-action"><i class="bi bi-github"></i>Report an issue</a></li></ul></div></div></div></footer></body></html>