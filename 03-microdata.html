<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.545">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Handbook on Statistical Disclosure Control - 3&nbsp; Microdata</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./04-magnitude-tabular-data.html" rel="next">
<link href="./02-regulations.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating slimcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./03-microdata.html"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Microdata</span></a></li></ol></nav>
        <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Handbook on Statistical Disclosure Control</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/julienjamme/handbook_sdc_from_doc_to_md" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface to the second edition</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01-Introduction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02-regulations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Confidentiality in legal acts and ethical codes</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03-microdata.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Microdata</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04-magnitude-tabular-data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Magnitude tabular data</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05-frequency-tables.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Frequency tables</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06-remote-access-issues.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Remote access issues</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./07-glossary.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Glossary</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction"><span class="header-section-number">3.1</span> Introduction</a></li>
  <li><a href="#sec-roadmap" id="toc-sec-roadmap" class="nav-link" data-scroll-target="#sec-roadmap"><span class="header-section-number">3.2</span> A roadmap to the release of a microdata file</a>
  <ul class="collapse">
  <li><a href="#need-of-confidentiality-protection" id="toc-need-of-confidentiality-protection" class="nav-link" data-scroll-target="#need-of-confidentiality-protection"><span class="header-section-number">3.2.1</span> Need of confidentiality protection</a></li>
  <li><a href="#characteristics-and-uses-of-microdata" id="toc-characteristics-and-uses-of-microdata" class="nav-link" data-scroll-target="#characteristics-and-uses-of-microdata"><span class="header-section-number">3.2.2</span> Characteristics and uses of microdata</a></li>
  <li><a href="#disclosure-risk-ex-ante" id="toc-disclosure-risk-ex-ante" class="nav-link" data-scroll-target="#disclosure-risk-ex-ante"><span class="header-section-number">3.2.3</span> Disclosure risk (ex ante)</a></li>
  <li><a href="#sdc-methods" id="toc-sdc-methods" class="nav-link" data-scroll-target="#sdc-methods"><span class="header-section-number">3.2.4</span> SDC-methods</a></li>
  <li><a href="#implementation" id="toc-implementation" class="nav-link" data-scroll-target="#implementation"><span class="header-section-number">3.2.5</span> Implementation</a></li>
  <li><a href="#ex-post-assessment-of-disclosure-risk-and-information-loss" id="toc-ex-post-assessment-of-disclosure-risk-and-information-loss" class="nav-link" data-scroll-target="#ex-post-assessment-of-disclosure-risk-and-information-loss"><span class="header-section-number">3.2.6</span> Ex post assessment of disclosure risk and information loss</a></li>
  </ul></li>
  <li><a href="#sec-risk-assessment" id="toc-sec-risk-assessment" class="nav-link" data-scroll-target="#sec-risk-assessment"><span class="header-section-number">3.3</span> Risk assessment</a>
  <ul class="collapse">
  <li><a href="#sec-risk-overview" id="toc-sec-risk-overview" class="nav-link" data-scroll-target="#sec-risk-overview"><span class="header-section-number">3.3.1</span> Overview</a></li>
  <li><a href="#sec-disclosure-risk-scenarios" id="toc-sec-disclosure-risk-scenarios" class="nav-link" data-scroll-target="#sec-disclosure-risk-scenarios"><span class="header-section-number">3.3.2</span> Disclosure risk scenarios</a></li>
  <li><a href="#sec-concepts" id="toc-sec-concepts" class="nav-link" data-scroll-target="#sec-concepts"><span class="header-section-number">3.3.3</span> Concepts and notation</a></li>
  <li><a href="#argus-threshold-rule" id="toc-argus-threshold-rule" class="nav-link" data-scroll-target="#argus-threshold-rule"><span class="header-section-number">3.3.4</span> ARGUS threshold rule</a></li>
  <li><a href="#sec-argus-individual-risk" id="toc-sec-argus-individual-risk" class="nav-link" data-scroll-target="#sec-argus-individual-risk"><span class="header-section-number">3.3.5</span> ARGUS individual risk methodology</a></li>
  <li><a href="#sec-poisson-log-linear" id="toc-sec-poisson-log-linear" class="nav-link" data-scroll-target="#sec-poisson-log-linear"><span class="header-section-number">3.3.6</span> The Poisson model with log-linear modelling</a></li>
  <li><a href="#sec-SUDA" id="toc-sec-SUDA" class="nav-link" data-scroll-target="#sec-SUDA"><span class="header-section-number">3.3.7</span> SUDA</a></li>
  <li><a href="#sec-record-linkage" id="toc-sec-record-linkage" class="nav-link" data-scroll-target="#sec-record-linkage"><span class="header-section-number">3.3.8</span> Record Linkage</a></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references"><span class="header-section-number">3.3.9</span> References</a></li>
  </ul></li>
  <li><a href="#sec-microdata-protection-methods" id="toc-sec-microdata-protection-methods" class="nav-link" data-scroll-target="#sec-microdata-protection-methods"><span class="header-section-number">3.4</span> Microdata protection methods</a>
  <ul class="collapse">
  <li><a href="#overview-of-concepts-and-methods" id="toc-overview-of-concepts-and-methods" class="nav-link" data-scroll-target="#overview-of-concepts-and-methods"><span class="header-section-number">3.4.1</span> Overview of concepts and methods</a></li>
  <li><a href="#sec-perturbative-masking" id="toc-sec-perturbative-masking" class="nav-link" data-scroll-target="#sec-perturbative-masking"><span class="header-section-number">3.4.2</span> Perturbative masking</a></li>
  <li><a href="#sec-nonperturbative-masking" id="toc-sec-nonperturbative-masking" class="nav-link" data-scroll-target="#sec-nonperturbative-masking"><span class="header-section-number">3.4.3</span> Non-perturbative masking</a></li>
  <li><a href="#sec-noise-addition-details" id="toc-sec-noise-addition-details" class="nav-link" data-scroll-target="#sec-noise-addition-details"><span class="header-section-number">3.4.4</span> Noise addition</a></li>
  <li><a href="#sec-microaggregation-further" id="toc-sec-microaggregation-further" class="nav-link" data-scroll-target="#sec-microaggregation-further"><span class="header-section-number">3.4.5</span> Microaggregation: further details</a></li>
  <li><a href="#sec-PRAM" id="toc-sec-PRAM" class="nav-link" data-scroll-target="#sec-PRAM"><span class="header-section-number">3.4.6</span> PRAM</a></li>
  <li><a href="#sec-synthetic-data" id="toc-sec-synthetic-data" class="nav-link" data-scroll-target="#sec-synthetic-data"><span class="header-section-number">3.4.7</span> Synthetic microdata</a></li>
  </ul></li>
  <li><a href="#sec-informationloss-microdata" id="toc-sec-informationloss-microdata" class="nav-link" data-scroll-target="#sec-informationloss-microdata"><span class="header-section-number">3.5</span> Measurement of information loss</a>
  <ul class="collapse">
  <li><a href="#concepts-and-types-of-information-loss-and-its-measures" id="toc-concepts-and-types-of-information-loss-and-its-measures" class="nav-link" data-scroll-target="#concepts-and-types-of-information-loss-and-its-measures"><span class="header-section-number">3.5.1</span> Concepts and types of information loss and its measures</a></li>
  <li><a href="#information-loss-measures-for-categorical-data" id="toc-information-loss-measures-for-categorical-data" class="nav-link" data-scroll-target="#information-loss-measures-for-categorical-data"><span class="header-section-number">3.5.2</span> Information loss measures for categorical data</a></li>
  <li><a href="#information-loss-measures-for-continuous-data" id="toc-information-loss-measures-for-continuous-data" class="nav-link" data-scroll-target="#information-loss-measures-for-continuous-data"><span class="header-section-number">3.5.3</span> Information loss measures for continuous data</a></li>
  <li><a href="#complex-measures-of-information-loss" id="toc-complex-measures-of-information-loss" class="nav-link" data-scroll-target="#complex-measures-of-information-loss"><span class="header-section-number">3.5.4</span> Complex measures of information loss</a></li>
  <li><a href="#practical-realization-of-trade-off-between-safety-and-utility-of-microdata" id="toc-practical-realization-of-trade-off-between-safety-and-utility-of-microdata" class="nav-link" data-scroll-target="#practical-realization-of-trade-off-between-safety-and-utility-of-microdata"><span class="header-section-number">3.5.5</span> Practical realization of trade-off between safety and utility of microdata</a></li>
  <li><a href="#example" id="toc-example" class="nav-link" data-scroll-target="#example"><span class="header-section-number">3.5.6</span> Example</a></li>
  <li><a href="#references-5" id="toc-references-5" class="nav-link" data-scroll-target="#references-5"><span class="header-section-number">3.5.7</span> References</a></li>
  </ul></li>
  <li><a href="#sec-software" id="toc-sec-software" class="nav-link" data-scroll-target="#sec-software"><span class="header-section-number">3.6</span> Software</a>
  <ul class="collapse">
  <li><a href="#sec-mu-argus" id="toc-sec-mu-argus" class="nav-link" data-scroll-target="#sec-mu-argus"><span class="header-section-number">3.6.1</span> <span class="math inline">\(\mu\)</span>-ARGUS</a></li>
  <li><a href="#sec-sdcmicro" id="toc-sec-sdcmicro" class="nav-link" data-scroll-target="#sec-sdcmicro"><span class="header-section-number">3.6.2</span> sdcMicro</a></li>
  </ul></li>
  <li><a href="#sec-intro-example" id="toc-sec-intro-example" class="nav-link" data-scroll-target="#sec-intro-example"><span class="header-section-number">3.7</span> Introductory example: rules at Statistics Netherlands</a></li>
  <li><a href="#sec-further-example" id="toc-sec-further-example" class="nav-link" data-scroll-target="#sec-further-example"><span class="header-section-number">3.8</span> Further examples</a>
  <ul class="collapse">
  <li><a href="#labour-force-survey" id="toc-labour-force-survey" class="nav-link" data-scroll-target="#labour-force-survey"><span class="header-section-number">3.8.1</span> Labour Force Survey</a></li>
  <li><a href="#community-innovation-survey" id="toc-community-innovation-survey" class="nav-link" data-scroll-target="#community-innovation-survey"><span class="header-section-number">3.8.2</span> Community Innovation Survey</a></li>
  <li><a href="#references-6" id="toc-references-6" class="nav-link" data-scroll-target="#references-6"><span class="header-section-number">3.8.3</span> References</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Microdata</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="introduction" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="introduction"><span class="header-section-number">3.1</span> Introduction</h2>
<p>There is a strong, widespread and increasing demand for NSIs to release Microdata Files (MF), that is, data sets containing for each respondent the score on a number of variables. Microdata files are samples generated from business or social surveys or from the Census or originate from administrative sources. It is in the interest of users to make the microdata as detailed as possible but this interest conflicts with the obligation that NSIs have to protect the confidentiality of the information provided by the respondent.</p>
<p>In <a href="01-Introduction.html#sec-concepts-and-definitions" class="quarto-xref"><span>Section 1.1</span></a> two definitions of disclosure were provided: re-identification disclosure and attribute disclosure. In the microdata setting the re-identification disclosure concept is used as we are releasing information at individual level. When releasing microdata, an NSI must assess the risk of re-identifying statistical units and disclosing confidential information. There are different options available to NSIs for managing these disclosure risks, namely applying statistical disclosure control techniques, restricting access or a combination of the two.</p>
<p>Applying SDC methods leads to a loss of information and statistical content and affects the inferences that users are able to make on the data. The goal for an effective statistical disclosure control strategy is to choose optimum SDC techniques which maximize the utility of the data while minimizing the disclosure risk. On the other hand, the user of protected microdata should obtain most important information on the expected information loss due to SDC process. It enables the assessment of the impact of changes in the original data resulting from the need to protect statistical confidentiality on the quality of the final results of the estimates and analyzes carried out by him/her.</p>
<p>In general, two types of microdata files are released by NSIs, namely public use files (PUF) and research use files (MFR). The disclosure risk in public use files is entirely managed by the design of the file and the application of SDC methods. For research use microdata files SDC methods will be applied in addition to some restrictions on access and use, <em>e.g.</em> under a licence or access agreement, such as those provided by Commission Regulation 831/2202, see <a href="02-regulations.html#sec-laws" class="quarto-xref"><span>Section 2.3</span></a>. Necessarily the research release files contain more detail than the public use files. The estimated expected information loss should be computed both as total and for each variable separately, if possible.</p>
<p>Some NSIs will also provide access to microdata in datalaboratories/research centres or via remote access/execution. Datalabs allow approved users on-site access to more identifiable microdata. Typically datalab users are legally prohibited from disclosing information and are subject to various stringent controls, <em>e.g.</em> close supervision on-site to protect the security of the data and output checking, to assist with disclosure control. For remote execution researchers are provided with a full description of the microdata. They then send prepared scripts to the NSI who run the analysis, check and return the results. Remote access is a secure on-line facility where the researchers connect to the NSI’s server (via passwords and other security devices) where the data and programs are located. The researchers can submit code for analysis of microdata or in some instances see the files and programs ‘virtually’ on their desktops. Confidentiality protection is by a combination of microdata modification, automatic checks on output requested, manual auditing of output and a contractual agreement. The researcher does not have complete access to the whole data itself; however they may have access to a small amount of unit record information for the purpose of seeing the data structure before carrying out their analysis.</p>
<p><a href="#sec-roadmap" class="quarto-xref"><span>Section 3.2</span></a> goes through the whole process of creating a microdata file for external users from the original microdata. The aim of such section is to briefly analyse the different stages of the disclosure process providing references to the relevant sections where each step will be described in more details. <a href="#sec-software" class="quarto-xref"><span>Section 3.6</span></a> is dedicated to the software. <a href="#sec-intro-example" class="quarto-xref">Sections&nbsp;<span>3.7</span></a> and <a href="#sec-further-example" class="quarto-xref"><span>3.8</span></a> provide some examples. Further and more detailed examples can be found in Case studies available on the CASC website (<a href="https://research.cbs.nl/casc/handbook.htm#casestudies">https://research.cbs.nl/casc/handbook.htm#casestudies</a>). <a href="06-remote-access-issues.html">Chapter 6</a> of the current handbook provides more details on different microdata access issues such as research centres, remote access/execution and licensing.</p>
</section>
<section id="sec-roadmap" class="level2" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="sec-roadmap"><span class="header-section-number">3.2</span> A roadmap to the release of a microdata file</h2>
<p>This section aims at introducing the reader to the process that, starting from the original microdata file as it is produced by survey specialists, ends with the creation of a file for external users. This roadmap will drive you through the six stage process for disclosure, mostly outlined in <a href="01-Introduction.html#sec-5-stages" class="quarto-xref"><span>Section 1.2</span></a> <em>i.e.</em></p>
<ol type="1">
<li>why is confidentiality protection needed;</li>
<li>what are the key characteristics and use of the data;</li>
<li>disclosure risk (ex ante);</li>
<li>disclosure control methods;</li>
<li>implementation;</li>
<li>assessment of disclosure risk and utility (ex post)</li>
</ol>
<p>Specifying, for each stage, the peculiarities of microdata release. In <a href="#tbl-roadmap" class="quarto-xref">Table&nbsp;<span>3.1</span></a> we present an overview of the process.</p>
<div id="tbl-roadmap" class="quarto-float anchored">
<figure class="quarto-float quarto-float-tbl figure">
<div aria-describedby="tbl-roadmap-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="table">
<colgroup>
<col style="width: 29%">
<col style="width: 70%">
</colgroup>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>Stage of disclosure process</strong></td>
<td style="text-align: center;"><strong>Analyses to be carried out / problem to be addressed</strong><br>
⇓<br>
<strong><em>Results expected</em></strong></td>
</tr>
<tr class="even">
<td style="text-align: left;">1. Why is confidentiality protection needed</td>
<td style="text-align: center;">Does the data refer to individuals or legal entity?<br>
⇓<br>
<em>We need to protect the statistical unit</em></td>
</tr>
<tr class="odd">
<td rowspan="5" style="text-align: left;">2. What are the key characteristics and use of the data</td>
<td style="text-align: center;">Analysis of the type/structure of the data<br>
⇓<br>
<em>Clear vision of which units need protections</em></td>
</tr>
<tr class="even">
<td style="text-align: center;">Analysis of survey methodology<br>
⇓<br>
<em>Type of sampling frame, sample/complete enumeration of strata, further analysis of survey methodology, calibration</em></td>
</tr>
<tr class="odd">
<td style="text-align: center;">Analysis of NSI objectives<br>
⇓<br>
<em>Type of release (PUF, MFR),dissemination policies, peculiarities of the phenomenon, coherence between multiple releases (PUF and MFR), coherence with released tables and on-line databases, etc.</em></td>
</tr>
<tr class="even">
<td style="text-align: center;">Analysis of user needs<br>
⇓<br>
<em>Priorities for variables, type of analysis, etc.</em></td>
</tr>
<tr class="odd">
<td style="text-align: center;">Analysis of the questionnaire<br>
⇓<br>
<em>List of variables to be removed, variables to be included, some ideas of level of details of structural variables</em></td>
</tr>
<tr class="even">
<td rowspan="3" style="text-align: left;">3. Disclosure risk (ex ante)</td>
<td style="text-align: center;">Disclosure scenario<br>
⇓<br>
<em>List of identifying variables</em></td>
</tr>
<tr class="odd">
<td style="text-align: center;">Definition of risk<br>
⇓<br>
<em>Risk measure</em></td>
</tr>
<tr class="even">
<td style="text-align: center;">Risk assessment<br>
⇓<br>
<em>If the risk is deemed too high need of disclosure limitation methods</em></td>
</tr>
<tr class="odd">
<td rowspan="2" style="text-align: left;">4. Disclosure limitation methods</td>
<td style="text-align: center;">Analysis of type of data involved, NSI policies and users needs<br>
⇓<br>
<em>Identification of a disclosure limitation method</em></td>
</tr>
<tr class="even">
<td style="text-align: center;">Information loss analysis</td>
</tr>
<tr class="odd">
<td style="text-align: left;">5. Implementation</td>
<td style="text-align: center;">Choice of software, parameters and thresholds for different methods</td>
</tr>
<tr class="even">
<td style="text-align: left;">6. assessment of disclosure risk and utility (ex post)</td>
<td style="text-align: center;">Ex post analysis of disclosure risk and information loss<br>
⇓<br>
<em>In case disclosure risk and/or utility loss is too high, return to step 4 or 5.</em></td>
</tr>
</tbody>
</table>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-tbl" id="tbl-roadmap-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;3.1: Roadmap to releasing a microdata file
</figcaption>
</figure>
</div>
<p>The idea is to identify for each stage of the process choices that have to be made, analyses that need to be done, problems that need to be addressed and methods to be selected. References to the relevant sections where technical topics are discussed in detail will help the beginners in following the process without getting lost in too technical aspects.</p>
<p>We now analyse in turn each of the six stages.</p>
<section id="need-of-confidentiality-protection" class="level3" data-number="3.2.1">
<h3 data-number="3.2.1" class="anchored" data-anchor-id="need-of-confidentiality-protection"><span class="header-section-number">3.2.1</span> Need of confidentiality protection</h3>
<p>The starting point deals with the need of confidentiality protection which is at the base of any release of microdata. If the microdata do not refer to legal entity or individual persons it can be released without confidentiality protection: an example is the amount of rain fall in a region. If microdata pertain only of public variables, in most cases they might be released: the legislation usually treats such data as excluded from statistical confidentiality. However, in general, data refer to individual or enterprises and contains confidential variables (health related data, income, turnover, expenses, etc.) and therefore need to be protected.</p>
</section>
<section id="characteristics-and-uses-of-microdata" class="level3" data-number="3.2.2">
<h3 data-number="3.2.2" class="anchored" data-anchor-id="characteristics-and-uses-of-microdata"><span class="header-section-number">3.2.2</span> Characteristics and uses of microdata</h3>
<p>Of course different levels of protection are needed for different type of users. This theme leads us to the second stage of the process <em>i.e.</em> the study of the key uses and characteristics of the data. Here the initial question is whether the microdata file we are going to release is intended for a general public (public use file) or whether it is created for research purpose (research use files). In the latter case the microdata will be released according to predefined procedures and legal binding (see also <a href="06-remote-access-issues.html#sec-licensing" class="quarto-xref"><span>Section 6.5</span></a>). The difference in user’s type implies different user’s needs, different disclosure scenarios, different types of analyses we expect to be performed with the released data, different statistics we may intend to preserve and different amount of protection we intend to apply. We now analyse all these issues in terms.</p>
<p><em>Type and structure of data</em><br>
Analysis of user needs involves first a study of the survey information content. This should be done together with a survey expert that has a deeper knowledge of the data, phenomenon and possible types of analysis that can be performed on the data.</p>
<p>Typical questions that need to be addressed are:</p>
<p><em>Which statistical units are involved in the survey?</em> Individuals, enterprises, households, etc. The type of units has a big influence on the risk assessment stage.</p>
<p><em>Do data present a particular structure</em>? Hierarchical data: students inside schools, graduates inside universities, employees inside an enterprise, individual inside household etc. If this is the case care needs to be taken in checking both levels/types of units involved. <em>E.g.</em>, do schools/universities/enterprises need to be protected besides students/graduates/employees?</p>
<p><em>What type of sampling design has been used?</em> Are there strata (or units of earlier stages in a two- or multistage sampling design) which have been censured? Of course a complete enumeration of a strata (typical in business surveys) implies different and higher risks than a sample. Is two- or multistage sampling used with different types of units in the different stages?</p>
<p>An analysis of the questionnaire is useful to analyse the type of information present in the file: possible identifying variables (of which identifiers and quasi-identifiers), confidential variables and sensitive variables.</p>
<p><em>Preliminary work on variables</em><br>
In this stage the setting of objectives from the viewpoint of the NSI and the user are defined. From the NSI side dissemination policies are clarified (<em>e.g.</em> level of dissemination of NACE, geography, etc. or coherence with published tables). From the user point of view a list of priorities in the structural variables of the survey, requests for minimum level of details for such variables and type of analysis to be performed (ratios, weighted totals, regressions, etc).</p>
<p>The characteristics of the phenomenon under study should also be considered as well as the dissemination policy of the Statistical Institute. This is particularly true for example in business data where some NACE classifications may never be released by their own, but always aggregated with others. Such a-priori aggregations generally depend on the economic structure of the country. It is not a sampling or dissemination problem, but rather a feature of the surveyed phenomenon. This will bring to aggregation of categories of some identifying variables deemed too detailed.</p>
<p>The output of this questionnaire analysis should be a preliminary list of variables to be removed and those to be released (because relevant to users need) together with some ideas of their level of details (depending on whether we are releasing a public use file or a research use file). Some examples to clarify these ideas. Variables that shouldn’t be released comprise variables used as internal checks (<em>e.g.</em> some paradata), flags for imputation, variables that were not validated, variables deemed as not useful because containing too many missing values, information on the design stratum from which the unit comes from etc. Obviously also direct identifiers should not be released. The case studies A1 and A2 on microdata release provide examples of such stage.</p>
<p>Categories of identifying variables with too significant identifying power are commonly aggregated into a single category.</p>
<p>This is particularly true when releasing public use files as certain variables when too detailed could retain a level of “sensitivity”. This may not be felt useful and/or appropriate for the general public. For example, in an household expenditure survey we might avoid releasing for the public use file very detailed information on the expenditure for housing (mortgage, rent) or detailed information on the age of the house or its number of rooms (when this is very high) as these might be considered as giving too much information for particular outlying cases.</p>
<p><em>Geography</em><br>
Another example is related to the level of geographical details that maybe different for a public use file or a research use file (especially if a data limitation technique is used). This happens because geographical information is a strongly identifying variable. Moreover, the geographical information collected from the respondent may be available in different variables for different purposes (place of birth, place of residence, place of work, place of study, commuting, etc.). All such geographical details need to be coherent/consistent throughout the file. To this end it may be convenient releasing relative information instead of absolute one: for example place of residence can be given at a certain detail (<em>e.g.</em> region) and then the other geographical information (place of work, study etc.) can be released with respect to this one. Examples of possible relative recodings (<em>e.g.</em> with respect to region of residence) are: region of work same as region of residence, different region but same macroregion, different macroregion.</p>
<p><em>Coherence with published tables</em><br>
At this initial stage of the analysis information should be collected on what has already been published/what it is going to be released from the microdata set: dissemination plan, which type of tables and what classification/aggregation was used for the variables. This is to avoid different classifications in different release: the geographical breakdown, as well as classification of other variables in the survey (<em>e.g.</em> age, type of work etc.), should be coherent with the published marginals. For example, if a certain classification of the variable age is published in a table the microdata file should use a classification which has compatible break points so that to avoid gaining information by differencing. Release of date of birth is highly discouraged. Also, as far as possible, published totals should be preserved for transparency.</p>
</section>
<section id="disclosure-risk-ex-ante" class="level3" data-number="3.2.3">
<h3 data-number="3.2.3" class="anchored" data-anchor-id="disclosure-risk-ex-ante"><span class="header-section-number">3.2.3</span> Disclosure risk (ex ante)</h3>
<p>Moreover, in case of multiple release of the same survey (<em>e.g.</em> PUF and microdata for research) coherence should be maintained also between different released files in the sense that releasing different files at the same time shouldn’t allow the gaining of more information than for one file alone (see, Trottini et al., 2006). The principles apply also to the release of longitudinal or panel microdata, where the differences between records pertaining to the same case in different waves will reflect ‘events’ that have occurred to that case, as well as the attributes of the individuals.</p>
<p>Once the characteristics and uses of the survey data are clear, it is time to start the real analysis of the disclosure risk in relation to files with originally collected data – <em>ex ante</em> assessment (in one of next subsections we will indicated also on a necessity of making <em>ex post</em> assessment of disclosure risk to verify effciency of used SDC methods). This implies first a definition of possible situations at risk (disclosure scenarios) and second a proper definition of the ‘risk’ in order to quantify the phenomenon (risk assessment).</p>
<p><em>Disclosure scenario</em><br>
A disclosure scenario is the definition of realistic assumptions about what an intruder might know about respondents and what information would be available to him to match against the microdata to be released and potentially make an identification and disclosure.</p>
<p>Again different types of releases may require different disclosure scenarios and different definitions of risk. For example the nosy neighbourhood scenario described in <a href="#sec-disclosure-risk-scenarios" class="quarto-xref"><span>Section 3.3.2</span></a>, possibly with knowledge of the presence of the respondent in the sample (implying that sample uniques are a relevant quantity of interest for risk definition), may be deemed adequate for a public use file. A different trust might be put in a researcher that needs to perform an analysis for research purposes. This implies, as a minimum step, a higher level of acceptable risk and a different scenario the spontaneous identification scenario.</p>
<p><em>Spontaneous recognition</em><br>
Spontaneous recognition is possible when the researchers unintentionally recognize some units. For example, when releasing enterprise microdata, it is publicly known that the largest enterprises are generally included in the microdata file because of their significant impact on the studied phenomenon. Moreover, the largest enterprises are also the most identifiable ones as recognisable by all (the largest car producer factory, the national mail delivery enterprise, etc.). Consequently, a spontaneous identification or recognition might occur. A description of different scenarios is presented in <a href="#sec-disclosure-risk-scenarios" class="quarto-xref"><span>Section 3.3.2</span></a>; examples of spontaneous identification scenarios for MFR are reported in case studies A1 and A2.&nbsp;</p>
<p><em>Definition of risk</em><br>
From the adopted scenario we can extract the list of identifying variables <em>i.e.</em> the variables that may allow the identification of a unit. These will be the basis for defining the risk of disclosure. Intuitively, a unit is at risk of identification when it cannot be confused with several other units. The difficulty is to express this simple concept using sound statistical methodology.</p>
<p>Different approaches are used if the identifying variables are categorical of continuous. In the former case at the basis of the definition is the concept of a ‘key’ (<em>i.e.</em> the combination of categories of the identifying variables): see <a href="#sec-risk-overview" class="quarto-xref"><span>Section 3.3.1</span></a> for a classification of different definitions. Whereas if continuous identifying variables are present in the file a possibility is to use the concept of density: see Ichim&nbsp;(2009) for a detailed analysis of definitions of risk in the case of continuous variables. Of course, the problem is even more complicated when we deal with a mixture of categorical and numerical key variables; for an example of this situation (quite common in enterprise microdata) see case study A1 (Community Innovation Survey). Another solution in this context can be the assessment of disclosure risk based on the (expected) number of units for which a value of the given continuous variable falls into the respectively defined (using established threshold of deviation) neighborhood of a given observation. This approach can be perceived as some variation of <span class="math inline">\(k\)</span>-anonymity in this case.<br>
</p>
<p><em>Risk assessment</em><br>
Once a formal definition of risk has been chosen we need to measure/estimate it. There are several possibilities for categorical identifying variables (these are reported in various subsections of <a href="#sec-risk-assessment" class="quarto-xref"><span>Section 3.3</span></a>) and for a mixture of categorical and continuous identifying variable we have already mentioned Case study A1. The final step of the risk assessment is the definition of a threshold to define when a unit or a file presents an acceptable risk and when, on the contrary, it has to be considered at risk. This threshold depends of course on the type of measure adopted and details on how to choose a threshold are reported in the relevant subsequent sections.</p>
<p>Choice of scenarios and level of acceptable risk are extremely dependent on different cultural situations in different member states, different policies applied by different institutes, different approaches to statistical analysis, different perceived risk. To this end it must be stressed that different countries may have extremely different situation/phenomenon therefore different scenarios and risk methods are indeed necessary.</p>
<p>Currently there is no general agreement on which risk methodology is best although different methods give in general similar answers for the extreme cases. However, as already stated in <a href="#sec-risk-overview" class="quarto-xref"><span>Section 3.3.1</span></a>, there is a strong need to further compare and understand differences between available methods. Pros and cons of each method are described in the relevant sections may be used as a guidelines for the most appropriate choice of the risk estimation in different situations. Further advice can be gained by studying of the examples and case studies.</p>
</section>
<section id="sdc-methods" class="level3" data-number="3.2.4">
<h3 data-number="3.2.4" class="anchored" data-anchor-id="sdc-methods"><span class="header-section-number">3.2.4</span> SDC-methods</h3>
<p>If the risk assessment stage shows that the disclosure risk is high then the application of statistical disclosure limitation methods is necessary to produce a microdata file for external users.</p>
<p><em>Masking methods</em><br>
Microdata protection methods can generate a protected microdata set either by <em>masking original data</em>, <em>i.e.</em> generating a modified version of the original microdata set or by <em>generating synthetic data</em> that preserve some statistical properties of the original data. Synthetic data are still difficult to implement; a description can be found in <a href="#sec-synthetic-data" class="quarto-xref"><span>Section 3.4.7</span></a>. Masking methods are divided into two categories depending on their effect on the original data&nbsp;(Willenborg and De Waal, 2001): <em>perturbative</em> and <em>non perturbative masking</em> methods.</p>
<p>Perturbative methods either modify the identifying variables or modify the confidential variables before publication. In the former way, unique combinations of scores of identifying variables in the original dataset may disappear and new unique combinations may appear in the perturbed dataset. In this way a user cannot be certain of an identification. Alternatively confidential variables can be modified; in this case even if an identification occurs, the wrong value is associated and disclosure of the original value is avoided (for an example of this case see case study A2). For a description of a variety of perturbative methods see sections <a href="#sec-perturbative-masking" class="quarto-xref"><span>3.4.2</span></a>, <a href="#sec-noise-addition-details" class="quarto-xref"><span>3.4.4</span></a>, <a href="#sec-microaggregation-further" class="quarto-xref"><span>3.4.5</span></a> and <a href="#sec-PRAM" class="quarto-xref"><span>3.4.6</span></a>.</p>
<p>Non-perturbative methods do not alter the values of the variables (either identifying or confidential); rather, they produce a reduction of detail in the original dataset. Examples of non-perturbative masking are presented in <a href="#sec-nonperturbative-masking" class="quarto-xref"><span>Section 3.4.3</span></a>.</p>
<p>The choice between a data reduction and a data perturbation method strongly depends on the policy of an institute and on the type of data/survey to be released. While the policy of an institute is outside of this debate, technical reasons may suggest the use of perturbative methods for the protection of continuous variables (mainly business data). Analysis of information loss should always be part of the selection process. The usual difference between types of release remains valid and it is linked to the difference between users needs. Again the examples and the case studies A1 and A2 may help in clarifying different situations.</p>
<p><em>User needs and types of protection</em><br>
From the needs of the users and the types of analyses that could be performed on the data one could gain information for the choice of the type of protection that could be applied to the microdata. Also users could express priorities in the need of maintaining some variables intact (<em>e.g.</em>, for business usually NACE is the most important variable, then employees, and so on).</p>
<p><em>Information loss</em><br>
For research purposes maybe we could be interested in maintaining the possibility of being able to reproduce the published tables. For a public use file maybe we could avoid, as much as possible, the use of local suppression as this may render data analysis difficult for non sophisticated users. In general, the implementation of perturbative methods should take into account what variables and relationships among them need to be kept from the user point of view. An assessment of information loss caused by the protection methods adopted is highly recommended. A brief description of information loss measures is reported in <a href="#sec-informationloss-microdata" class="quarto-xref"><span>Section 3.5</span></a>; examples of how to check in practice the amount of distortion or modification in the protected microdata is presented in case studies A1 and A2.</p>
<p>Finally, every time a data perturbation method is applied attention should be placed at relationships between different types of release (PUF, MFR, tables) so as to avoid as much as possible, different marginal totals from different sources.</p>
<p>An example of the application of this reasoning for the definition of a dissemination strategy can be found, for example, in Trottini et al.&nbsp;(2006).</p>
</section>
<section id="implementation" class="level3" data-number="3.2.5">
<h3 data-number="3.2.5" class="anchored" data-anchor-id="implementation"><span class="header-section-number">3.2.5</span> Implementation</h3>
<p>The next stage of the SDC process is the implementation of the whole procedure, choice of software, parameters and levels of acceptable risks.</p>
<p>Documentation is an essential part of any dissemination strategy both for auditing from external authorities and transparency towards users. The former may include description of legal and administrative steps for a risk management policy together with the technical solution applied. The latter is essential for a user to understand what has been changed or limited in the data because of confidentiality constraints. If a data perturbation method has been applied then, for transparency reasons, this should be clearly stated. Information on which statistics have been preserved and which have been modified and some order of magnitude of possible changes should be provided as far as possible. If a data reduction method has been applied with some local suppression then the distribution of such suppressions should be given for a series of different dimensions of interest (distribution by variables, by household size, household type, etc.) and any other statistics that are deemed relevant for the user. The released microdata should be obviously accompanied by all necessary metadata and information on methodologies used at various stage of the survey process (sampling, imputation, validation, etc.) together with information on magnitude of sampling errors, estimation domains etc.</p>
</section>
<section id="ex-post-assessment-of-disclosure-risk-and-information-loss" class="level3" data-number="3.2.6">
<h3 data-number="3.2.6" class="anchored" data-anchor-id="ex-post-assessment-of-disclosure-risk-and-information-loss"><span class="header-section-number">3.2.6</span> Ex post assessment of disclosure risk and information loss</h3>
<p>The last - but, of course, not least - stage of the procedure is the ex post assessment of disclosure risk and computation of the expected information loss due to SDC. The ex post risk assessment (usually made using the same measures as in the case of ex ante assessement, for comparability) allows for confirmation whether the used procedure eliminates or sufficiently reduces the threat of unit identification or not. If not, a modification of used methods (<em>e.g.</em> by changing some tools, modification of parameters, etc.) should be made. This means back to either the step “Disclosure limitation methods” or the step “Implementation”.</p>
<p>An assessment of information loss caused by the applied protection methods is highly recommended. The knowledge of possible loss of information is key for data utility for possible users. If the information loss is too great then the used methods or their parameterization should be changed (coming back to step “Disclosure limitation methods”). One should remain that simultaneously the disclosure risk should be also as small as possible. Thus, these quantities should be harmonized. A detailed description of information loss measures is reported in <a href="#sec-informationloss-microdata" class="quarto-xref"><span>Section 3.5</span></a>; examples of how to check in practice the amount of distortion or modification in the protected microdata is presented in case studies A1 and A2.</p>
<p>Of course, the results of computation of discloure risk (both final and indirect, if applicable) and information loss should be saved in the documentation of the whole process. However, whereas the values of measures of disclosure risk are confidential and known only by entitled staff of the data holder, the level of expected information loss should be made available to the user. It is a very important factor influencing the quality of the final analysis results obtained by him/her.</p>
</section>
</section>
<section id="sec-risk-assessment" class="level2" data-number="3.3">
<h2 data-number="3.3" class="anchored" data-anchor-id="sec-risk-assessment"><span class="header-section-number">3.3</span> Risk assessment</h2>
<section id="sec-risk-overview" class="level3" data-number="3.3.1">
<h3 data-number="3.3.1" class="anchored" data-anchor-id="sec-risk-overview"><span class="header-section-number">3.3.1</span> Overview</h3>
<p>Microdata has many analytical advantages over aggregated data, but also poses more serious disclosure issues because of the many variables that are disseminated in one file. For microdata, disclosure occurs when there is a possibility that an individual can be re-identified by an intruder using information contained in the file, and when on the basis of that, confidential information is obtained. Microdata are released only after taking out directly identifying variables, such as names, addresses, and identity numbers. However, other variables in the microdata can be used as indirect identifying variables. For individual microdata this are variables such as gender, age, occupation, place of residence, country of birth, family structure, etc. and for business microdata variables such as economic activity, number of employees, etc. These (indirect) identifying variables are mainly publicly available variables or variables that are present in public databases such as registers.</p>
<p>If the identifying variables are categorical then the compounding (cross-classification) of these variables defines a key. The disclosure risk is a function of such identifying variables/keys either in the sample alone or in both the sample and the population.</p>
<p>To assess the disclosure risk, we first need to make realistic assumptions about what an intruder might know about respondents and what information will be available to him to match against the microdata and potentially make an identification and disclosure. These assumptions are known as disclosure risk scenarios and more details and examples are provided in the next section of this handbook. Based on the disclosure risk scenario, the identifying variables are determined. The other variables in the file are confidential or sensitive variables and represent the data not to be disclosed. NSIs usually view all non-publicly available variables as confidential/sensitive variables regardless of their specific content, though there can be some variables, <em>e.g.</em> sexual identity, health conditions, income, that can be more sensitive.</p>
<p>In order to undertake a risk assessment of microdata, NSIs might rely on <em>ad-hoc</em> methods, experience and checklists based on assessing the detail and availability of identifying variables. There is a clear need for obtaining quantitative and objective disclosure risk measures for the risk of re-identification in the microdata. For microdata containing censuses or registers, the disclosure risk is known as we have all identifying variables available for the whole population. However, for microdata containing samples the population base is unknown or partially known through marginal distributions. Therefore, probabilistic modelling or heuristics are used to estimate disclosure risk measures at population level, based on the information available in the sample. This section provides an overview of methods and tools that are available in order to estimate quantitative disclosure risk measures.</p>
<p>Intuitively, a unit is at risk if we are able to single it out from the rest. The idea at the base of the definition of risk is a way to measure rareness of a unit either in the sample or in the population.</p>
<p>When the identifying variables are categorical (as it is usually the case in social surveys) the risk is cast in terms of the cells of the contingency table built by cross-tabulating the identifying variables: the keys. Consequently all the records in the same cell have the same value of the risk.</p>
<p><em>A classification of risk measures</em><br>
Several definitions of risk have been proposed in the literature; here we focus mainly on those for which tools are available to compute/estimate them easily. We can broadly classify disclosure risk measures into three types: risk measures based on keys in the sample, those based on keys in the population and that make use of statistical models or heuristics to estimate the quantities of interest and those based on the theory of record linkage. Whereas the first two classes are devoted to risk assessment for categorical identifying variables the third one may be used for categorical and continuous variables.</p>
<p><em>Risk based on keys in the sample</em><br>
For the first class of risk measures a unit is at risk if its combination of scores on the identifying variables is below a given threshold. The threshold rule used within the software package <span class="math inline">\(\mu\)</span>‑ARGUS is an example of this class of risk measures.</p>
<p><em>Risk based on keys in the population</em><br>
For the second type of approach we are concerned with the risk of a unit as determined by its combination of scores on the identifying variables within the population or its probability of re-identification. The idea then is that a unit is at risk if such quantity is above a given threshold. Because the frequency in the population is generally unknown, it may be estimated through a modelling process. Examples of this reasoning are the individual risk of disclosure based on the Negative Binomial distribution developed by Benedetti and Franconi (1998) and Franconi and Polettini (2004), which is outlined in <a href="#sec-argus-individual-risk" class="quarto-xref"><span>Section 3.3.5</span></a>, and the one based on the Poisson distribution and log-linear models developed by Skinner and Holmes (1998) and Elamir and Skinner (2004) which is described in <a href="#sec-poisson-log-linear" class="quarto-xref"><span>Section 3.3.6</span></a> along with current research on other probabilistic methods. Another approach based on keys in the population is the Special Uniques Detection (SUDA) Algorithm developed by Elliot et al.&nbsp;(2002) that uses a heuristic method to estimate the risk; this is outlined in <a href="#sec-SUDA" class="quarto-xref"><span>Section 3.3.7</span></a>.</p>
<p><em>Risk based on record linkage</em><br>
When identifying variables are continuous we cannot exploit the concept of rareness of the keys and we transform such concept into rareness in the neighbourhood of the record. A way to measure rareness in the neighbourhood is through record linkage techniques. This third class of disclosure risk is covered in <a href="#sec-record-linkage" class="quarto-xref"><span>Section 3.3.8</span></a>.</p>
<p><a href="#sec-disclosure-risk-scenarios" class="quarto-xref"><span>Section 3.3.2</span></a> provides an introduction to disclosure risk scenarios and <a href="#sec-concepts" class="quarto-xref"><span>Section 3.3.3</span></a> introduces concepts and notation used throughout this chapter. Sections to <a href="#sec-record-linkage" class="quarto-xref"><span>3.3.8</span></a> describe different approaches to microdata risk assessment as specified above. However, as microdata risk assessment is a novelty in statistical research there isn’t yet agreement on what method is the best, or at least best under given circumstances. In the following sections we comment on various approaches to risk measures and try to give advice on situations where they could or could not be applied. In any case, it has been recognised that research should be undertaken to evaluate these different approaches to microdata risk assessment, see for example Shlomo and Barton (2006).</p>
<p>The focus of these methods and this section of the handbook is for microdata samples from social surveys. For microdata samples from censuses or registers the disclosure risk is known. Business survey microdata are not typically released due to their disclosive nature (skewed distributions and very high sampling fractions).</p>
<p>In <a href="#sec-intro-example" class="quarto-xref"><span>Section 3.7</span></a> we make some suggestions on practical implementation and in <a href="#sec-further-example" class="quarto-xref"><span>Section 3.8</span></a> we give examples of real data sets and ways in which risk assessment could be carried out.</p>
</section>
<section id="sec-disclosure-risk-scenarios" class="level3" data-number="3.3.2">
<h3 data-number="3.3.2" class="anchored" data-anchor-id="sec-disclosure-risk-scenarios"><span class="header-section-number">3.3.2</span> Disclosure risk scenarios</h3>
<!-- *Need to add section from Luisa. My feeling is that as it stands this was quite long and could be simplified.*

*Ok, but later ... when we will be close to the final version* -->
<p>The definition of a disclosure scenario is a first step towards the development of a strategy for producing a “safe” microdata file (MF). A scenario synthetically describes (i) which is the information potentially available to the intruder, and (ii) how the intruder would use such information to identify an individual <em>i.e.</em> the intruder’s attack means and strategy. Often, defining more than one scenario might be convenient, because different sources of information might be alternatively or simultaneously available to the intruder. Moreover, re-identification risk can be assessed keeping into account different scenarios at the same time.</p>
<p>We refer to the information available to the intruder as an External Archive (EA), where information is provided at individual level, jointly with directly identifying data, such as name, surname, etc. The disclosure scenario is based on the assumption that the EA available to the intruder is an individual microdata archive. That is, for each individual directly identifying variables, and some other variables are available. Some of these further variables are assumed to be available also in the MF that we want to protect. The intruder’s strategy of attack would be to use this overlapping information to match direct identifier to a record in the MF. The matching variables are then the <em>identifying variables</em>.</p>
<p>We consider two different types of re-identification, <em>spontaneous recognition</em> and <em>re-</em>identification <em>via record matching</em> (or <em>linkage</em>) according to the information we assume to be available to the intruder. In the first case we consider that the intruder might rely on personal knowledge about one or a few target individuals, and spontaneously recognize a surveyed individual (<em>Nosy Neighbour scenario</em>). In such a case the External Archive contains one (or a few) records relative to <em>detailed</em> personal information. In the second case, we assume that the intruder (who might be an MF user) has access to a <em>public register</em> and that he or she tries to match the information provided by this EA, with that provided by the MF, in order to identify surveyed units. In such a case, the intruder’s chance of identifying a unit depends on the EA main characteristics, such as completeness, accuracy and data classification. Broadly speaking, we assume that the intruder has a lower chance of correctly identifying an individual when the information provided by the EA is not update, complete, accurate, or is classified according to standards different by those used in the statistical survey.</p>
<p>Moreover, as far as statistical disclosure control is concerned, experts are used to distinguish between social and economic microdata (without loss of generality we can consider respectively individuals and enterprises). In fact, the concept of disclosure risk is mainly based on the idea of rareness with respect to a set of identifying variables. For social survey microdata, because of the characteristics of the population under investigation and the nature of the data collected, identifying variables are mainly (or exclusively) categorical. For much of the information collected on enterprises however the identifying variables often take the form of quantitative variables with asymmetric distributions (Willenborg and de Waal, 2001). Disclosure scenarios are then described according to this statement.</p>
<p>The case study part of the Handbook&nbsp;contains examples of the Nosy Neighbour scenario and the EA scenario for social survey data. The issues involved with hierarchical and longitudinal data are also addressed. Finally, scenarios for business survey data are discussed.</p>
<p>In any case the definition of the scenario is essential as it defines the hypothesis underneath the risk estimation and the subsequent protection of the data.</p>
</section>
<section id="sec-concepts" class="level3" data-number="3.3.3">
<h3 data-number="3.3.3" class="anchored" data-anchor-id="sec-concepts"><span class="header-section-number">3.3.3</span> Concepts and notation</h3>
<p>For microdata, disclosure risk measures quantify the risk of re-identification. Individual per record disclosure risk measures are useful for identifying high-risk records and targeting the SDC methods. These individual risk measures can be aggregated to obtain global file level disclosure risk measures. These global risk measures are particularly useful to NSIs for their decision making process on whether the microdata is safe to be released and allows comparisons across different files.</p>
<p><em>Microdata disclosure</em><br>
Disclosure in a microdata context means a correct record re-identification operation that is achieved by an intruder when comparing a target individual in a sample with an available list of units (external file) that contains individual identifiers such as name and address plus a set of identifying variables. Re-identification occurs when the unit in the released file and a unit in the external file belong to the same individual in the population. The underlying hypothesis is that the intruder will always try to match a unit in the sample <span class="math inline">\(s\)</span> to be released and a unit in the external file using the identifying variables only. In addition, it is likely that the intruder will be interested in identifying those sample units that are unique on the identifying variables. A re-identification occurs when, based on a comparison of scores on the identifying variables, a unit <span class="math inline">\(i^*\)</span> in the external file is selected as matching to a unit <span class="math inline">\(i\)</span> in the sample and this link is correct and therefore confidential information about the individual is disclosed using the direct identifiers.</p>
<p>To define the disclosure scenario, the following assumptions are made. Most of them are conservative and contribute to the definition of a worst case scenario:</p>
<ol type="1">
<li>a sample <span class="math inline">\(s\)</span> from a population <span class="math inline">\(\mathcal{P}\)</span> is to be released, and sampling design weights are available;</li>
<li>the external file available to the intruder covers the whole population <span class="math inline">\(\mathcal{P}\)</span>; consequently for each <span class="math inline">\(i \in s\)</span> the matching unit <span class="math inline">\(i^*\)</span> does always exist in <span class="math inline">\(\mathcal{P}\)</span>;</li>
<li>the external file available to the intruder contains the individual direct identifiers and a set of categorical identifying variables that are also present in the sample;</li>
<li>the intruder tries to match a unit <span class="math inline">\(i\)</span> in the sample with a unit <span class="math inline">\(i^*\)</span> in the population register by comparing the values of the identifying variables in the two files;</li>
<li>the intruder has no extra information other than that contained in the external file;</li>
<li>a re-identification occurs when a link between a sample unit <span class="math inline">\(i\)</span> and a population unit <span class="math inline">\(i^*\)</span> is established and <span class="math inline">\(i^*\)</span> is actually the individual of the population from which the sampled unit <span class="math inline">\(i\)</span> was derived; <em>e.g.</em> the match has to be a correct match before an identification takes place.</li>
</ol>
<p>Moreover we add the following assumptions:</p>
<ol start="7" type="1">
<li>the intruder tries to match all the records in the sample with a record in the external file;</li>
<li>the identifying variables agree on correct matches, that is no errors, missing values or time-changes occur in recording the identifying variables in the two microdata file.</li>
</ol>
<p><em>Notation</em><br>
The following notation is introduced here and used throughout the chapter when describing different methods for estimating the disclosure risk of microdata.</p>
<p>Suppose the key has <span class="math inline">\(K\)</span> cells and each cell <span class="math inline">\(k = 1, \ldots, K\)</span> is the cross-product of the categories of the identifying variables. In general, we will be looking at a contingency table spanned by the identifying variables in the microdata and not a single vector. The contingency table contains the sample counts and is typically very large and very sparse. Let the population size in cell <span class="math inline">\(k\)</span> of the key be <span class="math inline">\(F_k\)</span> and the sample size <span class="math inline">\(f_k\)</span>. Also:</p>
<p><span class="math display">\[
\sum_{k = 1}^{K}F_{k} = N,\quad \sum_{k = 1}^{K}f_{k} = n.
\]</span></p>
<p>Formally the sample and population sizes in the models introduced in <a href="#sec-argus-individual-risk" class="quarto-xref"><span>Section 3.3.5</span></a> and <a href="#sec-poisson-log-linear" class="quarto-xref"><span>3.3.6</span></a> are random and their expectations are denoted by <span class="math inline">\(n\)</span> and <span class="math inline">\(N\)</span> respectively. In practice, the sample and population size are usually replaced by their natural estimators; the actual sample and population sizes, assumed to be known.</p>
<p>Observing the values of the key on individual <span class="math inline">\(i \in s\)</span> will classify such individual into one cell. We denote by <span class="math inline">\(k(i)\)</span> the index of the cell into which individual <span class="math inline">\(i \in s\)</span> is classified based on the values of the key.</p>
<p>According to the concept of re-identification disclosure given above, we define the (base) individual risk of disclosure of unit <span class="math inline">\(i\)</span> in the sample as its probability of re-identification under the worst case scenario. Therefore the risk <span class="math inline">\(r_i\)</span> that we get is certainly not smaller than the actual risk, the individual risk is a conservative estimate of the actual risk:</p>
<p><span id="eq-3.2.3.1"><span class="math display">\[
r_{i}=\mathbb{P}\left( i \text{ correctly linked with } i^* \mid s , \mathcal{P} \text{, worst case scenario }\right)
\tag{3.1}\]</span></span></p>
<p>All of the methods based on keys in the population described in this chapter aim to estimate this individual per-record disclosure risk measure that can be formulated as <span class="math inline">\(1/F_k\)</span>. The population frequencies <span class="math inline">\(F_k\)</span> are unknown parameters and therefore need to be estimated from the sample. A global file-level disclosure risk measure can be calculated by aggregating the individual disclosure risk measures over the sample:</p>
<p><span class="math display">\[
\tau_{1} = \sum\limits_{k}^{}\frac{1}{F_{k}}
\]</span></p>
<p>An alternative global risk measure can be calculated by aggregating the individual disclosure risk measures over the sample uniques of the cross-classified identifying variables. Since the uniques in the population <span class="math inline">\(F_k = 1\)</span>, are the dominant factor in the disclosure risk measure, we focus our attention on sample uniques <span class="math inline">\(f_k = 1\)</span>:</p>
<p><span class="math display">\[
\tau_{2} = \sum\limits_{k}^{}{I(f_{k} = 1)\frac{1}{F_{k}}}
\]</span></p>
<p>where <span class="math inline">\(I\)</span> represents an indicator function obtaining the value 1 if <span class="math inline">\(f_k = 1\)</span> or 0 if not.</p>
<p>Both of these global risk measures can also be presented as rates by dividing by <span class="math inline">\(n\)</span>, the sample size or the number of uniques.</p>
<p>We assume that the <span class="math inline">\(f_k\)</span> are observed but the <span class="math inline">\(F_k\)</span> are not observed.</p>
</section>
<section id="argus-threshold-rule" class="level3" data-number="3.3.4">
<h3 data-number="3.3.4" class="anchored" data-anchor-id="argus-threshold-rule"><span class="header-section-number">3.3.4</span> ARGUS threshold rule</h3>
<p>The ARGUS threshold rule is based on easily applicable rules and views of safety/unsafety of microdata that is used at Statistics Netherlands. The implementation of these rules was the main reason to start the development of the software package <span class="math inline">\(\mu\)</span>‑ARGUS.</p>
<p>In a disclosure scenario, keys a combination of identifying variables, are supposed to be used by an intruder to re-identify a respondent. Re-identification of a respondent can occur when this respondent is rare in the population with respect to a certain key value, <em>i.e.</em> a combination of values of identifying variables. Hence, rarity of respondents in the population with respect to certain key values should be avoided. When a respondent appears to be rare in the population with respect to a key value, then disclosure control measures should be taken to protect this respondent against re-identification.</p>
<p>Following the Nosy Neighbour scenario, the aim of the <span class="math inline">\(\mu\)</span>‑ARGUS threshold rule is to avoid the occurrence of combinations of scores that are rare in the population and not only avoiding population-uniques. To define what is meant by rare the data protector has to choose a threshold value for each key. If a key occurs more often than this threshold the key is considered safe, otherwise the key must be protected because of the risk of re-identification.</p>
<p>The level of the threshold and the number and size of the keys to be inspected depend of course on the level of protection you want to achieve. Public use files require much more protection than microdata files under contract that are only available to researchers under a contract. How this rule is used in practice is given in the example of <a href="#sec-intro-example" class="quarto-xref"><span>Section 3.7</span></a>.</p>
<p>If a key is considered unsafe according to this rule, protection is required. Therefore often global recoding and local suppression are applied. These techniques are described in the sections <a href="#sec-global-recoding" class="quarto-xref"><span>3.4.3.2</span></a> and <a href="#sec-local-suppression" class="quarto-xref"><span>3.4.3.4</span></a>.</p>
</section>
<section id="sec-argus-individual-risk" class="level3" data-number="3.3.5">
<h3 data-number="3.3.5" class="anchored" data-anchor-id="sec-argus-individual-risk"><span class="header-section-number">3.3.5</span> ARGUS individual risk methodology</h3>
<p>If a distinction between units rare in the sample from a unit rare in the population wants to be made then an inferential step may be followed. In the initial proposal by Benedetti and Franconi (1998), further developed in Franconi and Polettini (2004) and implemented in <span class="math inline">\(\mu\)</span>‑ARGUS, the uncertainty on <span class="math inline">\(F_k\)</span> is accounted for in a Bayesian fashion by introducing the distribution of the population frequencies given the sample frequencies. The individual risk of disclosure is then measured as the (posterior) mean of <span class="math inline">\(\frac{1}{F_k}\)</span> with respect to the distribution of <span class="math inline">\(F_k|f_k\)</span>:</p>
<p><span id="eq-individual-risk"><span class="math display">\[
r_{i} = \mathbb{E} \left( \frac{1}{F_{k}} \mid f_{k} \right) = \sum\limits_{h\geq f_{k}} \frac{1}{h} \mathbb{P} \left(F_{k} = h \mid f_{k} \right).
\tag{3.2}\]</span></span></p>
<p>where the posterior distribution of <span class="math inline">\(F_k|f_k\)</span> is negative binomial with success probability <span class="math inline">\(p_k\)</span> and number of successes <span class="math inline">\(f_k\)</span>. As the risk is a function of <span class="math inline">\(f_k\)</span> and <span class="math inline">\(p_k\)</span> its estimate can be obtained by estimating <span class="math inline">\(p_k\)</span>. Benedetti and Franconi (1998) propose to use</p>
<p><span id="eq-pk-estimator"><span class="math display">\[
{\hat{p}}_{k} = \frac{f_{k}}{\sum\limits_{i:k(i)=k}^{}w_{i}}
\tag{3.3}\]</span></span></p>
<p>where <span class="math inline">\(\sum\limits_{i:k(i)=k}^{}w_{i}\)</span> is an estimate of <span class="math inline">\(F_k\)</span> based on the sampling design weights <span class="math inline">\(w_i\)</span>, possibly calibrated (Deville and Särndal, 1992).</p>
<p><em>When is it possible to apply the individual risk estimation</em><br>
The procedure relies on the assumption that the available data are a sample from a larger population. <em>If the sampling weights are not available, or if data represent the whole population, the strategy used to estimate the individual risk is not meaningful</em>.</p>
<p>In the <span class="math inline">\(\mu\)</span>‑ARGUS manual (see <em>e.g.</em> Hundepool <em>et al.</em>, 2014) a fully detailed description of the approach is reported. This brief note is based on Polettini (2004).</p>
<p><em>Assessing the risk for the whole file</em><br>
The individual risk provides a measure of risk <em>at the individual level</em>. A <em>global</em> measure of disclosure risk for the whole file can be expressed in terms of the expected number of re-identifications in the file. The <em>expected number of re</em>-<em>identifications</em> is a measure of disclosure that depends on the number of records. For this reason, <span class="math inline">\(\mu\)</span>‑ARGUS evaluates also the <em>re‑identification rate</em> that is independent of <span class="math inline">\(n\)</span>:</p>
<p><span class="math display">\[
\xi = \frac{1}{n}\sum\limits_{k=1}^{K}{f_{k}r_{k}} \quad .
\]</span></p>
<p><span class="math inline">\(\xi\)</span> provides a measure of <em>global risk</em>, <em>i.e.</em> a measure of disclosure risk for the whole file, which does not depend on the sample size and can be used to assess the risk of the file or to compare different types of release; for the mathematical details see Polettini (2004).</p>
<p>The <em>percentage of expected re-identifications</em>, <em>i.e.</em> the value <span class="math inline">\(\psi=100\cdot\xi\%\)</span> provides an equivalent measure of global risk.</p>
<p><em>Application of local suppression within the individual risk methodology</em><br>
After the risk has been estimated, protection takes place. One option in protection is the application of <em>local suppression</em> (see <a href="#sec-local-suppression" class="quarto-xref"><span>Section 3.4.3.4</span></a>).</p>
<p>In <span class="math inline">\(\mu\)</span>‑ARGUS the technique of local suppression, when combined with the individual risk, is applied only to unsafe cells or combinations. Therefore, the user must input a <em>threshold</em> in terms of risk, <em>e.g.</em> probability of re-identification, to classify these as either safe or unsafe. Local suppression is applied to the unsafe individuals, so as to lower their probability of being re‑identified under the given threshold.</p>
<p>In order to select the risk threshold, that represents a level of <em>acceptable risk</em>, <em>i.e.</em> a risk value under which an individual can be considered safe, the <em>re‑identification rate</em> can be used. A <em>release</em> will be considered <em>safe</em> when the expected rate of correct re-identifications is below a level the NSI considers acceptable. As the re-identification rate is cast in terms of the individual risk, a threshold on the re-identification rate can be transformed into a threshold on the individual risk (see below). Under this approach, individuals are at risk because their probability of re-identification contributes a large proportion of expected re-identifications in the file.</p>
<p>In order to reduce the number of local suppressions, the procedure of releasing a safe file considers preliminary steps of protection using techniques such as <em>global recoding</em> (see <a href="#sec-global-recoding" class="quarto-xref"><span>Section 3.4.3.2</span></a>). Recoding of selected variables will indeed lower the individual risk and therefore the re-identification rate of the file.</p>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Expert level
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p><em>Threshold setting using the re-identification rate</em><br>
Consider the re-identification rate <span class="math inline">\(\xi\)</span>: a key <span class="math inline">\(k\)</span> contributes to <span class="math inline">\(\xi\)</span> an amount <span class="math inline">\(r_kf_k\)</span> of expected re‑identifications. Since units belonging to the same key <span class="math inline">\(k\)</span> have the same individual risk, keys can be arranged in increasing order of risk <span class="math inline">\(r_k\)</span>. Let the subscript (<span class="math inline">\(k\)</span>) denotes the <span class="math inline">\(k\)</span>-th element in this ordering. A threshold <span class="math inline">\(r^*\)</span> on the individual risk can be set. Consequently, unsafe cells are those for which <span class="math inline">\(r_{k} \geq r^*\)</span> that can be indexed by <span class="math inline">\((k) = k^{*} + 1,\ldots,K\)</span>. The key <span class="math inline">\(k^{*}\)</span> is in a one-to-one correspondence to <span class="math inline">\(r^{*}\)</span>. This allows setting an upper bound <span class="math inline">\(\xi^{*}\)</span> on the re‑identification rate of the released file (after data protection) substituting <span class="math inline">\(r_kf_k\)</span> with <span class="math inline">\(r^{*}f_{(k)}\)</span> for each (<span class="math inline">\(k\)</span>). For the mathematical details see Polettini (2004) and the Argus manual (<em>e.g.</em> Hundepool <em>et al.</em>, 2014).</p>
<p>The approach pursued so far can be reversed. Therefore, selecting a <em>threshold</em> <span class="math inline">\(\tau\)</span> <em>on the re-identification rate</em> <span class="math inline">\(\xi\)</span> determines a key index <span class="math inline">\(k^{*}\)</span> which corresponds to a value for <span class="math inline">\(r^{*}\)</span>. Using <span class="math inline">\(r^{*}\)</span> as a threshold for the individual risk keeps the re‑identification rate <span class="math inline">\(\xi\)</span> of the released file below <span class="math inline">\(\tau\)</span>. The search of such a <span class="math inline">\(k^{*}\)</span> is performed by a simple iterative algorithm.</p>
<p><em>Releasing hierarchical files</em><br>
A relevant characteristic of social microdata is its inherent hierarchical structure, which allows us to recognise groups of individuals in the file, the most typical case being the <em>household</em>. When defining the re-identification risk, it is important to take into account this dependence among units: indeed re-identification of an individual in the group may affect the probability of disclosure of all its members. So far, implementation of a hierarchical risk has been performed only with reference to households, <em>i.e.</em> a <em>household risk</em>.</p>
<p>Allowing for dependence in estimating the risk enables us to attain a higher level of safety than when merely considering the case of independence.</p>
<p><em>The household risk</em><br>
The household risk makes use of the same framework defined for the individual risk. In particular, the concept of re-identification holds with the additional assumption that <em>the intruder attempts a confidentiality breach by re-identification of individuals in households</em>.</p>
<p>The <em>household risk</em> is defined as the probability that <em>at least</em> one individual in the household is re-identified. For a given household <span class="math inline">\(g\)</span> of size <span class="math inline">\(|g|\)</span>, whose members are labelled <span class="math inline">\(i_1, \ldots, i_{|g|}\)</span>, the household risk is:</p>
<p><span class="math display">\[
r^{h}(g) = \mathbb{P} \left(i_{1} \cup i_{2} \cup \ldots \cup i_{|g|} \text { re-identified } \right)
\]</span></p>
<p>and is the same for all the individuals in household <span class="math inline">\(g\)</span> and equals <span class="math inline">\(r_{g}^{h}\)</span>.</p>
<p><em>Threshold setting for the household risk</em><br>
Since all the individuals in a given household have the same household risk, the expected number of re‑identified records in household <span class="math inline">\(g\)</span> equals <span class="math inline">\(|g|r_{g}^{h}\)</span>. The re‑identification rate in a hierarchical file can be then defined as <span class="math inline">\(\xi^{h} = \frac{1}{n}\sum\limits_{g=1}^{G}{|g|r_{g}^{h}}\)</span>, where <span class="math inline">\(G\)</span> is the total number of households in the file. The re‑identification rate can be used to define a threshold <span class="math inline">\(r^{h^{\ast}}\)</span> on the household risk <span class="math inline">\(r^{h}\)</span>, much in the same way as for the individual risk. For the mathematical details see Polettini (2004) and the Argus manual (<em>e.g.</em> Hundepool <em>et al.</em>, 2014).</p>
<p>Note that the household risk <span class="math inline">\(r_{g}^{h}\)</span> of household <span class="math inline">\(g\)</span> is computed by the individual risks of its household members. For a given household, it might happen that a household is unsafe (<span class="math inline">\(r_{g}^{h}\)</span> exceeds the threshold) because just one of its members, <span class="math inline">\(i\)</span>, say, has a high value <span class="math inline">\(r_{i}\)</span> of the individual risk. To protect the households, the followed approach is therefore to protect individuals in households, first protecting those individuals who contribute most to the household risk. For this reason, inside <em>unsafe households</em>, detection of <em>unsafe individuals</em> is needed. In other words, the threshold on the household risk <span class="math inline">\(r^{h}\)</span> has to be transformed into a threshold on the individual risk <span class="math inline">\(r_{i}\)</span>. To this aim, it can be noticed that the household risk is bounded by the sum of the individual risks of the members of the household: <span class="math inline">\(r_{g}^{h} \leq \sum\limits_{j=1}^{|g|}r_{i_{j}}\)</span>.</p>
<p>Consider to apply a threshold <span class="math inline">\(r^{h^{\ast}}\)</span> on the household risk. In order for household <span class="math inline">\(g\)</span> to be classified safe (<em>i.e.</em> <span class="math inline">\(r_{g}^{h} &lt; r^{h^{\ast}}\)</span>) it is <em>sufficient</em> that all of its components have individual risk less than <span class="math inline">\(\delta_{g} = r^{h ^{\ast}}/|g|\)</span>.</p>
<p>This is clearly an approach possibly leading to overprotection, as we check whether a <em>bound</em> on the household risk is below a given threshold.</p>
<p>It is important to remark that the threshold <span class="math inline">\(\delta_g\)</span> just defined depends on the size of the household to which individual <span class="math inline">\(i\)</span> belongs. This implies that for two individuals that are classified in the same key <span class="math inline">\(k\)</span> (and therefore have the same individual risk <span class="math inline">\(r_{k}\)</span>), but belong to different households with different sizes, it might happen that one is classified safe, while the other unsafe (unless the household size is included in the set of identifying variables).</p>
<p>In practice, denoting by <span class="math inline">\(g(i)\)</span> the household to which record <span class="math inline">\(i\)</span> belongs, the approach pursued so far consists in turning a threshold <span class="math inline">\(r^{h^{\ast}}\)</span> on the household risk into a <em>vector of thresholds</em> on the <em>individual risks</em> <span class="math inline">\(r_{i} = 1,\ldots,n\)</span>:</p>
<p><span class="math display">\[
\delta_{g} = \delta_{g(i)} = \frac{r^{h^{\ast}}}{|g(i)|} \quad .
\]</span></p>
<p><em>Individuals</em> are finally set to unsafe whenever <span class="math inline">\(r_{i} \geq \delta_{g(i)}\)</span>; local suppression is then applied to those records, if requested. Suppression of these records ensures that after protection the household risk is below the threshold <span class="math inline">\(\delta_{g}\)</span>.</p>
<p><em>Choice of identifying variables in hierarchical files</em><br>
For household data it is important to include in the identifying variables that are used to estimate the household risks also the available information on the household, such as the number of components or the household type.</p>
<p>Suppose one computes the risk using the household size as the only identifying variable in a household data file, and that such file contains households whose risk is above a fixed threshold. Since information on the number of components in the household cannot be removed from a file with household structure, these records cannot be safely released, and no suppression can make them safe. This permits to check for presence of very peculiar households (usually, the very large ones) that can be easily recognised in the population just by their size and whose main characteristic, namely their size, can be immediately computed from the file. For a discussion on this issue see Polettini (2004).</p>
</div>
</div>
</div>
</section>
<section id="sec-poisson-log-linear" class="level3" data-number="3.3.6">
<h3 data-number="3.3.6" class="anchored" data-anchor-id="sec-poisson-log-linear"><span class="header-section-number">3.3.6</span> The Poisson model with log-linear modelling</h3>
<p>As defined in Skinner and Elamir (2004), assuming that the <span class="math inline">\(F_{k}\)</span> are independently Poisson distributed with means <span class="math inline">\(\left\{\lambda_{k} \right\}\)</span> and assuming a Bernoulli sampling scheme with equal selection probably <span class="math inline">\(\pi\)</span>, then <span class="math inline">\(f_{k}\)</span> and <span class="math inline">\(F_{k} - f_{k}\)</span> are independently Poisson distributed as: <span class="math inline">\(f_{k} \mid \lambda_{k} \sim \operatorname{Pois} \left(\pi\lambda_{k} \right)\)</span> and <span class="math inline">\(F_{k} - f_{k} \mid \lambda_{k} \sim \operatorname{Pois} \left( ( 1 - \pi ) \lambda_{k} \right)\)</span> . The individual risk measure for a sample unique is defined as <span class="math inline">\(r_{k} = \mathbb{E}_{\lambda_{k}} \left( \frac{1}{F_{k}} \mid f_{k} = 1 \right)\)</span> which is equal to:</p>
<p><span class="math display">\[
r_{k} = \frac{1}{\lambda_{k} (1 - \pi) } \left[ 1 - e^{ - \lambda_{k} (1 - \pi) } \right]
\]</span></p>
<p>In this approach the parameters <span class="math inline">\(\left\{ \lambda_{k} \right\}\)</span> are estimated by taking into account the structure and dependencies in the data through log-linear modelling. Assuming that the sample frequencies <span class="math inline">\(f_{k}\)</span> are independently Poisson distributed with a mean of <span class="math inline">\(u_{k} = \pi\lambda_{k}\)</span>, a log-linear model for the <span class="math inline">\(u_{k}\)</span> can be expressed as: <span class="math inline">\(\text{log}(u_{k}) = x_{k}^{'}\beta\)</span> where <span class="math inline">\(x_{k}\)</span> is a design vector denoting the main effects and interactions of the model for the key variables. Using standard procedures, such as iterative proportional fitting, we obtain the Poisson maximum-likelihood estimates for the vector <span class="math inline">\(\beta\)</span> and calculate the fitted values: <span class="math inline">\({\hat{u}}_{k} = \text{exp}(x_{k}^{'}\hat{\beta})\)</span>. The estimate for <span class="math inline">\({\hat{\lambda}}_{k}\)</span> is equal to <span class="math inline">\(\frac{{\hat{u}}_{k}}{\pi}\)</span> which is substituted for <span class="math inline">\(\lambda_{k}\)</span> in the above formula for <span class="math inline">\(r_{k}\)</span>. The individual disclosure risk measures can be aggregated to obtain a global (file-level) measure:</p>
<p><span class="math display">\[
{\hat{\tau}}_{2} = \sum\limits_{k \in \text{SU}}^{}{\hat{r_k} =}\sum\limits_{k \in \text{SU}}^{}{\frac{1}{{\hat{\lambda}}_{k}(1 - \pi)}\lbrack 1 - e^{- {\hat{\lambda}}_{k}(1 - \pi)}\rbrack}
\]</span></p>
<p>where <span class="math inline">\(\text{SU}\)</span> is the set of all sample uniques.</p>
<p>More details on this method are available from Skinner and Shlomo (2005, 2006) and Shlomo and Barton (2006).</p>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Expert level
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Skinner and Shlomo (2005, 2006) have developed goodness-of-fit criteria for selecting the most robust log-linear model that will provide accurate estimates for the global disclosure risk measure detailed above. The method begins with a log-linear model where a high test statistic indicates under-fitting (<em>i.e.</em>, the disclosure risk measures will be over-estimated). Then a forward search algorithm is employed by gradually adding in higher order interaction terms into the model until the test statistic approaches the level (based on a Normal distribution approximation) where the fit of the log-linear model is accepted.</p>
<p>This method is still under development. At present there is a need to develop clear and user-friendly software to implement the method. However, the Office for National Statistics in the UK has used it to inform microdata release decisions. The method is based on theoretical well-defined disclosure risk measures and goodness of fit criteria which ensure the fit of the log-linear model and the accuracy of the disclosure risk measures. It requires a model search algorithm which takes some computer time and requires intervention.</p>
<p>New methods for probabilistic risk assessment are under development based on a generalized Negative Binomial smoothing model for sample disclosure risk estimation which subsumes both the model used in <span class="math inline">\(\mu\)</span>‑ARGUS and the Poisson log-linear model above. The method is useful for key variables that are ordinal where local neighbourhoods can be defined for inference on cell <span class="math inline">\(k\)</span>. The Bayesian assumption of <span class="math inline">\(\lambda_{k} \sim \text{Gamma}(\alpha_{k},\beta_{k})\)</span> is added independently to the Poisson model above which then transforms the marginal distribution to the generalized Negative Binomial Distribution: <span class="math display">\[
f_{k} \sim \text{NB}(\alpha_{k},p_{k} = \frac{1}{1 + \text{N}\pi_{k}\beta_{k}})
\]</span> and</p>
<p><span class="math display">\[
F_{k}|f_{k} \sim \text{NB}(\alpha_{k} + f_{k},\rho_{k} = \frac{1 + \text{N}\pi_{k}\beta_{k}}{1 + \text{N}\beta_{k}})
\]</span></p>
<p>where <span class="math inline">\(\pi_{k}\)</span> is the sampling fraction. In each local neighbourhood of cell <em>k</em> a smoothing polynomial regression model is carried out to estimate <span class="math inline">\(\alpha_{k}\)</span> and <span class="math inline">\(\beta_{k}\)</span>, and disclosure risk measures are estimated based on the Negative Binomial Distribution, <span class="math inline">\({\hat{\tau}}_{2} = \sum_{k \in \text{SU}}^{}{\hat{r_k} =}\sum_{k \in \text{SU}}^{}\frac{{\hat{\rho}}_{k}(1 - {\hat{\rho}}_{k})^{{\hat{\alpha}}_{k}}}{{\hat{\alpha}}_{k}(1 - {\hat{\rho}}_{k})}\)</span> , see: Rinott and Shlomo (2005, 2006).</p>
</div>
</div>
</div>
</section>
<section id="sec-SUDA" class="level3" data-number="3.3.7">
<h3 data-number="3.3.7" class="anchored" data-anchor-id="sec-SUDA"><span class="header-section-number">3.3.7</span> SUDA</h3>
<p>The Special Uniques Detection Algorithm (SUDA) (Elliot et.al., 2005) is a software system (windows application available as freeware under restricted licence) that provides disclosure risk broken down by record, variable, variable value and by interactions of those. It is based on the concept of a “special unique”. A special unique is a record that is a sample unique on a set of variables and that is also unique on a subset of those variables. Empirical work has shown that special uniques are more likely to be population unique than random uniques. Special uniques can be classified according to the size and number of the smallest subset of key variables that defines the record as unique, known as minimal sample uniques (MSU). In the algorithm, all MSUs are found for each record on all possible subsets of the key variables where the maximum size of the subsets <em>m</em> is specified by the user.</p>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Expert level
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>SUDA grades and orders records within a microdata file according to the level of risk. The method assigns a per record matching probability to a sample unique based on the number and size of minimal uniques. The DIS Measure (Skinner and Elliot, 2000) is the conditional probability of a correct match given a unique match:</p>
<p><span class="math display">\[
p(cm \mid um) = \frac{\sum\limits_{k = 1}^{K} I\left(f_{k} = 1 \right)}{\sum\limits_{k = 1}^{K} F_{k} I \left(f_{k} = 1 \right) }
\]</span></p>
<p>and is estimated by a simple sample-based measure which is approximately unbiased without modelling assumptions. Elliot (2005) describes a heuristic which combines the DIS measure with scores resulting from the algorithm (<em>i.e.</em>, SUDA scores). This method known as DIS-SUDA produces estimates of intruder confidence in a match against a given record being correct. This is closely related to the probability that the match is correct and is heuristically linked to the estimate of</p>
<p><span class="math display">\[
\tau_2 = \sum\limits_k{I(f_k=1)\frac{1}{F_k}}
\]</span></p>
<p>The advantage of this method is that it relates to a practical model of data intrusion, and it is possible to compare different values directly. The disadvantages are that it is sensitive to level of the max MSU parameter and is calculated in a heuristic manner. In addition it is difficult to compare disclosure risk across different files. However, the method has been extensively tested and was used successfully for the detection of high-risk records in the UK Sample of Anonymized Records (SAR) drawn from the 2001 Census (Merrett, 2004). The assessment showed that the DIS-SUDA measure calculated from the algorithm provided a good estimate for the individual disclosure risk measure, especially for the case when the number of key variables, <span class="math inline">\(m = 6\)</span>. The algorithm also identifies the variables and value of variables that are contributing most to the disclosure risk of the record.</p>
<p>A new algorithm, SUDA2 has been developed, Elliot et al (2005), that improves SUDA using several methods. The development provides a much faster tool that can handle larger datasets.</p>
</div>
</div>
</div>
</section>
<section id="sec-record-linkage" class="level3" data-number="3.3.8">
<h3 data-number="3.3.8" class="anchored" data-anchor-id="sec-record-linkage"><span class="header-section-number">3.3.8</span> Record Linkage</h3>
<p>Roughly speaking, record linkage consists of linking each record <span class="math inline">\(a\)</span> in file <span class="math inline">\(A\)</span> (protected file) to a record <span class="math inline">\(b\)</span> in file <span class="math inline">\(B\)</span> (original file). The pair <span class="math inline">\((a,b)\)</span> is a match if <span class="math inline">\(b\)</span> turns out to be the original record corresponding to <span class="math inline">\(a\)</span>.</p>
<p>To apply this method to measure the risk of identity disclosure, it is assumed that an intruder has got an external dataset sharing some (key or outcome) variables with the released protected dataset and containing additionally some identifier variables (<em>e.g.</em> passport number, full name, etc.). The intruder is assumed to try to link the protected dataset with the external dataset using the shared variables. The number of matches gives an estimation of the number of protected records whose respondent can be re-identified by the intruder. Accordingly, disclosure risk is defined as the proportion of matches among the total number of records in <span class="math inline">\(A\)</span>.</p>
<p>The main types of record linkage used to measure identity disclosure in SDC are discussed below. An illustrative example can be found on the CASC-website as one of the case-studies linked to this handbook (see <a href="https://research.cbs.nl/casc/handbook.htm#casestudies">https://research.cbs.nl/casc/handbook.htm#casestudies</a>).</p>
<section id="distance-based-record-linkage" class="level4" data-number="3.3.8.1">
<h4 data-number="3.3.8.1" class="anchored" data-anchor-id="distance-based-record-linkage"><span class="header-section-number">3.3.8.1</span> Distance-based record linkage</h4>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-4-contents" aria-controls="callout-4" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Expert level
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-4" class="callout-4-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Distance-based record linkage consists of linking each record <span class="math inline">\(a\)</span> in file <span class="math inline">\(A\)</span> to its nearest record <span class="math inline">\(b\)</span> in file <span class="math inline">\(B\)</span>. Therefore, this method requires a definition of a distance function for expressing <em>nearness</em> between records. This record-level distance can be constructed from distance functions defined at the level of variables. Construction of record-level distances requires standardizing variables to avoid scaling problems and assigning each variable a weight on the record-level distance.</p>
<p>Distance-based record linkage was first proposed in Pagliuca and Seri (1999) to assess the disclosure risk after microaggregation, see <a href="#sec-microaggregation" class="quarto-xref"><span>Section 3.4.2.3</span></a>. Those authors used the Euclidean distance and equal weights for all variables. (Domingo-Ferrer and Torra, 2001) later used distance-based record linkage for evaluating other masking methods as well; in their empirical work, distance-based record linkage outperforms probabilistic record linkage (described below). Recently, (Torra and Miyamoto, 2004) have shown that method-specific distance functions might be defined to increase the proportion of matches for particular SDC methods.</p>
<p>The record linkage algorithm introduced in&nbsp;(Bacher, Brand and Bender, 2002) is similar in spirit to distance-based record linkage. This is so because it is based on cluster analysis and, therefore, links records that are near to each other.</p>
<p>The main advantages of using distances for record linkage are simplicity for the implementer and intuitiveness for the user. Another strong point is that subjective information (about individuals or variables) can be included in the re-identification process by properly modifying distances. In fact, the next version of the <span class="math inline">\(\mu\)</span>‑ARGUS microdata protection package (<em>e.g.</em> Hundepool <em>et al.</em>, 2014) will incorporate distance-based record linkage as a disclosure risk assessment method.</p>
<p>The main difficulty of distance-based record linkage consists of coming up with appropriate distances for the variables under consideration. For one thing, the weight of each variable must be decided and this decision is often not obvious. Choosing a suitable distance is also especially thorny in the cases of categorical variables and of masking methods such as local recoding where the masked file contains new labels with respect to the original dataset.</p>
</div>
</div>
</div>
</section>
<section id="probabilistic-record-linkage" class="level4" data-number="3.3.8.2">
<h4 data-number="3.3.8.2" class="anchored" data-anchor-id="probabilistic-record-linkage"><span class="header-section-number">3.3.8.2</span> Probabilistic record linkage</h4>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-5-contents" aria-controls="callout-5" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Expert level
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-5" class="callout-5-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Like distance-based record linkage, probabilistic record linkage aims at linking pairs of records <span class="math inline">\((a,b)\)</span> in datasets <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>, respectively. For each pair, an index is computed. Then, two thresholds <span class="math inline">\(LT\)</span> and <span class="math inline">\(NLT\)</span> in the index range are used to label the pair as linked, clerical or non-linked pair: if the index is above <span class="math inline">\(LT\)</span>, the pair is linked; if it is below <span class="math inline">\(NLT\)</span>, the pair is non-linked; a clerical pair is one that cannot be automatically classified as linked or non-linked and requires human inspection. When independence between variables is assumed, the index can be computed from the following conditional probabilities for each variable: the probability <span class="math inline">\(\mathbb{P}\left( 1\mid M \right)\)</span> of coincidence between the values of the variable in two records <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> given that these records are a real match, and the probability <span class="math inline">\(\mathbb{P}\left( 0\mid U \right)\)</span>of non-coincidence between the values of the variable given that <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> are a real unmatch.</p>
<p>Like in the previous section, disclosure risk is defined as the number of matches (linked pairs that are correctly linked) over the number of records in file <span class="math inline">\(A\)</span>.</p>
<p>To use probabilistic record linkage in an effective way, we need to set the thresholds <span class="math inline">\(LT\)</span> and <span class="math inline">\(NLT\)</span> and estimate the conditional probabilities <span class="math inline">\(\mathbb{P}\left( 1 \mid M \right)\)</span> and <span class="math inline">\(\mathbb{P}\left( 0 \mid U \right)\)</span> used in the computation of the indices. In plain words, thresholds are computed from: (i) the probability <span class="math inline">\(\mathbb{P}\left( \text{LP} \mid U \right)\)</span> of linking a pair that is an unmatched pair (a <em>false positive</em> or <em>false linkage</em>) and (ii) the probability <span class="math inline">\(\mathbb{P}\left( \text{NP} \mid M \right)\)</span>of not linking a pair that is a match (a <em>false negative</em> or <em>false unlinkage</em>). Conditional probabilities <span class="math inline">\(\mathbb{P}\left( 1 \mid M \right)\)</span> and <span class="math inline">\(\mathbb{P}\left( 0 \mid U \right)\)</span> are usually estimated using the EM algorithm (Dempster, Laird and Rubin 1977).</p>
<p>Original descriptions of this kind of record linkage can be found in Fellegi and Sunter (1969) and Jaro (1989). Torra and Domingo-Ferrer (2003) describe the method in detail (with examples) and Winkler (1993) presents a review of the state of the art on probabilistic record linkage. In particular, this latter paper includes a discussion concerning non-independent variables. A (hierarchical) graphical model has recently been proposed&nbsp;(Ravikumar and Cohen, 2004) that compares favourably with previous approaches.</p>
<p>Probabilistic record linkage methods are less simple than distance-based ones. However, they do not require rescaling or weighting of variables. The user only needs to provide two probabilities as input: the maximum acceptable probability <span class="math inline">\(\mathbb{P}\left( \text{LP} \mid U \right)\)</span> of false positive and the maximum acceptable probability <span class="math inline">\(\mathbb{P}\left( \text{NP} \mid M \right)\)</span> of false negative.</p>
</div>
</div>
</div>
</section>
<section id="other-record-linkage-methods" class="level4" data-number="3.3.8.3">
<h4 data-number="3.3.8.3" class="anchored" data-anchor-id="other-record-linkage-methods"><span class="header-section-number">3.3.8.3</span> Other record linkage methods</h4>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-6-contents" aria-controls="callout-6" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Expert level
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-6" class="callout-6-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Recently, the use of other record linkage methods has also been considered for disclosure risk assessment. While in the previous record linkage methods it is assumed that the two files to be linked share a set of variables, other methods have been developed where this constraint is relaxed. Under appropriate conditions, (Torra, 2004) shows that re-identification is still possible when files do not share any variables. Domingo-Ferrer and Torra (2003) propose the use of such methods for disclosure risk assessment.</p>
</div>
</div>
</div>
</section>
</section>
<section id="references" class="level3" data-number="3.3.9">
<h3 data-number="3.3.9" class="anchored" data-anchor-id="references"><span class="header-section-number">3.3.9</span> References</h3>
<p>Bacher J., Brand R., and Bender S. (2002), <em>Re-identifying register data by survey data using cluster analysis: an empirical study</em>. International Journal of Uncertainty, Fuzziness and Knowledge Based Systems, 10(5):589–607, 2002.</p>
<p>Benedetti, R. and Franconi, L. (1998). <em>Statistical and technological solutions for controlled data dissemination</em>, Pre-proceedings of New Techniques and Technologies for Statistics, 1, 225-232.</p>
<p>Coppola, L. and Seri, G. (2005). <em>Confidentiality aspects of household panel survey: the case study of Italian sample from EU-SILC.</em> Monographs of official statistics – Proceedings of the Work session on statistical data confidentiality – Geneve 9-11 November 2005, 175-180.</p>
<p>Cox, L.H. (1995). <em>Protecting confidentiality in business surveys</em>. Business Survey Methods, Cox, B.G., Binder, D.A., Chinnappa, B.N., Christianson, A., Colledge, M.J. e Kott, P.S. (Eds.), New-York: Wiley, 443‑476.</p>
<p>Dempster A. P., Laird N. M., and Rubin D. B. (1977), <em>Maximum likelihood from incomplete data via the em algorithm.</em> Journal of the Royal Statistical Society, 39:1–38, 1977.</p>
<p>Deville, J.C. and Särndal, C.E. (1992). <em>Calibration estimators in survey sampling</em>, Journal of the American Statistical Association 87, 367–382.</p>
<p>Domingo-Ferrer J., and Torra, V. (2001), <em>A quantitative comparison of disclosure control methods for microdata.</em> In P.&nbsp;Doyle, J.&nbsp;I. Lane, J.&nbsp;J.&nbsp;M. Theeuwes, and L.&nbsp;Zayatz, editors, Confidentiality, Disclosure and Data Access: Theory and Practical Applications for Statistical Agencies, pages 111–134, Amsterdam, 2001. North-Holland. <a href="https://crises-deim.urv.cat/webCrises/publications/bcpi/cliatpasa01Aquantitative.pdf">https://crises-deim.urv.cat/webCrises/publications/bcpi/cliatpasa01Aquantitative.pdf</a>.</p>
<p>Domingo-Ferrer, J., and Torra, V. (2003), <em>Disclosure risk assessment in statistical microdata protection via advanced record linkage.</em> Statistics and Computing, 13:343–354.</p>
<p>Elamir, E., Skinner, C. (2004) <em>Record-level Measures of Disclosure Risk for Survey Microdata</em>, Journal of Official Statistics, Vol. 22, No.&nbsp;3, 2006, pp.&nbsp;525–539. See also: Southampton Statistical Sciences Research Institute, University of Southampton, methodology working paper:<br>
<a href="https://eprints.soton.ac.uk/8175/">https://eprints.soton.ac.uk/8175/</a></p>
<p>Elliot, M. J., (2000). <em>DIS: A new approach to the Measurement of Statistical Disclosure Risk.</em> International Journal of Risk Management 2(4), pp 39-48.</p>
<p>Elliot, M. J., Manning, A. M.&amp; Ford, R. W. (2002<strong>).</strong> '<em>A Computational Algorithm for Handling the Special Uniques Problem</em>'. International Journal of Uncertainty, Fuzziness and Knowledge Based Systems 5(10), pp 493-509.</p>
<p>Elliot, M. J., Manning, A., Mayes, K., Gurd, J. &amp; Bane, M. (2005). ’<em>SUDA: A Program for Detecting Special Uniques’.</em> Proceedings of the UNECE/Eurostat work session on statistical data confidentiality, Geneva, November 2005</p>
<p>Elliot, M. J., Skinner, C. J., and Dale, A. (1998). <em>'Special Uniques, Random Uniques, and Sticky Populations: Some Counterintuitive Effects of Geographical Detail on Disclosure Risk'</em>. Research in Official Statistics 1(2), pp 53-67.</p>
<p>Fellegi, I. P., and Sunter, A.B. (1969), <em>A theory for record linkage</em>. Journal of the American Statistical Association, 64(328):1183–1210.</p>
<p>Franconi, L. and Polettini, S. (2004). <em>Individual risk estimation in <span class="math inline">\(\mu\)</span>-ARGUS: a review.</em> In: Domingo-Ferrer, J. (Ed.), Privacy in Statistical Databases. Lecture Notes in Computer Science. Springer, 262‑272</p>
<p>Franconi, L. and Seri, G. (2000). <em>Microdata Protection at the Italian National Statistical Insititute (Istat): A User Perspective. Of Significance</em> Journal of the Association of Public Data Users – Volume 2 Number 1 2000, page. 57-64.</p>
<p>Hundepool, A., Van de&nbsp;Wetering, A., Ramaswamy, R., Franconi, L., Capobianchi, A., de Wolf, P.P., Domingo-Ferrer, J., Torra, V., Brand, R., and Giessing, S. (2014), <em><span class="math inline">\(\mu\)</span>-ARGUS version 5.1 Software and User’s Manual</em>. Statistics Netherlands, Voorburg NL, 2014. <a href="https://research.cbs.nl/casc/Software/MUmanual5.1.3.pdf">https://research.cbs.nl/casc/Software/MUmanual5.1.3.pdf</a>.</p>
<p>Jackson, P., Longhurst, J. (2005), <em>Providing access to data and making microdata safe, experiences of the ONS</em>, proceedings of the UNECE/Eurostat work session on statistical data confidentiality, Geneva, November 2005</p>
<p>Jaro, M.A.&nbsp;(1989), <em>Advances in record-linkage methodology as applied to matching the 1985 census of tampa, florida.</em> Journal of the American Statistical Association, 84(406):414–420.</p>
<p>Pagliuca, D. and Seri, G. (1999), <em>Some results of individual ranking method on the system of enterprise accounts annual survey</em>, Esprit SDC Project, Deliverable MI-3/D2.</p>
<p>Polettini, S. and Seri, G (2004). <em>Revision of “Guidelines for the protection of social microdata using the individual risk methodology”</em>. Deliverable 1.2-D3, available at CASC web site.</p>
<p>Ravikumar, P., and Cohen, W.W. (2004),. <em>A hierarchical graphical model for record linkage.</em> In UAI 2004, USA, 2004. Association for Uncertainty in Artificial Intelligence.</p>
<p>Rinott, Y. ,Shlomo, N (2006) <em>A Generalized Negative Binomial Smoothing Model for Sample Disclosure Risk Estimation</em> ,. PSD'2006 Privacy in Statistical Databases, Springer LNCS proceedings, to appear.</p>
<p>Rinott, Y., Shlomo, N. (forthcoming) <em>A Smoothing Model for Sample Disclosure Risk Estimation</em>, Volume in memory of Yehuda Vardi in the IMS Lecture Notes Monograph Series.</p>
<p>Shlomo, N. (2006), <em>Review of statistical disclosure control methods for census frequency tables</em>, ONS Survey Methodology Bulletin.</p>
<p>Shlomo, N., Barton, J. (2006) <em>Comparison of Methods for Estimating Disclosure Risk Measures for Microdata at the UK Office for National Statistics</em>, PSD'2006 Privacy in Statistical Databases Conference, CD Proceedings, to appear</p>
<p>Skinner, C., Shlomo, N. (2005), <em>Assessing disclosure risk in microdata using record-level measures</em>, proceedings of the UNECE/Eurostat work session on statistical data confidentiality, Geneva, November 2005</p>
<p>Skinner, C.J., Shlomo, N. (2006) <em>Assessing Identification Risk in Survey Microdata Using Log-linear Models</em>, Journal of the American Statistical Association, 103 (483). pp.&nbsp;989-1001. See also: <a href="http://eprints.lse.ac.uk/39112/1/Assessing_Identification_Risk_in_Survey_Microdata%28lsero%29.pdf">http://eprints.lse.ac.uk/39112/1/Assessing_Identification_Risk_in_Survey_Microdata%28lsero%29.pdf</a>.</p>
<p>Skinner, C., Holmes, D. (1998), <em>Estimating the re-identification risk per record in microdata</em>, JOS, Vol.14.</p>
<p>Torra, V. (2004), <em>Owa operators in data modeling and re-identification.</em> IEEE Trans. on Fuzzy Systems, vol.&nbsp;12, no. 5, pp.&nbsp;652-660.</p>
<p>Torra V., and Domingo-Ferrer J. (2003). <em>Record linkage methods for multidatabase data mining</em>. In V.&nbsp;Torra, editor, Information Fusion in Data Mining, pages 101–132, Germany, Springer.</p>
<p>Torra, V., and Miyamoto, S. (2004),. <em>Evaluating fuzzy clustering algorithms for microdata protection.</em> In J.&nbsp;Domingo-Ferrer and V.&nbsp;Torra, editors, Privacy in Statistical Databases, volume 3050 of LNCS, pages 175–186, Berlin Heidelberg. Springer.</p>
<p>Willenborg, L. and De Waal, T. (1996). <em>Statistical Disclosure Control in Practice.</em> Lecture Notes in Statistics, 111, New-York: Springer Verlag.</p>
<p>Willenborg, L. and De Waal, T. (2001). <em>Elements of statistical disclosure control.</em> Lecture Notes in Statistics, 115, New York: Springer-Verlag.</p>
<p>Winkler, W. E. (1993),. <em>Matching and record linkage.</em> Technical Report RR93/08, Statistical Research Division, U. S. Bureau of the Census (USA), 1993.</p>
</section>
</section>
<section id="sec-microdata-protection-methods" class="level2 page-columns page-full" data-number="3.4">
<h2 data-number="3.4" class="anchored" data-anchor-id="sec-microdata-protection-methods"><span class="header-section-number">3.4</span> Microdata protection methods</h2>
<section id="overview-of-concepts-and-methods" class="level3" data-number="3.4.1">
<h3 data-number="3.4.1" class="anchored" data-anchor-id="overview-of-concepts-and-methods"><span class="header-section-number">3.4.1</span> Overview of concepts and methods</h3>
<p>In this section we explain the basic concepts and methods related to microdata protection. Sections <a href="#sec-perturbative-masking" class="quarto-xref"><span>3.4.2</span></a>, <a href="#sec-nonperturbative-masking" class="quarto-xref"><span>3.4.3</span></a>, <a href="#sec-noise-addition-details" class="quarto-xref"><span>3.4.4</span></a> and <a href="#sec-microaggregation-further" class="quarto-xref"><span>3.4.5</span></a> give in-depth descriptions of some particularly complex methods: microaggregation, rank swapping, additive noise and synthetic data (the first two implemented in <span class="math inline">\(\mu\)</span>‑ARGUS).</p>
<p>A microdata set <span class="math inline">\(\mathbf{X}\)</span> can be viewed as a file with <span class="math inline">\(n\)</span> records, where each record contains <span class="math inline">\(m\)</span> variables on an individual respondent. The variables can be classified in four categories which are not necessarily disjoint:</p>
<ul>
<li><em>Identifiers</em>. These are variables that <em>unambiguously</em> identify the respondent. Examples are the passport number, social security number, etc.</li>
<li><em>Quasi-identifiers or key variables</em>. These are variables which identify the respondent with some degree of ambiguity. (Nonetheless, a combination of quasi-identifiers may provide unambiguous identification.) Examples are name, address, gender, age, telephone number, etc.</li>
<li><em>Confidential outcome variables</em>. These are variables which contain sensitive information on the respondent. Examples are salary, religion, political affiliation, health condition, etc.</li>
<li><em>Non-confidential outcome variables</em>. Those variables which do not fall in any of the categories above.</li>
</ul>
<p>The purpose of SDC is to prevent confidential information from being linked to specific respondents. Therefore, we will assume in what follows that original microdata sets to be protected have been pre-processed so as to remove identifiers and quasi-identifiers with low ambiguity (such as name).</p>
<p>The purpose of microdata SDC mentioned in the previous section can be stated more formally by saying that, given an original microdata set <span class="math inline">\(\mathbf{X}\)</span>, the goal is to release a protected microdata set <span class="math inline">\(\mathbf{X}'\)</span> in such a way that:</p>
<ol type="1">
<li>Disclosure risk (<em>i.e.</em> the risk that a user or an intruder can use <span class="math inline">\(\mathbf{X}'\)</span> to determine confidential variables on a specific individual among those in <span class="math inline">\(\mathbf{X}\)</span>) is low.</li>
<li>User analyses (regressions, means, etc.) on <span class="math inline">\(\mathbf{X}'\)</span> and on <span class="math inline">\(\mathbf{X}\)</span> yield the same or at least similar results.</li>
</ol>
<p>Microdata protection methods can generate the protected microdata set <span class="math inline">\(\mathbf{X}'\)</span></p>
<ul>
<li>either by <em>masking original data</em>, <em>i.e.</em> generating <span class="math inline">\(\mathbf{X}'\)</span> a modified version of the original microdata set <span class="math inline">\(\mathbf{X}\)</span>;</li>
<li>or by <em>generating synthetic data</em> <span class="math inline">\(\mathbf{X}'\)</span> that preserve some statistical properties of the original data <span class="math inline">\(\mathbf{X}\)</span>.</li>
</ul>
<p>Masking methods can in turn be divided in two categories depending on their effect on the original data (Willenborg and DeWaal, 2001):</p>
<ul>
<li><em>Perturbative masking</em>. The microdata set is distorted before publication. In this way, unique combinations of scores in the original dataset may disappear and new unique combinations may appear in the perturbed dataset; such confusion is beneficial for preserving statistical confidentiality. The perturbation method used should be such that statistics computed on the perturbed dataset do not differ significantly from the statistics that would be obtained on the original dataset.</li>
<li><em>Non-perturbative masking</em>. Non-perturbative methods do not alter data; rather, they produce partial suppressions or reductions of detail in the original dataset. Global recoding, local suppression and sampling are examples of non-perturbative masking.</li>
</ul>
<p>At a first glance, synthetic data seem to have the philosophical advantage of circumventing the re-identification problem: since published records are invented and do not derive from any original record, some authors claim that no individual having supplied original data can complain from having been re-identified. At a closer look, some authors (<em>e.g.</em>,&nbsp;Winkler, 2004 and Reiter, 2005) claim that even synthetic data might contain some records that allow for re-identification of confidential information. In short, synthetic data overfitted to original data might lead to disclosure just as original data would. On the other hand, a clear problem of synthetic data is data utility: only the statistical properties explicitly selected by the data protector are preserved, which leads to the question whether the data protector should not directly publish the statistics he wants preserved rather than a synthetic microdata set.</p>
<p>So far in this section, we have classified microdata protection methods by their operating principle. If we consider the type of data on which they can be used, a different dichotomic classification applies:</p>
<ul>
<li><em>Continuous data</em>. A variable is considered continuous if it is numerical and arithmetic operations can be performed with it. Examples are income and age. Note that a numerical variable does not necessarily have an infinite range, as is the case for age. When designing methods to protect continuous data, one has the advantage that arithmetic operations are possible, and the drawback that every combination of numerical values in the original dataset is likely to be unique, which leads to disclosure if no action is taken.</li>
<li><em>Categorical data</em>. A variable is considered categorical when it takes values over a finite set and standard arithmetic operations do not make sense. Ordinal scales and nominal scales can be distinguished among categorical variables. In ordinal scales the order between values is relevant, whereas in nominal scales it is not. In the former case, max and min operations are meaningful while in the latter case only pairwise comparison is possible. The instruction level is an example of ordinal variable, whereas eye colour is an example of nominal variable. In fact, all quasi-identifiers in a microdata set are normally categorical nominal. When designing methods to protect categorical data, the inability to perform arithmetic operations is certainly inconvenient, but the finiteness of the value range is one property that can be successfully exploited.</li>
</ul>
</section>
<section id="sec-perturbative-masking" class="level3" data-number="3.4.2">
<h3 data-number="3.4.2" class="anchored" data-anchor-id="sec-perturbative-masking"><span class="header-section-number">3.4.2</span> Perturbative masking</h3>
<p>Perturbative statistical disclosure control (SDC) methods allow for the release of the entire microdata set, although perturbed values rather than exact values are released. Not all perturbative methods are designed for continuous data; this distinction is addressed further below for each method.</p>
<p>Most perturbative methods reviewed below (including noise addition, rank swapping, microaggregation and post-randomization) are special cases of matrix masking. If the original microdata set is <span class="math inline">\(\mathbf{X}\)</span>, then the masked microdata set <span class="math inline">\(\mathbf{X}'\)</span> is computed as</p>
<p><span class="math display">\[
\mathbf{X}'=\mathbf{A}\mathbf{X}\mathbf{B} + \mathbf{C}
\]</span></p>
<p>where <span class="math inline">\(\mathbf{A}\)</span> is a record-transforming mask, <span class="math inline">\(\mathbf{B}\)</span> is a variable-transforming mask and <span class="math inline">\(\mathbf{C}\)</span> is a displacing mask or noise (Duncan and Pearson, 1991).</p>
<p><a href="#tbl-perturbative-methods" class="quarto-xref">Table&nbsp;<span>3.2</span></a> lists the perturbative methods described below. For each method, the table indicates whether it is suitable for continuous and/or categorical data.</p>
<div id="tbl-perturbative-methods" class="quarto-float anchored">
<figure class="quarto-float quarto-float-tbl figure">
<div aria-describedby="tbl-perturbative-methods-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="table">
<thead>
<tr class="header">
<th><em>Method</em></th>
<th style="text-align: center;"><em>Continuous data</em></th>
<th style="text-align: center;"><em>Categorical data</em></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Noise addition</td>
<td style="text-align: center;">X</td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td>Microaggregation</td>
<td style="text-align: center;">X</td>
<td style="text-align: center;">(X)</td>
</tr>
<tr class="odd">
<td>Rank swapping</td>
<td style="text-align: center;">X</td>
<td style="text-align: center;">X</td>
</tr>
<tr class="even">
<td>Rounding</td>
<td style="text-align: center;">X</td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td>Resampling</td>
<td style="text-align: center;">X</td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td>PRAM</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">X</td>
</tr>
<tr class="odd">
<td>MASSC</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">X</td>
</tr>
</tbody>
</table>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-tbl" id="tbl-perturbative-methods-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;3.2: Perturbative methods vs.&nbsp;data types. “X” denotes applicable and “(X)” denotes applicable with some adaptation.
</figcaption>
</figure>
</div>
<section id="sec-noise-addition" class="level4" data-number="3.4.2.1">
<h4 data-number="3.4.2.1" class="anchored" data-anchor-id="sec-noise-addition"><span class="header-section-number">3.4.2.1</span> Noise addition</h4>
<p>The main noise addition algorithms in the literature are:</p>
<ul>
<li>Masking by uncorrelated noise addition</li>
<li>Masking by correlated noise addition</li>
<li>Masking by noise addition and linear transformation</li>
<li>Masking by noise addition and nonlinear transformation (Sullivan, 1989).</li>
</ul>
<p>For more details on specific algorithms, the reader can check Brand (2002).</p>
<p>In practice, only a limited set of noise addition methods is more commonly used: the first three listed methods. When using linear transformations, a decision has to be made whether to reveal to the data user the parameter <span class="math inline">\(c\)</span> determining the transformations to allow for bias adjustment in the case of sub-populations.</p>
<p>With the exception of the not very practical method of Sullivan(1989), noise addition is not suitable to protect categorical data. On the other hand, it is well suited for continuous data for the following reasons:</p>
<ul>
<li>It makes no assumptions on the range of possible values for <span class="math inline">\(\mathbf{X}_{i}\)</span> (which may be infinite).</li>
<li>The noise being added is typically continuous and with mean zero, which suits well with continuous original data.</li>
<li>No exact matching is possible with external files. Depending on the amount of noise added, approximate (interval) matching might be possible. More details can be found in <a href="#sec-perturbative-masking" class="quarto-xref"><span>Section 3.4.2</span></a>.</li>
</ul>
</section>
<section id="multiplicative-noise" class="level4" data-number="3.4.2.2">
<h4 data-number="3.4.2.2" class="anchored" data-anchor-id="multiplicative-noise"><span class="header-section-number">3.4.2.2</span> Multiplicative Noise</h4>
<p>One main challenge regarding additive noise with constant variance is that on one hand small values are strongly perturbed and on the other large values are weakly perturbed. For instance, in a business microdata set the large enterprises -- which are much easier to re-identify than the smaller ones -- remain still high at risk after noise addition. A possible way out is given by the multiplicative noise approach explained below.</p>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-7-contents" aria-controls="callout-7" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Expert level
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-7" class="callout-7-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Let <span class="math inline">\(\mathbf{X}\)</span> be the matrix of the original data and <span class="math inline">\(\mathbf{Z}\)</span> the matrix of continuous perturbation variables with expectation 1 and variance <span class="math inline">\(\sigma_{\mathbf{Z}}^{2} &gt; 0\)</span>. The corresponding anonymised data <span class="math inline">\(\mathbf{X}^{a}\)</span> is then obtained as</p>
<p><span class="math display">\[
\left(\mathbf{X}^{a}\right)_{ij} : = \mathbf{Z}_{ij} \cdot \mathbf{X}_{ij}
\]</span></p>
<p>for each pair <span class="math inline">\((i,j)\)</span>.</p>
<p>The following approach has been suggested by Höhne (2004). In a first step, for each record it is randomly decided whether its values are increased or decreased, each with 0.5-probability. This is done using the main factors <span class="math inline">\(1 - f\)</span> and <span class="math inline">\(1 + f\)</span>. In order to avoid that all values of some record are perturbed with the same noise, these main factors are themselves perturbed with some additive noise <span class="math inline">\(s\)</span> (where <span class="math inline">\(s &lt; f/2\)</span>). The following transformation is needed to preserve the first and second moments of the distribution:</p>
<p><span class="math display">\[
\mathbf{X}_{i}^{a^{R}} : = \frac{ \sigma_{\mathbf{X}} }{\sigma_{\mathbf{X}^{a}}} \left( \mathbf{X}_{i}^{a} - \mu_{\mathbf{X}^{a}} \right) + \mu_{\mathbf{X}},
\]</span></p>
<p>where <span class="math inline">\(\mu_{\mathbf{X}}\)</span> and <span class="math inline">\(\mu_{\mathbf{X}^{a}}\)</span> define the average of the original and anonymised variables, <span class="math inline">\(\sigma_{\mathbf{X}}\)</span> and <span class="math inline">\(\sigma_{\mathbf{X}^{a}}\)</span> the corresponding standard deviations, respectively.</p>
<p>Particularly if the original data follow a strongly skewed distribution, the deviations using this method may strongly depend on the configuration of the noise factors for some few, but large values. That is, despite consistency, means and sums might be unsatisfactorily reproduced. For this reason, (Höhne, 2004) suggests a slight modification of the method. At first, we generate normal distributed random variables <span class="math inline">\(\mathbf{W}_{i}\)</span> with expectation greater than zero and 'small' variance, s.t. the realisation of <span class="math inline">\(\mathbf{W}_{i}\)</span> yields a positive value. Afterwards, the data is sorted in descending order by the variable under consideration. Then, the record with the largest entry in this variable is diminished by</p>
<p><span class="math display">\[
\mathbf{X}_{1}^{a} = \left( 1 - \mathbf{W}_{1} \right) \mathbf{X}_{1} \quad .
\]</span></p>
<p>The records <span class="math inline">\(\mathbf{X}_{2} , \ldots , \mathbf{X}_{n-1}\)</span> are now perturbed as follows:</p>
<p><span class="math display">\[
\mathbf{X}_{i}^{a} = \begin{cases}
\left( 1 - \mathbf{W}_{i} \right) \mathbf{X}_{i} , &amp;\text{if}\quad \sum\limits_{k=1}^{i-1}\mathbf{X}_{k}^{a} &gt; \sum\limits_{k=1}^{i-1}\mathbf{X}_{k}\\
\left( 1 + \mathbf{W}_{i}\right) \mathbf{X}_{i} , &amp;\text{if}\quad \sum\limits_{k=1}^{i-1}\mathbf{X}_{k}^{a} \leq \sum\limits_{k=1}^{i-1}\mathbf{X}_{k} \quad .
\end{cases}
\]</span></p>
<p>Hence, means and sums are preserved and the diminishing and enlarging effects of single values cancel out each other. For the remaining record <span class="math inline">\(\mathbf{X}_{n}\)</span> we set</p>
<p><span class="math display">\[
\mathbf{X}_{n}^{a} = \mathbf{X}_{n} - \left( \sum\limits_{k=1}^{n-1} \mathbf{X}_{k}^{a} - \sum\limits_{k=1}^{n-1} \mathbf{X}_{k} \right)
\]</span></p>
<p>in order to preserve the overall sum.</p>
</div>
</div>
</div>
</section>
<section id="sec-microaggregation" class="level4" data-number="3.4.2.3">
<h4 data-number="3.4.2.3" class="anchored" data-anchor-id="sec-microaggregation"><span class="header-section-number">3.4.2.3</span> Microaggregation</h4>
<p>Microaggregation is a family of SDC techniques for continuous microdata. The rationale behind microaggregation is that confidentiality rules in use allow publication of microdata sets if records correspond to groups of <span class="math inline">\(k\)</span> or more individuals, where no individual dominates (<em>i.e.</em> contributes too much to) the group and <span class="math inline">\(k\)</span> is a threshold value. Strict application of such confidentiality rules leads to replacing individual values with values computed on small aggregates (microaggregates) prior to publication. This is the basic principle of microaggregation.</p>
<p>To obtain microaggregates in a microdata set with <span class="math inline">\(n\)</span> records, these are combined to form <span class="math inline">\(g\)</span> groups of size at least <span class="math inline">\(k\)</span>. For each variable, the average value over each group is computed and is used to replace each of the original averaged values. Groups are formed using a criterion of maximal similarity. Once the procedure has been completed, the resulting (modified) records can be published.</p>
<p>Microaggregation exists in several variants:</p>
<ul>
<li>Fixed vs.&nbsp;variable group (Defays and Nanopoulos, 1993), (Mateo-Sanz and Domingo-Ferrer, 1999), (Domingo-Ferrer and Mateo-Sanz, 2002), (Sande, 2002).</li>
<li>Exact optimal vs.&nbsp;heuristic microaggregation (Hansen and Mukherjee, 2003), (Oganian and Domingo-Ferrer, 2001).</li>
<li>Categorical microaggregation (V. Torra, 2004).</li>
</ul>
<p>More details on the microaggregation implemented in <span class="math inline">\(\mu\)</span>‑ARGUS are given in <a href="#sec-microaggregation-further" class="quarto-xref"><span>Section 3.4.5</span></a>.</p>
</section>
<section id="sec-data-rank-swapping" class="level4" data-number="3.4.2.4">
<h4 data-number="3.4.2.4" class="anchored" data-anchor-id="sec-data-rank-swapping"><span class="header-section-number">3.4.2.4</span> Data swapping and rank swapping</h4>
<p>Data swapping was originally presented as an SDC method for databases containing only categorical variables (Dalenius and Reiss, 1978). The basic idea behind the method is to transform a database by exchanging values of confidential variables among individual records. Records are exchanged in such a way that low-order frequency counts or marginals are maintained.</p>
<p>Even though the original procedure was not very used in practice (see Fienberg and McIntyre, 2004), its basic idea had a clear influence in subsequent methods. In Reiss, Post and Dalenius (1982) and Reiss (1984) data swapping was introduced to protect continuous and categorical microdata, respectively. Another variant of data swapping for microdata is <em>rank swapping</em>. Although originally described only for ordinal variables (Greenberg, 1987), rank swapping can also be used for any numerical variable (Moore, 1996). First, values of a variable <span class="math inline">\(\mathbf{X}_{i}\)</span> are ranked in ascending order, then each ranked value of <span class="math inline">\(\mathbf{X}_{i}\)</span> is swapped with another ranked value randomly chosen within a restricted range (<em>e.g.</em> the rank of two swapped values cannot differ by more than <span class="math inline">\(p\%\)</span> of the total number of records, where <span class="math inline">\(p\)</span> is an input parameter). This algorithm is independently used on each original variable in the original data set.</p>
<p>It is reasonable to expect that multivariate statistics computed from data swapped with this algorithm will be less distorted than those computed after an unconstrained swap. In earlier empirical work by these authors on continuous microdata protection (Domingo-Ferrer and Torra, 2001), rank swapping has been identified as a particularly well-performing method in terms of the trade-off between disclosure risk and information loss. Consequently, it is one of the techniques that have been implemented in the <span class="math inline">\(\mu\)</span>‑ARGUS package (see <em>e.g.</em> Hundepool <em>et al.</em>, 2014).</p>
<div class="callout callout-style-simple callout-note">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-body-container">
<p><strong>Example.</strong> In <a href="#tbl-ex-rank-swapping" class="quarto-xref">Table&nbsp;<span>3.3</span></a>, we can see an original microdata set on the left and its rankswapped version on the right. There are four variables and ten records in the original dataset; the second variable is alphanumeric, and the standard alphabetic order has been used to rank it. A value of <span class="math inline">\(p=10\)</span> has been used for all variables.</p>
<div id="tbl-ex-rank-swapping" class="quarto-layout-panel anchored">
<figure class="quarto-float quarto-float-tbl figure">
<div aria-describedby="tbl-ex-rank-swapping-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 44.4%;justify-content: center;">
<table class="table">
<caption>Original file</caption>
<colgroup>
<col style="width: 5%">
<col style="width: 25%">
<col style="width: 15%">
<col style="width: 25%">
</colgroup>
<tbody>
<tr class="odd">
<td style="text-align: right;">1</td>
<td style="text-align: center;">K</td>
<td style="text-align: right;">3.7</td>
<td style="text-align: right;">4.4</td>
</tr>
<tr class="even">
<td style="text-align: right;">2</td>
<td style="text-align: center;">L</td>
<td style="text-align: right;">3.8</td>
<td style="text-align: right;">3.4</td>
</tr>
<tr class="odd">
<td style="text-align: right;">3</td>
<td style="text-align: center;">N</td>
<td style="text-align: right;">3.0</td>
<td style="text-align: right;">4.8</td>
</tr>
<tr class="even">
<td style="text-align: right;">4</td>
<td style="text-align: center;">M</td>
<td style="text-align: right;">4.5</td>
<td style="text-align: right;">5.0</td>
</tr>
<tr class="odd">
<td style="text-align: right;">5</td>
<td style="text-align: center;">L</td>
<td style="text-align: right;">5.0</td>
<td style="text-align: right;">6.0</td>
</tr>
<tr class="even">
<td style="text-align: right;">6</td>
<td style="text-align: center;">H</td>
<td style="text-align: right;">6.0</td>
<td style="text-align: right;">7.5</td>
</tr>
<tr class="odd">
<td style="text-align: right;">7</td>
<td style="text-align: center;">H</td>
<td style="text-align: right;">4.5</td>
<td style="text-align: right;">10.0</td>
</tr>
<tr class="even">
<td style="text-align: right;">8</td>
<td style="text-align: center;">F</td>
<td style="text-align: right;">6.7</td>
<td style="text-align: right;">11.0</td>
</tr>
<tr class="odd">
<td style="text-align: right;">9</td>
<td style="text-align: center;">D</td>
<td style="text-align: right;">8.0</td>
<td style="text-align: right;">9.5</td>
</tr>
<tr class="even">
<td style="text-align: right;">10</td>
<td style="text-align: center;">C</td>
<td style="text-align: right;">10.0</td>
<td style="text-align: right;">3.2</td>
</tr>
</tbody>
</table>
</div>
<div class="quarto-figure-spacer quarto-layout-cell" style="flex-basis: 11.1%;justify-content: flex-start;">
<p>&nbsp;</p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 44.4%;justify-content: center;">
<table class="table">
<caption>Rankswapped file</caption>
<colgroup>
<col style="width: 5%">
<col style="width: 25%">
<col style="width: 15%">
<col style="width: 25%">
</colgroup>
<tbody>
<tr class="odd">
<td style="text-align: right;">1</td>
<td style="text-align: center;">H</td>
<td style="text-align: right;">3.0</td>
<td style="text-align: right;">4.8</td>
</tr>
<tr class="even">
<td style="text-align: right;">2</td>
<td style="text-align: center;">L</td>
<td style="text-align: right;">4.5</td>
<td style="text-align: right;">3.2</td>
</tr>
<tr class="odd">
<td style="text-align: right;">3</td>
<td style="text-align: center;">M</td>
<td style="text-align: right;">3.7</td>
<td style="text-align: right;">4.4</td>
</tr>
<tr class="even">
<td style="text-align: right;">4</td>
<td style="text-align: center;">N</td>
<td style="text-align: right;">5.0</td>
<td style="text-align: right;">6.0</td>
</tr>
<tr class="odd">
<td style="text-align: right;">5</td>
<td style="text-align: center;">L</td>
<td style="text-align: right;">4.5</td>
<td style="text-align: right;">5.0</td>
</tr>
<tr class="even">
<td style="text-align: right;">6</td>
<td style="text-align: center;">F</td>
<td style="text-align: right;">6.7</td>
<td style="text-align: right;">9.5</td>
</tr>
<tr class="odd">
<td style="text-align: right;">7</td>
<td style="text-align: center;">K</td>
<td style="text-align: right;">3.8</td>
<td style="text-align: right;">11.0</td>
</tr>
<tr class="even">
<td style="text-align: right;">8</td>
<td style="text-align: center;">H</td>
<td style="text-align: right;">6.0</td>
<td style="text-align: right;">10.0</td>
</tr>
<tr class="odd">
<td style="text-align: right;">9</td>
<td style="text-align: center;">C</td>
<td style="text-align: right;">10.0</td>
<td style="text-align: right;">7.5</td>
</tr>
<tr class="even">
<td style="text-align: right;">10</td>
<td style="text-align: center;">D</td>
<td style="text-align: right;">8.0</td>
<td style="text-align: right;">3.4</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-tbl" id="tbl-ex-rank-swapping-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;3.3: Example of rank swapping.
</figcaption>
</figure>
</div>
</div>
</div>
</div>
</section>
<section id="sec-rounding" class="level4" data-number="3.4.2.5">
<h4 data-number="3.4.2.5" class="anchored" data-anchor-id="sec-rounding"><span class="header-section-number">3.4.2.5</span> Rounding</h4>
<p>Rounding methods replace original values of variables with rounded values. For a given variable <span class="math inline">\(X_{i}\)</span>, rounded values are chosen among a set of rounding points defining a <em>rounding set</em>. In a multivariate original dataset, rounding is usually performed one variable at a time (<em>univariate</em> rounding); however, multivariate rounding is also possible&nbsp;(Willenborg and DeWaal, 2001). The operating principle of rounding makes it suitable for continuous data.</p>
<div class="callout callout-style-simple callout-note">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-body-container">
<p><strong>Example</strong> Assume a non-negative continuous variable <span class="math inline">\(X\)</span>. Then we have to determine a set of rounding points<span class="math inline">\(\left\{ p_0,\cdots,p_r \right\}\)</span>. One possibility is to take rounding points as multiples of a base value <span class="math inline">\(b\)</span>, that is, <span class="math inline">\(p_{i} = b i\)</span> for <span class="math inline">\(i = 1,\cdots,r\)</span>. The set of attraction for each rounding point <span class="math inline">\(p_i\)</span> is defined as the interval <span class="math inline">\(\left\lbrack p_{i} - b/2,p_{i} + b/2 \right)\)</span>, for <span class="math inline">\(i = 1\)</span> to <span class="math inline">\(r - 1\)</span>. For <span class="math inline">\(p_0\)</span> and <span class="math inline">\(p_r\)</span>, respectively, the sets of attraction are <span class="math inline">\(\left\lbrack 0, b/2 \right)\)</span> and <span class="math inline">\(\left\lbrack p_{r} - b/2, X_{\text{max}} \right\rbrack\)</span>, where <span class="math inline">\(X_{\text{max}}\)</span> is the largest possible value for variable <span class="math inline">\(X\)</span>. Now an original value <span class="math inline">\(x\)</span> of <span class="math inline">\(X\)</span> is replaced with the rounding point corresponding to the set of attraction where <span class="math inline">\(x\)</span> lies.</p>
</div>
</div>
</div>
</section>
<section id="resampling" class="level4" data-number="3.4.2.6">
<h4 data-number="3.4.2.6" class="anchored" data-anchor-id="resampling"><span class="header-section-number">3.4.2.6</span> Resampling</h4>
<p>Originally proposed for protecting tabular data (Heer, 1993), (Domingo-Ferrer and Mateo-Sanz, 1999), resampling can also be used for microdata. Take <span class="math inline">\(t\)</span> independent samples <span class="math inline">\(S_{1},\cdots,S_{t}\)</span> of the values of an original variable <span class="math inline">\(X_{i}\)</span>. Sort all samples using the same ranking criterion. Build the masked variable <span class="math inline">\(Z_{i}\)</span> as <span class="math inline">\({\overline{x}}_{1},\cdots,{\overline{x}}_{n}\)</span>, where <span class="math inline">\(n\)</span> is the number of records and <span class="math inline">\({\overline{x}}_{j}\)</span> is the average of the <span class="math inline">\(j\)</span>-th ranked values in <span class="math inline">\(S_{1},\cdots,S_{t}\)</span>.</p>
</section>
<section id="pram" class="level4" data-number="3.4.2.7">
<h4 data-number="3.4.2.7" class="anchored" data-anchor-id="pram"><span class="header-section-number">3.4.2.7</span> PRAM</h4>
<p>The Post-RAndomization Method or PRAM (Gouweleeuw <em>et al.</em>, 1997) is a probabilistic, perturbative method for disclosure protection of categorical variables in microdata files. In the masked file, the scores on some categorical variables for certain records in the original file are changed to a different score according to a prescribed probability mechanism, namely a Markov matrix. The Markov approach makes PRAM very general, because it encompasses noise addition, data suppression and data recoding.</p>
<p>PRAM information loss and disclosure risk largely depend on the choice of the Markov matrix and are still (open) research topics&nbsp;(De Wolf et al., 1999).</p>
<p>The PRAM matrix contains a row for each possible value of each variable to be protected. This rules out using the method for continuous data. More details on PRAM can be found in <a href="#sec-PRAM" class="quarto-xref"><span>Section 3.4.6</span></a>.</p>
</section>
<section id="massc" class="level4" data-number="3.4.2.8">
<h4 data-number="3.4.2.8" class="anchored" data-anchor-id="massc"><span class="header-section-number">3.4.2.8</span> MASSC</h4>
<p>MASSC (Singh, Yu and Dunteman, 2003) is a masking method whose acronym summarizes its four steps: Micro Agglomeration, Substitution, Subsampling and Calibration. We briefly recall the purpose of those four steps:</p>
<ol type="1">
<li>Micro agglomeration is applied to partition the original dataset into risk strata (groups of records which are at a similar risk of disclosure). These strata are formed using the key variables, <em>i.e.</em> the quasi-identifiers in the records. The idea is that those records with rarer combinations of key variables are at a higher risk.</li>
<li>Optimal probabilistic substitution is then used to perturb the original data (<em>i.e.</em> substitution is governed by a Markov matrix like in PRAM, see [Singh, Yu and Dunteman, 2003] for details).</li>
<li>Optimal probabilistic subsampling is used to suppress some variables or even entire records (<em>i.e.</em> variables and/or records are suppressed with a certain probability set as parameters).</li>
<li>Optimal sampling weight calibration is used to preserve estimates for outcome variables in the treated database whose accuracy is critical for the intended data use.</li>
</ol>
<p>MASSC, to the best of our knowledge, is the first attempt at designing a perturbative masking method in such a way that disclosure risk can be analytically quantified. Its main shortcoming is that its disclosure model simplifies reality by considering only disclosure resulting from linkage of key variables with external sources. Since key variables are typically categorical, the uniqueness approach can be used to analyze the risk of disclosure; however, doing so ignores the fact that continuous outcome variables can also be used for respondent re-identification. As an example, if respondents are companies and turnover is one outcome variable, everyone in a certain industrial sector knows which is the company with largest turnover. Thus, in practice, MASSC is a method only suited when continuous variables are not present.</p>
</section>
</section>
<section id="sec-nonperturbative-masking" class="level3" data-number="3.4.3">
<h3 data-number="3.4.3" class="anchored" data-anchor-id="sec-nonperturbative-masking"><span class="header-section-number">3.4.3</span> Non-perturbative masking</h3>
<p>Non-perturbative masking does not rely on distortion of the original data but on partial suppressions or reductions of detail. Some of the methods are usable on both categorical and continuous data, but others are not suitable for continuous data. <a href="#tbl-non-perturbative-methods" class="quarto-xref">Table&nbsp;<span>3.4</span></a> lists the non-perturbative methods described below. For each method, the <a href="#tbl-non-perturbative-methods" class="quarto-xref">Table&nbsp;<span>3.4</span></a> indicates whether it is suitable for continuous and/or categorical data.</p>
<div id="tbl-non-perturbative-methods" class="quarto-float anchored">
<figure class="quarto-float quarto-float-tbl figure">
<div aria-describedby="tbl-non-perturbative-methods-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="table">
<thead>
<tr class="header">
<th><em>Method</em></th>
<th style="text-align: center;"><em>Continuous data</em></th>
<th style="text-align: center;"><em>Categorical data</em></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Sampling</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">X</td>
</tr>
<tr class="even">
<td>Global recoding</td>
<td style="text-align: center;">X</td>
<td style="text-align: center;">X</td>
</tr>
<tr class="odd">
<td>Top and bottom coding</td>
<td style="text-align: center;">X</td>
<td style="text-align: center;">X</td>
</tr>
<tr class="even">
<td>Local suppression</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">X</td>
</tr>
</tbody>
</table>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-tbl" id="tbl-non-perturbative-methods-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;3.4: Non-perturbative methods vs.&nbsp;data types.
</figcaption>
</figure>
</div>
<section id="sec-sampling" class="level4" data-number="3.4.3.1">
<h4 data-number="3.4.3.1" class="anchored" data-anchor-id="sec-sampling"><span class="header-section-number">3.4.3.1</span> Sampling</h4>
<p>Instead of publishing the original microdata file, what is published is a sample <span class="math inline">\(S\)</span> of the original set of records.</p>
<p>Sampling methods are suitable for categorical microdata, but their adequacy for continuous microdata is less clear in a general disclosure scenario. The reason is that such methods leave a continuous variable <span class="math inline">\(V_{i}\)</span> unperturbed for all records in <span class="math inline">\(S\)</span>. Thus, if variable <span class="math inline">\(V_{i}\)</span> is present in an external administrative public file, unique matches with the published sample are very likely: indeed, given a continuous variable <span class="math inline">\(V_{i}\)</span> and two respondents <span class="math inline">\(o_{1}\)</span> and <span class="math inline">\(o_{2}\)</span>, it is highly unlikely that <span class="math inline">\(V_{i}\)</span> will take the same value for both <span class="math inline">\(o_{1}\)</span> and <span class="math inline">\(o_{2}\)</span> unless <span class="math inline">\(o_{1} = o_{2}\)</span> (this is true even if <span class="math inline">\(V_{i}\)</span> has been truncated to represent it digitally).</p>
<p>If, for a continuous identifying variable, the score of a respondent is only approximately known by an attacker (as assumed in&nbsp;Willenborg and De Waal, 1996) it might still make sense to use sampling methods to protect that variable. However, assumptions on restricted attacker resources are perilous and may prove definitely too optimistic if good quality external administrative files are at hand. For the purpose of illustration, the example&nbsp;below gives the technical specifications of a real-world application of sampling.</p>
<div class="callout callout-style-simple callout-note">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-body-container">
<p><strong>Example</strong> Statistics Catalonia released in 1995 a sample of the 1991 population census of Catalonia. The information released corresponds to 36 categorical variables (including the recoded versions of initially continuous variables); some of the variables are related to the individual person and some to the household. The technical specifications of the sample were as follows:</p>
<ul>
<li>Sampling algorithm: Simple random sampling.</li>
<li>Sampling unit: Individuals in the population whose residence was in Catalonia as of March 1, 1991.</li>
<li>Population size: 6,059,494 inhabitants</li>
<li>Sample size: 245,944 individual records</li>
<li>Sampling fraction: <em>0.0406</em></li>
</ul>
<p>With the above sampling fraction, the maximum absolute error for estimating a maximum-variance proportion is 0.2 percent.</p>
</div>
</div>
</div>
</section>
<section id="sec-global-recoding" class="level4" data-number="3.4.3.2">
<h4 data-number="3.4.3.2" class="anchored" data-anchor-id="sec-global-recoding"><span class="header-section-number">3.4.3.2</span> Global recoding</h4>
<p>For a categorical variable <span class="math inline">\(V_{i}\)</span>, several categories are combined to form new (less specific) categories, thus resulting in a new <span class="math inline">\(V_{i}'\)</span> with <span class="math inline">\(\left| D\left( V_{i}' \right) \right| &lt; \left| D\left( V_{i} \right) \right|\)</span> where <span class="math inline">\(|\cdot |\)</span> is the cardinality operator and <span class="math inline">\(D(V_i)\)</span> denotes the domain of variable <span class="math inline">\(V_i\)</span>, <em>i.e.</em>, the possible values <span class="math inline">\(V_i\)</span> can have. For a continuous variable, global recoding means replacing <span class="math inline">\(V_{i}\)</span> by another variable <span class="math inline">\(V_{i}'\)</span> which is a discretized version of <span class="math inline">\(V_{i}\)</span>. In other words, a potentially infinite range <span class="math inline">\(D\left( V_{i} \right)\)</span> is mapped onto a finite range <span class="math inline">\(D\left( V_{i}' \right)\)</span>. This is the technique used in <span class="math inline">\(\mu\)</span>‑ARGUS (see <em>e.g.</em> Hundepool <em>et al.</em> 2014).</p>
<p>This technique is more appropriate for categorical microdata, where it helps disguise records with strange combinations of categorical variables. Global recoding is used heavily by statistical offices.</p>
<div class="callout callout-style-simple callout-note">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-body-container">
<p><strong>Example.</strong> If there is a record with “Marital status = Widow/er” and “Age = 17”, global recoding could be applied to “Marital status” to create a broader category “Widow/er or divorced”, so that the probability of the above record being unique would diminish. Global recoding can also be used on a continuous variable, but the inherent discretization leads very often to an unaffordable loss of information. Also, arithmetical operations that were straightforward on the original <span class="math inline">\(V_{i}\)</span> are no longer easy or intuitive on the discretized <span class="math inline">\(V_{i}'\)</span>.</p>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-note">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-body-container">
<p><strong>Example.</strong> We can recode the variable ‘Occupation’, by combining the categories ‘Statistician’ and ‘Mathematician’ into a single category ‘Statistician or Mathematician’. When the number of female statisticians in Urk (a small town) plus the number of female mathematicians in Urk is sufficiently high, then the combination ‘Place of residence = Urk’, ‘Gender = Female’ and ‘Occupation = Statistician or Mathematician’ is considered safe for release. Note that instead of recoding ‘Occupation’ one could also recode ‘Place of residence’ for instance.</p>
</div>
</div>
</div>
<p>It is important to realise that global recoding is applied to the whole data set, not only to the unsafe part of the set. This is done to obtain a uniform categorisation of each variable. Suppose, for instance, that we recode the ‘Occupation’ in the above way. Suppose furthermore that both the combinations ‘Place of residence = Amsterdam’, ‘Gender = Female’ and ‘Occupation = Statistician’, and ‘Place of residence = Amsterdam’, ‘Gender = Female’ and ‘Occupation = Mathematician’ are considered safe. To obtain a uniform categorisation of ‘Occupation’ we would, however, not publish these combinations, but only the combination ‘Place of residence = Amsterdam’, ‘Gender = Female’ and ‘Occupation = Statistician or Mathematician’.</p>
</section>
<section id="sec-top-and-bottom-coding" class="level4" data-number="3.4.3.3">
<h4 data-number="3.4.3.3" class="anchored" data-anchor-id="sec-top-and-bottom-coding"><span class="header-section-number">3.4.3.3</span> Top and bottom coding</h4>
<p>Top and bottom coding is a special case of global recoding which can be used on variables that can be ranked, that is, continuous or categorical ordinal. The idea is that top values (those above a certain threshold) are lumped together to form a new category. The same is done for bottom values (those below a certain threshold). See&nbsp;the <span class="math inline">\(\mu\)</span>‑ARGUS manual (<em>e.g.</em> Hundepool <em>et al.</em> 2014).</p>
</section>
<section id="sec-local-suppression" class="level4" data-number="3.4.3.4">
<h4 data-number="3.4.3.4" class="anchored" data-anchor-id="sec-local-suppression"><span class="header-section-number">3.4.3.4</span> Local suppression</h4>
<p>Certain values of individual variables are suppressed with the aim of increasing the set of records agreeing on a combination of key values. Ways to combine local suppression and global recoding are discussed in (De Waal and Willenborg, 1995) and implemented in <span class="math inline">\(\mu\)</span>‑ARGUS&nbsp;(see <em>e.g.</em> Hundepool <em>et al.</em> 2014).</p>
<p>If a continuous variable <span class="math inline">\(V_{i}\)</span> is part of a set of key variables, then each combination of key values is probably unique. Since it does not make sense to systematically suppress the values of <span class="math inline">\(V_{i}\)</span>, we conclude that local suppression is rather oriented to categorical variables.</p>
<p>When local suppression is applied, one or more values in an unsafe combination are suppressed, <em>i.e.</em> replaced by a missing value. For instance, in the above example we can protect the unsafe combination ‘Place of residence = Urk’, ‘Gender = Female’ and ‘Occupation = Statistician’ by suppressing the value of ‘Occupation’, assuming that the number of females in Urk is sufficiently high. The resulting combination is then given by ‘Place of residence = Urk’, ‘Gender = Female’ and ‘Occupation = missing’. Note that instead of suppressing the value of ‘Occupation’ one could also suppress the value of another variable of the unsafe combination. For instance, when the number of female statisticians in the Netherlands is sufficiently high then one could suppress the value of ‘Place of residence’ instead of the value of ‘Occupation’ in the above example to protect the unsafe combination. A local suppression is only applied to a particular value. When, for instance, the value of ‘Occupation’ is suppressed in a particular record, then this does not imply that the value of ‘Occupation’ has to be suppressed in another record. The freedom that one has in selecting the values that are to be suppressed allows one to minimise the number of local suppressions.</p>
</section>
<section id="references-1" class="level4" data-number="3.4.3.5">
<h4 data-number="3.4.3.5" class="anchored" data-anchor-id="references-1"><span class="header-section-number">3.4.3.5</span> References</h4>
<p>Brand, R. (2002<em>). Microdata protection through noise addition.</em> In J. Domingo-Ferrer, editor, Inference Control in Statistical Databases, volume 2316 of LNCS, pages 97–116, Berlin Heidelberg, 2002. Springer.</p>
<p>Dalenius T., and Reiss, S.P. (1978). <em>Data-swapping: a technique for disclosure control</em> (extended abstract). In Proc. of the ASA Section on Survey Research Methods, pages 191–194, Washington DC, 1978. American Statistical Association.</p>
<p><em>Defays, D., and Nanopoulos, P. (1993). Panels of enterprises and confidentiality: the small aggregates method</em>. In Proc. of 92 Symposium on Design and Analysis of Longitudinal Surveys, pages 195–204, Ottawa, 1993. Statistics Canada.</p>
<p>De Waal, A.G., and Willenborg, L.C.R.J. (1995). <em>Global recodings and local suppressions in microdata sets</em>. In Proceedings of Statistics Canada Symposium’95, pages 121–132, Ottawa, 1995. Statistics Canada.</p>
<p>De Waal, A.G. and Willenborg, L.C.R.J. (1999). <em>Information loss through global recoding and local suppression</em>. Netherlands Official Statistics, 14:17–20, 1999. special issue on SDC.</p>
<p>De Wolf, P.P., Gouweleeuw, J. M., Kooiman, P., and Willenborg, L.C.R.J. (1999). <em>Reflections on PRAM</em>. In J. Domingo-Ferrer, editor, Statistical Data Protection, pages 337–349, Luxemburg, 1999. Office for Official Publications of the European Communities.</p>
<p>Domingo-Ferrer, J., and Mateo-Sanz, J.M. (1999). <em>On resampling for statistical confidentiality in contingency tables.</em> Computers &amp; Mathematics with Applications, 38:13–32, 1999.</p>
<p>Domingo-Ferrer, J., and Mateo-Sanz, J.M. (2002). <em>Practical data-oriented microaggregation for statistical disclosure control</em>. IEEE Transactions on Knowledge and Data Engineering, 14(1):189–201, 2002.</p>
<p>Domingo-Ferrer, J., Mateo-Sanz, J.M., and Torra, V. (2001). <em>Comparing sdc methods for microdata on the basis of information loss and disclosure risk.</em> In Pre-proceedings of ETK-NTTS’2001 (vol.&nbsp;2), pages 807–826, Luxemburg, 2001. Eurostat.</p>
<p>Domingo-Ferrer, J., and Torra, V., (2001). <em>Disclosure protection methods and information loss for microdata</em>. In P. Doyle, J.I. Lane, J.J.M. Theeuwes, and L. Zayatz, editors, Confidentiality, Disclosure and Data Access: Theory and Practical Applications for Statistical Agencies, pages 91–110, Amsterdam, 2001. North-Holland. <a href="https://crises-deim.urv.cat/webCrises/publications/bcpi/cliatpasa01Disclosure.pdf">https://crises-deim.urv.cat/webCrises/publications/bcpi/cliatpasa01Disclosure.pdf</a>.</p>
<p>Duncan, G.T., and Pearson, R.W. (1991). <em>Enhancing access to microdata while protecting confidentiality: prospects for the future</em>. Statistical Science, 6:219–239, 1991.</p>
<p>Fienberg, S.E., and McIntyre, J. (2004). <em>Data swapping: variations on a theme by dalenius and reiss.</em> In J. Domingo-Ferrer and V. Torra, editors, Privacy in Statistical Databases, volume 3050 of LNCS, pages 14–29, Berlin Heidelberg, 2004. Springer.</p>
<p>Gouweleeuw, J.M., Kooiman, P., Willenborg, L.C.R.J., and de Wolf, P.P. (1997<em>). Post randomisation for statistical disclosure control: Theory and implementation</em>, Research paper no. 9731 (Voorburg: Statistics Netherlands).</p>
<p>Greenberg, B. (1987). <em>Rank swapping for ordinal data</em>, Washington, DC: U. S. Bureau of the Census (unpublished manuscript).</p>
<p>Hansen, S.L., and Mukherjee, S. (2003). <em>A polynomial algorithm for optimal univariate microaggregation</em>. IEEE Transactions on Knowledge and Data Engineering, 15(4):1043–1044, 2003.</p>
<p>Heer, G.R. (1993). <em>A bootstrap procedure to preserve statistical confidentiality in contingency tables</em>. In D. Lievesley, editor, Proc. of the International Seminar on Statistical Confidentiality, pages 261–271, Luxemburg, 1993. Office for Official Publications of the European Communities.</p>
<p>Höhne (2004), Varianten von Zufallsüberlagerung (German), working paper of the project group 'De facto anonymisation of business microdata', Wiesbaden.</p>
<p>Hundepool, A., Van de Wetering, A., Ramaswamy, R., Franconi, L., Capobianchi, A., de Wolf, P.P., Domingo-Ferrer, J., Torra, V. and Giessing, S. (2014). <em><span class="math inline">\(\mu\)</span>-ARGUS version 5.1 Software and User’s Manual</em>. Statistics Netherlands, Voorburg NL, may 2005. <a href="https://research.cbs.nl/casc/Software/MUmanual5.1.3.pdf">https://research.cbs.nl/casc/Software/MUmanual5.1.3.pdf</a>.</p>
<p>Kooiman, P., Willenborg, L, and Gouweleeuw, J.M. (1998). <em>PRAM: A method for disclosure limitation of microdata</em>. Technical report, Statistics Netherlands (Voorburg, NL), 1998.</p>
<p>Mateo-Sanz, J.M., and Domingo-Ferrer, J. (1999) . <em>A method for data-oriented multivariate microaggregation</em>. In J. Domingo-Ferrer, editor, Statistical Data Protection, pages 89–99, Luxemburg, 1999. Office for Official Publications of the European Communities.</p>
<p>Moore, R. (1996). <em>Controlled data swapping techniques for masking public use microdata sets, 1996</em>. U. S. Bureau of the Census, Washington, DC, (unpublished manuscript).</p>
<p>Oganian, A., and Domingo-Ferrer, J. (2001). <em>On the complexity of optimal microaggregation for statistical disclosure control</em>. Statistical Journal of the United Nations Economic Commissions for Europe, 18(4):345–354, 2001.</p>
<p>Reiss, S.P, (1984). <em>Practical data-swapping: the first steps</em>. ACM Transactions on Database Systems, 9:20–37, 1984.</p>
<p>Reiss, S.P., Post, M.J., and Dalenius, T. (1982). <em>Non-reversible privacy transformations</em>. In Proceedings of the ACM Symposium on Principles of Database Systems, pages 139–146, Los Angeles, CA, 1982. ACM.</p>
<p>Reiter, J.P. (2005). <em>Releasing multiply-imputed, synthetic public use microdata: An illustration and empirical study.</em> Journal of the Royal Statistical Society, Series A, 168:185–205, 2005.</p>
<p>Sande, G. (2002). <em>Exact and approximate methods for data directed microaggregation in one or more dimensions</em>. International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems, 10(5):459–476, 2002.</p>
<p>Singh, A. C, Yu, F., and Dunteman, G.H. (2003) <em>. Massc: A new data mask for limiting statistical information loss and disclosure</em>. In H. Linden, J. Riecan, and L. Belsby, editors, Work Session on Statistical Data Confidentiality 2003, Monographs in Official Statistics, pages 373–394, Luxemburg, 2004. Eurostat.</p>
<p>Sullivan, G.R. (1989). <em>The Use of Added Error to Avoid Disclosure in Microdata Releases</em>. PhD thesis, Iowa State University, 1989.</p>
<p>Torra, V. (2004). <em>Microaggregation for categorical variables: a median based approach</em>. In J. Domingo-Ferrer and V. Torra, editors, Privacy in Statistical Databases, volume 3050 of LNCS, pages 162–174, Berlin Heidelberg, 2004. Springer.</p>
<p>Willenborg, L. and De Waal, T. (1996) . <em>Statistical Disclosure Control in Practice</em>. Springer-Verlag, New York, 1996.</p>
<p>Willenborg, L., and De Waal, T. (2001). <em>Elements of Statistical Disclosure Control.</em> Springer-Verlag, New York, 2001.</p>
<p>Winkler, W.E. (2004). <em>Re-identification methods for masked microdata.</em> In J. Domingo-Ferrer and V. Torra, editors, Privacy in Statistical Databases, volume 3050 of LNCS, pages 216–230, Berlin Heidelberg, 2004. Springer.</p>
</section>
</section>
<section id="sec-noise-addition-details" class="level3" data-number="3.4.4">
<h3 data-number="3.4.4" class="anchored" data-anchor-id="sec-noise-addition-details"><span class="header-section-number">3.4.4</span> Noise addition</h3>
<p>We sketch in this Section the operation of the main noise addition algorithms in the literature for microdata protection. For more details on specific algorithms, the reader can check (Brand, 2002).</p>
<section id="masking-by-uncorrelated-noise-addition" class="level4" data-number="3.4.4.1">
<h4 data-number="3.4.4.1" class="anchored" data-anchor-id="masking-by-uncorrelated-noise-addition"><span class="header-section-number">3.4.4.1</span> Masking by uncorrelated noise addition</h4>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-8-contents" aria-controls="callout-8" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Expert level
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-8" class="callout-8-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Masking by additive noise assumes that the vector of observations <span class="math inline">\(x_{j}\)</span> for the <em>j</em>-th variable of the original dataset <span class="math inline">\(X_{j}\)</span> is replaced by a vector where <span class="math inline">\(\varepsilon_{j}\)</span> is a vector of normally distributed errors drawn from a random variable <span class="math inline">\(\varepsilon_{j} \sim  N\left( 0,\sigma_{\varepsilon_{j}}^{2} \right)\)</span>, such that <span class="math inline">\(\text{Cov}\left( \varepsilon_{t},\varepsilon_{l} \right)=0\)</span> for all <span class="math inline">\(t \neq l\)</span> (white noise).</p>
<p>The general assumption in the literature is that the variances of the <span class="math inline">\(\varepsilon_{j}\)</span> are proportional to those of the original variables. Thus, if <span class="math inline">\(\sigma_{j}^{2}\)</span> is the variance of <span class="math inline">\(X_{j}\)</span>, then <span class="math inline">\(\sigma_{\varepsilon_{j}}^{2}: = \alpha\sigma_{j}^{2}\)</span>.</p>
<p>In the case of a <span class="math inline">\(p\)</span>-dimensional dataset, simple additive noise masking can be written in matrix notation as <span class="math inline">\(Z=X + \epsilon\)</span>, where <span class="math inline">\(X \sim  (\mu,\Sigma)\)</span>, <span class="math inline">\(\varepsilon \sim  \left( 0,\Sigma_{\varepsilon} \right)\)</span> and</p>
<p><span class="math inline">\(\Sigma_{\varepsilon} = \alpha \cdot \text{diag}\left( \sigma_{1}^{2},\sigma_{2}^{2},\cdots,\sigma_{p}^{2} \right)\)</span>, for <span class="math inline">\(\alpha &gt; 0\)</span></p>
<p>This method preserves means and covariances, <em>i.e.</em></p>
<p><span class="math display">\[\begin{align}
\mathbb{E}(Z) &amp;= \mathbb{E}(X) + \mathbb{E}(\epsilon) = \mathbb{E}(X) = \mu\\
\operatorname{Cov}(Z_j,Z_l) &amp;= \operatorname{Cov}(X_j,X_l) \quad \forall j\neq l
\end{align}\]</span></p>
<p>Unfortunately, neither variances nor correlation coefficients are preserved:</p>
<p><span class="math display">\[
\operatorname{Var}\left( Z_{j} \right) = \operatorname{Var}\left( X_{j} \right) + \alpha\operatorname{Var}\left( X_{j} \right) = (1 + \alpha)\operatorname{Var}\left( X_{j} \right)
\]</span></p>
<p><span class="math display">\[
\rho(Z_j, Z_l)=\frac{\operatorname{Cov}(Z_j, Z_l)}{\sqrt{\operatorname{Var}(X_j)\operatorname{Var}(X_l)}} = \frac{1}{1+\alpha} \rho(X_j, X_l),\forall j \neq l
\]</span></p>
</div>
</div>
</div>
</section>
<section id="masking-by-correlated-noise-addition" class="level4" data-number="3.4.4.2">
<h4 data-number="3.4.4.2" class="anchored" data-anchor-id="masking-by-correlated-noise-addition"><span class="header-section-number">3.4.4.2</span> Masking by correlated noise addition</h4>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-9-contents" aria-controls="callout-9" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Expert level
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-9" class="callout-9-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Correlated noise addition also preserves means and additionally allows preservation of correlation coefficients. The difference with the previous method is that the covariance matrix of the errors is now proportional to the covariance matrix of the original data, <em>i.e.</em> <span class="math inline">\(\varepsilon \sim  (0,\Sigma)\)</span>, where <span class="math inline">\(\Sigma_{\varepsilon} = \alpha\Sigma\)</span>.</p>
<p>With this method, we have that the covariance matrix of the masked data is</p>
<p><span id="eq-sigma_z"><span class="math display">\[
\Sigma_{z} = \Sigma + \alpha\Sigma = (1 + \alpha)\Sigma  .
\tag{3.4}\]</span></span></p>
<p>Preservation of correlation coefficients follows, since</p>
<p><span class="math display">\[
\rho (Z_j, Z_l) = \frac{1 + \alpha}{1 + \alpha}\frac{\operatorname{Cov}\left( X_{j},X_{l} \right)}{\sqrt{\operatorname{Var}\left( X_{j} \right)\operatorname{Var}\left( X_{l} \right)}} = \rho(X_{j},X_{l})
\]</span></p>
<p>Regarding variances and covariances, we can see from <a href="#eq-sigma_z" class="quarto-xref">Equation&nbsp;<span>3.4</span></a> that masked data only provide biased estimates for them. However, it is shown in Kim (1990) that the covariance matrix of the original data can be consistently estimated from the masked data as long as <span class="math inline">\(\alpha\)</span> is known.</p>
<p>As a summary, masking by correlated noise addition outputs masked data with higher analytical validity than masking by uncorrelated noise addition. Consistent estimators for several important statistics can be obtained as long as <span class="math inline">\(\alpha\)</span> is revealed to the data user. However, simple noise addition as discussed in this section and in the previous one is seldom used because of the very low level of protection it provides&nbsp;(Tendick, 1991), (Tendick and Matloff, 1994).</p>
</div>
</div>
</div>
</section>
<section id="masking-by-noise-addition-and-linear-transformations" class="level4" data-number="3.4.4.3">
<h4 data-number="3.4.4.3" class="anchored" data-anchor-id="masking-by-noise-addition-and-linear-transformations"><span class="header-section-number">3.4.4.3</span> Masking by noise addition and linear transformations</h4>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-10-contents" aria-controls="callout-10" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Expert level
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-10" class="callout-10-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>In&nbsp;Kim (1986), a method is proposed that ensures by additional transformations that the sample covariance matrix of the masked variables is an unbiased estimator for the covariance matrix of the original variables. The idea is to use simple additive noise on the <span class="math inline">\(p\)</span> original variables to obtain overlayed variables</p>
<p><span class="math display">\[
Z_{j} = X_{j} + \varepsilon_{j},\quad \text{for } j = 1,\ldots,p
\]</span> As in the previous section on correlated masking, the covariances of the errors <span class="math inline">\(\varepsilon_{j}\)</span> are taken proportional to those of the original variables. Usually, the distribution of errors is chosen to be normal or the distribution of the original variables, although in&nbsp;Roque (2000) mixtures of multivariate normal noise are proposed.</p>
<p>In a second step, every overlayed variable <span class="math inline">\(Z_{j}\)</span> is transformed into a masked variable <span class="math inline">\(G_{j}\)</span> as</p>
<p><span class="math display">\[
G_{j} = cZ_{j} + d_{j}
\]</span></p>
<p>In matrix notation, this yields</p>
<p><span class="math display">\[
Z = X + \varepsilon
\]</span></p>
<p><span class="math display">\[
G = cZ_{j} + D = c(X + \varepsilon) + D
\]</span></p>
<p>where <span class="math inline">\(X \sim  N(\mu,\Sigma),\varepsilon \sim  \left( 0,\alpha\Sigma \right),G \sim  (\mu,\Sigma)\)</span> and <span class="math inline">\(D\)</span> is a matrix whose <span class="math inline">\(j\)</span>-th column contains the scalar <span class="math inline">\(d_{j}\)</span> in all rows. Parameters <span class="math inline">\(c\)</span> and <span class="math inline">\(d_{j}\)</span> are determined under the restrictions that <span class="math inline">\(\mathbb{E}\left( G_{j} \right) = \mathbb{E}\left( X_{j} \right)\)</span> and <span class="math inline">\(\operatorname{Var}\left( G_{j} \right) = \operatorname{Var}\left( X_{j} \right)\)</span> for <span class="math inline">\(j = 1,\cdots,p\)</span>. In fact, the first restriction implies that <span class="math inline">\(d_{j} = (1 - c)\mathbb{E}\left( X_{j} \right)\)</span>, so that the linear transformations depend on a single parameter <span class="math inline">\(c\)</span>.</p>
<p>Due to the restrictions used to determine <span class="math inline">\(c\)</span>, this methods preserves expected values and covariances of the original variables and is quite good in terms of analytical validity. Regarding analysis of regression estimates in subpopulations, it is shown in&nbsp;Kim (1990) that (masked) sample means and covariances are asymptotically biased estimates of the corresponding statistics on the original subpopulations. The magnitude of the bias depends on the parameter <span class="math inline">\(c\)</span>, so that estimates can be adjusted by the data user as long as <span class="math inline">\(c\)</span> is revealed to her —revealing <span class="math inline">\(c\)</span> to the user has a fundamental disadvantage, though: the user can undo the linear transformation, so that this method becomes equivalent to plain uncorrelated noise addition&nbsp;(Domingo-Ferrer, Sebé, and Castellà, 2004)</p>
<p>The most prominent shortcomings of this method are that it does not preserve the univariate distributions of the original data and that it cannot be applied to discrete variables due to the structure of the transformations.</p>
</div>
</div>
</div>
</section>
<section id="masking-by-noise-addition-and-nonlinear-transformations" class="level4" data-number="3.4.4.4">
<h4 data-number="3.4.4.4" class="anchored" data-anchor-id="masking-by-noise-addition-and-nonlinear-transformations"><span class="header-section-number">3.4.4.4</span> Masking by noise addition and nonlinear transformations</h4>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-11-contents" aria-controls="callout-11" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Expert level
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-11" class="callout-11-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>An algorithm combining simple additive noise and nonlinear transformation is proposed in Sullivan (1989). The advantages of this proposal are that it can be applied to discrete variables and that univariate distributions are preserved.</p>
<p>The method consists of several steps:</p>
<ol type="1">
<li>Calculate the empirical distribution function for every original variable.</li>
<li>Smooth the empirical distribution function.</li>
<li>Convert the smoothed empirical distribution function into a uniform random variable and this into a standard normal random variable.</li>
<li>Add noise to the standard normal variable.</li>
<li>Back-transform to values of the distribution function.</li>
<li>Back-transform to the original scale.</li>
</ol>
<p>In the European project CASC (IST-2000-25069), the practicality and usability of this algorithm was assessed. Unfortunately, the internal CASC report by&nbsp;Brand and Giessing (2002) concluded that:<br>
<em>“All in all, the results indicate that an algorithm as complex as the one proposed by Sullivan can only be applied by experts. Every application is very time-consuming and requires expert knowledge on the data and the algorithm.”</em></p>
</div>
</div>
</div>
</section>
<section id="summary-on-noise-addition" class="level4" data-number="3.4.4.5">
<h4 data-number="3.4.4.5" class="anchored" data-anchor-id="summary-on-noise-addition"><span class="header-section-number">3.4.4.5</span> Summary on noise addition</h4>
<p>In practice, only simple noise addition or noise addition with linear transformation are used. When using linear transformations, a decision has to be made whether to reveal to the data user the parameter <span class="math inline">\(c\)</span> determining the transformations to allow for bias adjustment in the case of subpopulations.</p>
<p>With the exception of the not very practical method of Sullivan (1989), additive noise is not suitable to protect categorical data. On the other hand, it is well suited for continuous data for the following reasons:</p>
<ul>
<li>It makes no assumptions on the range of possible values for <span class="math inline">\(\mathbf{X}_{i}\)</span> (which may be infinite).</li>
<li>The noise being added is typically continuous and with mean zero, which suits well continuous original data.</li>
<li>No exact matching is possible with external files. Depending on the amount of noise added, approximate (interval) matching might be possible.</li>
</ul>
</section>
<section id="references-2" class="level4" data-number="3.4.4.6">
<h4 data-number="3.4.4.6" class="anchored" data-anchor-id="references-2"><span class="header-section-number">3.4.4.6</span> References</h4>
<p>Brand, R. (2002). <em>Microdata protection through noise addition</em>. In J. Domingo-Ferrer, editor, Inference Control in Statistical Databases, volume 2316 of LNCS, pages 97–116, Berlin Heidelberg, 2002. Springer.</p>
<p>Brand, R. and Giessing, S. (2002). <em>Tests of the applicability of sullivan’s algorithm to synthetic data and real business data in official statistics</em>, European Project IST-2000-25069 CASC, Deliverable 1.1-D1, <a href="https://research.cbs.nl/casc/deliv/11d1.pdf">https://research.cbs.nl/casc/deliv/11d1.pdf</a>.</p>
<p>Domingo-Ferrer, J., Sebé, F., and Castellà, J. (2004). <em>On the security of noise addition for privacy in statistical databases.</em> In J. Domingo-Ferrer and V. Torra, editors, Privacy in Statistical Databases, volume 3050 of LNCS, pages 149–161, Berlin Heidelberg, 2004. Springer.</p>
<p>Kim, J. J. (1986). <em>A method for limiting disclosure in microdata based on random noise and transformation.</em> In Proceedings of the Section on Survey Research Methods, pages 303–308, Alexandria VA, American Statistical Association.</p>
<p>Kim, J. J. (1990). <em>Subpopulation estimation for the masked data.</em> In Proceedings of the ASA Section on Survey Research Methods, pages 456–461, Alexandria VA, 1990. American Statistical Association.</p>
<p>Roque, G. M. (2000).. <em>Masking Microdata Files with Mixtures of Multivariate Normal Distributions</em>. PhD thesis, University of California at Riverside, 2000.</p>
<p>Sullivan, G. R. (1989<em>). The Use of Added Error to Avoid Disclosure in Microdata Releases</em>. PhD thesis, Iowa State University.</p>
<p>Tendick, P. (1991). <em>Optimal noise addition for preserving confidentiality in multivariate data.</em> Journal of Statistical Planning and Inference, 27:341–353, 1991.</p>
<p>Tendick, P., and Matloff, N. (1994). <em>A modified random perturbation method for database security</em>. ACM Transactions on Database Systems, 19:47–63.</p>
</section>
</section>
<section id="sec-microaggregation-further" class="level3" data-number="3.4.5">
<h3 data-number="3.4.5" class="anchored" data-anchor-id="sec-microaggregation-further"><span class="header-section-number">3.4.5</span> Microaggregation: further details</h3>
<p>Consider a microdata set with <span class="math inline">\(p\)</span> continuous variables and <span class="math inline">\(n\)</span> records (<em>i.e.</em>, the result of recording <span class="math inline">\(p\)</span> variables on <span class="math inline">\(n\)</span> individuals). A particular record can be viewed as an instance of <span class="math inline">\(\mathbf{X}' = \left( \mathbf{X}_{1},\cdots,\mathbf{X}_{p} \right)\)</span>, where the <span class="math inline">\(\mathbf{X}_{i}\)</span> are the variables. With these individuals, <span class="math inline">\(g\)</span> groups are formed with <span class="math inline">\(n_{i}\)</span> individuals in the <span class="math inline">\(i\)</span>-th group (<span class="math inline">\(n_{i} \geq k\)</span> and <span class="math inline">\(n = \Sigma_{}^{}n_{i}\)</span>). Denote by <span class="math inline">\(x_{\text{ij}}\)</span> the <span class="math inline">\(j\)</span>-th record in the <span class="math inline">\(i\)</span>-th group; denote by <span class="math inline">\({\overline{x}}_{i}\)</span> the average record over the <span class="math inline">\(i\)</span>-th group, and by <span class="math inline">\(\overline{x}\)</span> the average record over the whole set of <span class="math inline">\(n\)</span> individuals.</p>
<p>The optimal <span class="math inline">\(k\)</span>-partition (from the information loss point of view) is defined to be the one that maximizes within-group homogeneity; the higher the within-group homogeneity, the lower the information loss, since microaggregation replaces values in a group by the group centroid. The sum of squares criterion is common to measure homogeneity in clustering. The within-groups sum of squares <span class="math inline">\(\text{SSE}\)</span> is defined as</p>
<p><span class="math display">\[
\text{SSE} = \sum\limits_{i = 1}^{g}\sum\limits_{j = 1}^{n_{i}}\left( x_{\text{ij}} - {\overline{x}}_{i} \right)^{T}\left( x_{\text{ij}} - {\overline{x}}_{i} \right)
\]</span></p>
<p>The lower <span class="math inline">\(\text{SSE}\)</span>, the higher the within group homogeneity. The total sum of squares is</p>
<p><span class="math display">\[
\text{SST} = \sum_{i = 1}^{g}\sum_{j = 1}^{n_{i}}\left( x_{\text{ij}} - \overline{x} \right)^{T}\left( x_{\text{ij}} - \overline{x} \right)
\]</span></p>
<p>In terms of sums of squares, the optimal <span class="math inline">\(k\)</span>-partition is the one that minimizes SSE.</p>
<p>For a microdata set consisting of <span class="math inline">\(p\)</span> variables, these can be microaggregated together or partitioned into several groups of variables. Also the way to form groups may vary. We next review the main proposals in the literature.</p>
<div class="callout callout-style-simple callout-note">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-body-container">
<p><strong>Example.</strong> This example illustrates the use of microaggregation for SDC and, more specifically, for <span class="math inline">\(k\)</span>-anonymization&nbsp;(Samarati and L.&nbsp;Sweeney, 1998), (Samarati, 2001), (Sweeney, 2002), (Domingo-Ferrer and Torra, 2005). A <span class="math inline">\(k\)</span>-anonymous dataset allows no re-identification of a respondent within a group of at least <span class="math inline">\(k\)</span>respondents. We show in <a href="#tbl-ex-sme-dataset" class="quarto-xref">Table&nbsp;<span>3.5</span></a> a dataset giving, for 11 small or medium enterprises (SMEs) in a certain town, the company name, the surface in square meters of the company’s premises, its number of employees, its turnover and its net profit. Clearly, the company name is an identifier. We will consider that turnover and net profit are confidential outcome variables. A first SDC measure is to suppress the identifier “Company name” when releasing the dataset for public use. However, note that the surface of the company’s premises and its number of employees can be used by a snooper as key variables. Indeed, it is easy for anybody to gauge to a sufficient accuracy the surface and number of employees of a target SME. Therefore, if the only privacy measure taken when releasing the dataset in <a href="#tbl-ex-sme-dataset" class="quarto-xref">Table&nbsp;<span>3.5</span></a> is to suppress the company name, a snooper knowing that company K&amp;K Sarl has about a dozen employees crammed in a small flat of about 50 m will still be able to use the released data to link company K&amp;K Sarl with turnover 645,223 Euros and net profit 333,010 Euros. <a href="#tbl-ex-three-anonym-sme" class="quarto-xref">Table&nbsp;<span>3.6</span></a> is a 3-anonymous version of the dataset in <a href="#tbl-ex-sme-dataset" class="quarto-xref">Table&nbsp;<span>3.5</span></a>. The identifier “Company name” was suppressed and optimal bivariate microaggregation with <span class="math inline">\(k = 3\)</span> was used on the key variables “Surface” and “No.&nbsp;employees” (in general, if there are <span class="math inline">\(p\)</span> key variables, multivariate microaggregation with dimension <span class="math inline">\(p\)</span> should be used to mask all of them). Both variables were standardized to have mean <span class="math inline">\(0\)</span> and variance <span class="math inline">\(1\)</span> before microaggregation, in order to give them equal weight, regardless of their scale. Due to the small size of the dataset, it was feasible to compute optimal microaggregation by exhaustive search. The information or variability loss incurred for those two variables in standardized form can be measured by the within-groups sum of squares. Dividing by the total sum of squares <span class="math inline">\(SST=22\)</span> —sum of squared Euclidean distances from all 11 pairs of standardized (surface, number of employees) to their average— yielded a variability loss measure <span class="math inline">\(SSE_{opt}/SST=0.34\)</span> bounded between 0 and 1.</p>
<p><em>It can be seen that the 11 records were microaggregated into three groups: one group with the 1st, 2nd, 3rd and 10th records (companies with large surface and many employees), a second group with the 4th, 5th and 9th records (companies with large surface and few employees) and a third group with the 6th, 7th, 8th and 11th records (companies with a small surface). Upon seeing <a href="#tbl-ex-three-anonym-sme" class="quarto-xref">Table&nbsp;<span>3.6</span></a>, a snooper knowing that company K&amp;K Sarl crams a dozen employees in a small flat hesitates between the four records in the third group. Therefore, since turnover and net profit are different for all records in the third group, the snooper cannot be sure about their values for K&amp;K Sarl.</em></p>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-note no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="tbl-ex-sme-dataset" class="quarto-float anchored">
<figure class="quarto-float quarto-float-tbl figure">
<div aria-describedby="tbl-ex-sme-dataset-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="table">
<colgroup>
<col style="width: 18%">
<col style="width: 17%">
<col style="width: 18%">
<col style="width: 21%">
<col style="width: 23%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: center;"><strong><em>Company name</em></strong></th>
<th style="text-align: right;"><strong><em>Surface (m2)</em></strong></th>
<th style="text-align: right;"><strong><em>No.&nbsp;employees</em></strong></th>
<th style="text-align: right;"><strong><em>Turnover (Euros)</em></strong></th>
<th style="text-align: right;"><strong><em>Net profit (Euros)</em></strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">A&amp;A Ltd</td>
<td style="text-align: right;">790</td>
<td style="text-align: right;">55</td>
<td style="text-align: right;">3,212,334</td>
<td style="text-align: right;">313,250</td>
</tr>
<tr class="even">
<td style="text-align: center;">B&amp;B SpA</td>
<td style="text-align: right;">710</td>
<td style="text-align: right;">44</td>
<td style="text-align: right;">2,283,340</td>
<td style="text-align: right;">299,876</td>
</tr>
<tr class="odd">
<td style="text-align: center;">C&amp;C Inc</td>
<td style="text-align: right;">730</td>
<td style="text-align: right;">32</td>
<td style="text-align: right;">1,989,233</td>
<td style="text-align: right;">200,213</td>
</tr>
<tr class="even">
<td style="text-align: center;">D&amp;D BV</td>
<td style="text-align: right;">810</td>
<td style="text-align: right;">17</td>
<td style="text-align: right;">984,983</td>
<td style="text-align: right;">143,211</td>
</tr>
<tr class="odd">
<td style="text-align: center;">E&amp;E SL</td>
<td style="text-align: right;">950</td>
<td style="text-align: right;">3</td>
<td style="text-align: right;">194,232</td>
<td style="text-align: right;">51,233</td>
</tr>
<tr class="even">
<td style="text-align: center;">F&amp;F GmbH</td>
<td style="text-align: right;">510</td>
<td style="text-align: right;">25</td>
<td style="text-align: right;">119,332</td>
<td style="text-align: right;">20,333</td>
</tr>
<tr class="odd">
<td style="text-align: center;">G&amp;G AG</td>
<td style="text-align: right;">400</td>
<td style="text-align: right;">45</td>
<td style="text-align: right;">3,012,444</td>
<td style="text-align: right;">501,233</td>
</tr>
<tr class="even">
<td style="text-align: center;">H&amp;H SA</td>
<td style="text-align: right;">330</td>
<td style="text-align: right;">50</td>
<td style="text-align: right;">4,233,312</td>
<td style="text-align: right;">777,882</td>
</tr>
<tr class="odd">
<td style="text-align: center;">I&amp;I LLC</td>
<td style="text-align: right;">510</td>
<td style="text-align: right;">5</td>
<td style="text-align: right;">159,999</td>
<td style="text-align: right;">60,388</td>
</tr>
<tr class="even">
<td style="text-align: center;">J&amp;J Co</td>
<td style="text-align: right;">760</td>
<td style="text-align: right;">52</td>
<td style="text-align: right;">5,333,442</td>
<td style="text-align: right;">1,001,233</td>
</tr>
<tr class="odd">
<td style="text-align: center;">K&amp;K Sarl</td>
<td style="text-align: right;">50</td>
<td style="text-align: right;">12</td>
<td style="text-align: right;">645,223</td>
<td style="text-align: right;">333,010</td>
</tr>
</tbody>
</table>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-tbl" id="tbl-ex-sme-dataset-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;3.5: Example - SME dataset. “Company name” is an identifier to be suppressed before publishing the dataset.
</figcaption>
</figure>
</div>
<div id="tbl-ex-three-anonym-sme" class="quarto-float anchored">
<figure class="quarto-float quarto-float-tbl figure">
<div aria-describedby="tbl-ex-three-anonym-sme-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="table">
<colgroup>
<col style="width: 21%">
<col style="width: 22%">
<col style="width: 26%">
<col style="width: 28%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: right;"><strong><em>Surface (m2)</em></strong></th>
<th style="text-align: right;"><strong><em>No.&nbsp;employees</em></strong></th>
<th style="text-align: right;"><strong><em>Turnover (Euros)</em></strong></th>
<th style="text-align: right;"><strong><em>Net profit (Euros)</em></strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: right;">747.5</td>
<td style="text-align: right;">46</td>
<td style="text-align: right;">3,212,334</td>
<td style="text-align: right;">313,250</td>
</tr>
<tr class="even">
<td style="text-align: right;">747.5</td>
<td style="text-align: right;">46</td>
<td style="text-align: right;">2,283,340</td>
<td style="text-align: right;">299,876</td>
</tr>
<tr class="odd">
<td style="text-align: right;">747.5</td>
<td style="text-align: right;">46</td>
<td style="text-align: right;">1,989,233</td>
<td style="text-align: right;">200,213</td>
</tr>
<tr class="even">
<td style="text-align: right;">756.67</td>
<td style="text-align: right;">8</td>
<td style="text-align: right;">984,983</td>
<td style="text-align: right;">143,211</td>
</tr>
<tr class="odd">
<td style="text-align: right;">756.67</td>
<td style="text-align: right;">8</td>
<td style="text-align: right;">194,232</td>
<td style="text-align: right;">51,233</td>
</tr>
<tr class="even">
<td style="text-align: right;">322.5</td>
<td style="text-align: right;">33</td>
<td style="text-align: right;">119,332</td>
<td style="text-align: right;">20,333</td>
</tr>
<tr class="odd">
<td style="text-align: right;">322.5</td>
<td style="text-align: right;">33</td>
<td style="text-align: right;">3,012,444</td>
<td style="text-align: right;">501,233</td>
</tr>
<tr class="even">
<td style="text-align: right;">322.5</td>
<td style="text-align: right;">33</td>
<td style="text-align: right;">4,233,312</td>
<td style="text-align: right;">777,882</td>
</tr>
<tr class="odd">
<td style="text-align: right;">756.67</td>
<td style="text-align: right;">8</td>
<td style="text-align: right;">159,999</td>
<td style="text-align: right;">60,388</td>
</tr>
<tr class="even">
<td style="text-align: right;">747.5</td>
<td style="text-align: right;">46</td>
<td style="text-align: right;">5,333,442</td>
<td style="text-align: right;">1,001,233</td>
</tr>
<tr class="odd">
<td style="text-align: right;">322.5</td>
<td style="text-align: right;">33</td>
<td style="text-align: right;">645,223</td>
<td style="text-align: right;">333,010</td>
</tr>
</tbody>
</table>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-tbl" id="tbl-ex-three-anonym-sme-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;3.6: Example - 3-anonymous version of the SME dataset after optimal microaggregation of key variables
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<section id="fixed-vs.-variable-group-size" class="level4" data-number="3.4.5.1">
<h4 data-number="3.4.5.1" class="anchored" data-anchor-id="fixed-vs.-variable-group-size"><span class="header-section-number">3.4.5.1</span> Fixed vs.&nbsp;variable group size</h4>
<p>Classical microaggregation algorithms (Defays and Nanopoulos, 1993) required that all groups except perhaps one be of size <span class="math inline">\(k\)</span>; allowing groups to be of size <span class="math inline">\(k\)</span> depending on the structure of data was termed <em>data-oriented microaggregation</em> (Mateo-Sanz and Domingo-Ferrer, 1999), (Domingo-Ferrer and Mateo-Sanz, 2002). <a href="#fig-fixed-vs-variable-sized-group" class="quarto-xref">Figure&nbsp;<span>3.1</span></a> illustrates the advantages of variable-sized groups. If classical fixed-size microaggregation with <span class="math inline">\(k = 3\)</span> is used, we obtain a partition of the data into three groups, which looks rather unnatural for the data distribution given. On the other hand, if variable-sized groups are allowed then the five data on the left can be kept in a single group and the four data on the right in another group; such a variable-size grouping yields more homogeneous groups, which implies lower information loss.</p>
<p>However, except for specific cases such as the one depicted in <a href="#fig-fixed-vs-variable-sized-group" class="quarto-xref">Figure&nbsp;<span>3.1</span></a>, the small gain in within-group homogeneity obtained with variable-sized groups hardly justifies the higher computational overhead of this option with respect to fixed-sized groups. This is particularly evident for multivariate data, as noted by&nbsp;Sande (2002).</p>
<div id="fig-fixed-vs-variable-sized-group" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-fixed-vs-variable-sized-group-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Images/media/image66.png" style="width:3.8in;height:2.4in" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-fixed-vs-variable-sized-group-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.1: Variable-sized groups versus fixed-sized groups
</figcaption>
</figure>
</div>
</section>
<section id="exact-optimal-vs.-heuristic-microaggregation" class="level4" data-number="3.4.5.2">
<h4 data-number="3.4.5.2" class="anchored" data-anchor-id="exact-optimal-vs.-heuristic-microaggregation"><span class="header-section-number">3.4.5.2</span> Exact optimal vs.&nbsp;heuristic microaggregation</h4>
<p>For <span class="math inline">\(p = 1\)</span>, <em>i.e.</em> a univariate dataset or a multivariate dataset where variables are microaggregated one at a time, an exact polynomial shortest-path algorithm exists to find the <span class="math inline">\(k\)</span>-partition that optimally solves the microaggregation problem&nbsp;(Hansen and Mukherjee, 2003). See its description in <a href="#sec-Hansen_Mukherjee" class="quarto-xref"><span>Section 3.4.5.3</span></a>.</p>
<p>For <span class="math inline">\(p &gt; 1\)</span>, finding an exact optimal solution to the microaggregation problem, <em>i.e.</em> finding a grouping where groups have maximal homogeneity and size at least <span class="math inline">\(k\)</span>, has been shown to be NP-hard (Oganian and Domingo-Ferrer, 2001).</p>
<p>Unfortunately, the univariate optimal algorithm by Hansen and Mukherjee (2003) is not very useful in practice and this for two reasons: i) microdata sets are normally multivariate and using univariate microaggregation to microaggregate them one variable at a time is not good in terms of disclosure risk (see Domingo-Ferrer et al., 2002); ii) although polynomial-time, the optimal algorithm is quite slow when the number of records is large.</p>
<p>Thus, practical methods in the literature are heuristic:</p>
<ul>
<li>Univariate methods deal with multivariate datasets by microaggregating one variable at a time, <em>i.e.</em> variables are sequentially and independently microaggregated. These heuristics are known as individual ranking (Defays and Nanopoulos, 1993). While they are fast and cause little information loss, these univariate heuristics have the same problem of high disclosure risk as univariate optimal microaggregation.</li>
<li>Multivariate methods either rank multivariate data by projecting them onto a single axis (<em>e.g.</em> using the first principal component or the sum of <span class="math inline">\(z\)</span>-scores (Defays and Nanopoulos, 1993) or directly deal with unprojected data (Mateo-Sanz and Domingo-Ferrer, 1999), (Domingo-Ferrer and Mateo-Sanz, 2002). When working on unprojected data, we can microaggregate all variables of the dataset at a time, or independently microaggregate groups of two variables at a time, three variables at a time, etc. In any case, it is preferable that variables within a group which is microaggregated at a time be correlated (W.E. Winkler, 2004) in order to keep as much as possible the analytic properties of the file.</li>
</ul>
<p>We next describe the two microaggregation algorithms implemented in <span class="math inline">\(\mu\)</span>‑ARGUS.</p>
</section>
<section id="sec-Hansen_Mukherjee" class="level4" data-number="3.4.5.3">
<h4 data-number="3.4.5.3" class="anchored" data-anchor-id="sec-Hansen_Mukherjee"><span class="header-section-number">3.4.5.3</span> Hansen-Mukherjee’s optimal univariate microaggregation</h4>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-12-contents" aria-controls="callout-12" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Expert level
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-12" class="callout-12-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>In Hansen and Mukherjee (2003) a polynomial-time algorithm was proposed for <em>univariate</em> optimal microaggregation. Authors formulate the microaggregation problem as a shortest-path problem on a graph. They first construct the graph and then show that the optimal microaggregation corresponds to the shortest path in this graph. Each arc of the graph corresponds to a possible group that may be part of an optimal partition. The arc label is the <span class="math inline">\(\text{SSE}\)</span> that would result if that group were to be included in the partition. We next detail the graph construction.</p>
<p>Let <span class="math inline">\(\mathbf{V} = \left\{ v_{1},\cdots,v_{n} \right\}\)</span> be a vector consisting of <span class="math inline">\(n\)</span> real numbers sorted into ascending order, so that <span class="math inline">\(v_{1}\)</span> is the smallest value and <span class="math inline">\(v_{n}\)</span> the largest value. Let <span class="math inline">\(k\)</span> be an integer group size such that <span class="math inline">\(1 \leq k &lt; n\)</span>. Now, a graph <span class="math inline">\(G_{n,k}\)</span> is constructed as follows:</p>
<ol type="1">
<li>For each value <span class="math inline">\(\mathbf{X}_{i}\)</span> in <span class="math inline">\(\mathbf{X}\)</span>, create a node with label <span class="math inline">\(i\)</span>. Create also an additional node with label 0.</li>
<li>For each pair of graph nodes <span class="math inline">\((i,j)\)</span> such that <span class="math inline">\(1 + k \leq j &lt; i + 2k\)</span>, create a directed arc <span class="math inline">\((i,j)\)</span>from node <span class="math inline">\(i\)</span> to node <span class="math inline">\(j\)</span>.</li>
<li>Map each arc <span class="math inline">\((i,j)\)</span> to the group of values <span class="math inline">\(C(i,j) = \left\{ \mathbf{X}_{h}:i &lt; h \leq j \right\}\)</span>. Let the length <span class="math inline">\(L(i,j)\)</span> of the arc be the within group sum of squares for <span class="math inline">\(C(i,j)\)</span>, that is, <span class="math display">\[
L(i,j) = \sum\limits_{h = i + 1}^{j}\left( \mathbf{X}_{h} - {\overline{\mathbf{X}}}_{(i,j)} \right)^{2}
\]</span> where <span class="math inline">\({\overline{\mathbf{X}}}_{(i,j)} = \frac{1}{j - i}\sum_{h=i+1}^{j}\mathbf{X}_{h}\)</span></li>
</ol>
<p>It is proven in&nbsp;Hansen and Mukherjee (2003) that the optimal <span class="math inline">\(k\)</span>-partition for <span class="math inline">\(V\)</span> is found by taking as groups the <span class="math inline">\(C(i,j)\)</span> corresponding to the arcs in the shortest path between nodes 0 and <span class="math inline">\(n\)</span>. For minimal group size <span class="math inline">\(k\)</span> and a dataset of <span class="math inline">\(n\)</span> real numbers sorted in ascending order, the complexity of this optimal univariate microaggregation is <span class="math inline">\(O\left( k^{2}n \right)\)</span>, that is, linear in the size of the dataset.</p>
</div>
</div>
</div>
</section>
<section id="the-mdav-heuristic-for-multivariate-microaggregation" class="level4" data-number="3.4.5.4">
<h4 data-number="3.4.5.4" class="anchored" data-anchor-id="the-mdav-heuristic-for-multivariate-microaggregation"><span class="header-section-number">3.4.5.4</span> The MDAV heuristic for multivariate microaggregation</h4>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-13-contents" aria-controls="callout-13" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Expert level
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-13" class="callout-13-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>The multivariate microaggregation heuristic implemented in <span class="math inline">\(\mu\)</span>‑ARGUS is called MDAV (Maximum Distance to Average Vector). MDAV performs multivariate fixed group size microaggregation on unprojected data. MDAV is also described in&nbsp;Domingo-Ferrer and Torra (2005).</p>
<p>MDAV Algorithm</p>
<ol type="1">
<li>Compute the average record <span class="math inline">\(\overline{x}\)</span> of all records in the dataset. Consider the most distant record <span class="math inline">\(x_{r}\)</span> to the average record <span class="math inline">\(\overline{x}\)</span> (using the squared Euclidean distance).</li>
<li>Find the most distant record <span class="math inline">\(x_{s}\)</span> from the record <span class="math inline">\(x_{r}\)</span> considered in the previous step.</li>
<li>Form two groups around <span class="math inline">\(x_{r}\)</span> and <span class="math inline">\(x_{s}\)</span>, respectively. One group contains <span class="math inline">\(x_{r}\)</span> and the <span class="math inline">\(k - 1\)</span> records closest to <span class="math inline">\(x_{r}\)</span>. The other group contains <span class="math inline">\(x_{s}\)</span> and the <span class="math inline">\(k - 1\)</span> records closest to <span class="math inline">\(x_{s}\)</span>.</li>
<li>If there are at least 3k records which do not belong to any of the two groups formed in Step&nbsp;3, go to Step&nbsp;1 taking as new dataset the previous dataset minus the groups formed in the last instance of Step&nbsp;3.</li>
<li>If there are between <span class="math inline">\(3k - 1\)</span> and <span class="math inline">\(2k\)</span> records which do not belong to any of the two groups formed in Step&nbsp;3:
<ol type="a">
<li>compute the average record <span class="math inline">\(\overline{x}\)</span> of the remaining records;</li>
<li>find the most distant record <span class="math inline">\(x_{r}\)</span> from <span class="math inline">\(\overline{x}\)</span>;</li>
<li>form a group containing <span class="math inline">\(x_{r}\)</span> and the <span class="math inline">\(k-1\)</span> records closest to <span class="math inline">\(x_{r}\)</span>;</li>
<li>form another group containing the rest of records. Exit the Algorithm.</li>
</ol></li>
<li>If there are less than <span class="math inline">\(2k\)</span> records which do not belong to the groups formed in Step 3, form a new group with those records and exit the Algorithm.</li>
</ol>
<p>The above algorithm can be applied independently to each group of variables resulting from partitioning the set of variables in the dataset.</p>
</div>
</div>
</div>
</section>
<section id="categorical-microaggregation" class="level4" data-number="3.4.5.5">
<h4 data-number="3.4.5.5" class="anchored" data-anchor-id="categorical-microaggregation"><span class="header-section-number">3.4.5.5</span> Categorical microaggregation</h4>
<p>Recently&nbsp;(Torra, 2004), microaggregation has been extended to categorical data. Such an extension is based on existing definitions for aggregation and clustering, the two basic operations required in microaggregation. Specifically, the median is used for aggregating ordinal data and the plurality rule (voting) for aggregating nominal data. Clustering of categorical data is based on the <span class="math inline">\(k\)</span>-modes algorithm, which is a partitive clustering method similar to <span class="math inline">\(c\)</span>-means.</p>
</section>
<section id="references-3" class="level4" data-number="3.4.5.6">
<h4 data-number="3.4.5.6" class="anchored" data-anchor-id="references-3"><span class="header-section-number">3.4.5.6</span> References</h4>
<p>Defays, D., and Nanopoulos, P. (1993<em>). Panels of enterprises and confidentiality: the small aggregates method</em>. In Proc. of 92 Symposium on Design and Analysis of Longitudinal Surveys, pages 195–204, Ottawa, 1993. Statistics Canada.</p>
<p>Domingo-Ferrer, J., and Mateo-Sanz, J. M. (2002). <em>Practical data-oriented microaggregation for statistical disclosure control</em>. IEEE Transactions on Knowledge and Data Engineering, 14(1):189–201, 2002.</p>
<p>Domingo-Ferrer, J., Mateo-Sanz, J. M., Oganian, A., and Torres, À. (2002). <em>On the security of microaggregation with individual ranking: analytical attacks</em>. International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems, 10(5):477–492, 2002.</p>
<p>Domingo-Ferrer, J., and Torra, V. (2005). <em>Ordinal, continuous and heterogenerous k-anonymity through microaggregation.</em> Data Mining and Knowledge Discovery, 11(2):195–212, 2005.</p>
<p>Hansen, S. L. and Mukherjee, S. (2003<em>). A polynomial algorithm for optimal univariate microaggregation</em>. IEEE Transactions on Knowledge and Data Engineering, 15(4):1043–1044, 2003.</p>
<p>Hundepool, A., Van de Wetering, A., Ramaswamy, R., Franconi, L., Capobianchi, A., De Wolf, P.P., Domingo-Ferrer, J., Torra, V., and Giessing, S. (2014). <em><span class="math inline">\(\mu\)</span>-ARGUS version 5.1 Software and User’s Manual</em>. Statistics Netherlands, Voorburg NL, 2014. <a href="https://research.cbs.nl/casc/Software/MUmanual5.1.3.pdf">https://research.cbs.nl/casc/Software/MUmanual5.1.3.pdf</a>.</p>
<p>Mateo-Sanz, J. M. and Domingo-Ferrer, J. (1999). <em>A method for data-oriented multivariate microaggregation</em>. In J. Domingo-Ferrer, editor, Statistical Data Protection, pages 89–99, Luxemburg, 1999. Office for Official Publications of the European Communities.</p>
<p>Oganian, A:, and Domingo-Ferrer, J. (2001). <em>On the complexity of optimal microaggregation for statistical disclosure control.</em> Statistical Journal of the United Nations Economic Comission for Europe, 18(4):345–354, 2001.</p>
<p>Samarati, P. (2001). <em>Protecting respondents’ identities in microdata release.</em> IEEE Transactions on Knowledge and Data Engineering, 13(6):1010–1027, 2001.</p>
<p>Samarati, P., and Sweeney, L. (1998). <em>Protecting privacy when disclosing information: k-anonymity and its enforcement through generalization and suppression</em>. Technical report, SRI International, 1998.</p>
<p>Sande, G. (2002). <em>Exact and approximate methods for data directed microaggregation in one or more dimensions</em>. International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems, 10(5):459–476, 2002.</p>
<p>Sweeney, L. (2002<em>). k-anonimity: A model for protecting privacy.</em> International Journal of Uncertainty, Fuzziness and Knowledge Based Systems, 10(5):557–570, 2002.</p>
<p>Torra, V. (2004). <em>Microaggregation for categorical variables: a median based approach</em>. In J. Domingo-Ferrer and V. Torra, editors, Privacy in Statistical Databases, volume 3050 of Lecture Notes in Computer Science, pages 162–174, Berlin Heidelberg, 2004. Springer.</p>
<p>Winkler, W. E. (2004). <em>Masking and re-identification methods for public-use microdata: overview and research problems.</em> In J. Domingo-Ferrer and V. Torra, editors, Privacy in Statistical Databases, volume 3050 of Lecture Notes in Computer Science, pages 231–246, Berlin Heidelberg, 2004. Springer.</p>
</section>
</section>
<section id="sec-PRAM" class="level3 page-columns page-full" data-number="3.4.6">
<h3 data-number="3.4.6" class="anchored" data-anchor-id="sec-PRAM"><span class="header-section-number">3.4.6</span> PRAM</h3>
<p>PRAM is a disclosure control technique that can be applied to categorical data. Basically, it is a form of intended misclassification, using a known and predetermined probability mechanism. Applying PRAM means that for each record in a microdata file, the score on one or more categorical variables is changed with a certain probability. This is done independently for each of the records. PRAM is thus a perturbative method. Since PRAM uses a probability mechanism, the disclosure risk is directly influenced by this method. An intruder can never be certain that a record she thinks she has identified is indeed the identified person: with a certain probability this has been a perturbed record.</p>
<p>Since the probability mechanism that is used when applying PRAM is known, characteristics of the (latent) true data can still be estimated from the perturbed data file. To that end, one can make use of correction methods similar to those used in case of misclassification and randomised response situations.</p>
<p>PRAM was used in 2001 UK Census to produce an end-user licence version of the Samples of Anonymised Records (SARs). See Gross(2004) for a full description.</p>
<section id="pram-the-method" class="level4 page-columns page-full" data-number="3.4.6.1">
<h4 data-number="3.4.6.1" class="anchored" data-anchor-id="pram-the-method"><span class="header-section-number">3.4.6.1</span> PRAM, the method</h4>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-14-contents" aria-controls="callout-14" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Expert level
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-14" class="callout-14-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>In this section a short theoretical description of PRAM is given. For a detailed description of the method, see <em>e.g.</em>, Gouweleeuw <em>et al.</em> (1998a and 1998b). For a discussion of several issues concerning the method and its consequences, see <em>e.g.</em>, De Wolf <em>et al.</em> (1998).</p>
<p>Let <span class="math inline">\(\xi\)</span> denote a categorical variable in the original file to which PRAM will be applied and let <span class="math inline">\(X\)</span> denote the same variable in the perturbed file. Moreover, assume that <span class="math inline">\(\xi\)</span>, and hence <span class="math inline">\(X\)</span> as well, has <span class="math inline">\(K\)</span> categories, labelled <span class="math inline">\(1,\ldots,K\)</span>. The probabilities that define PRAM are denoted as</p>
<p><span class="math display">\[
p_{\text{kl}} = \mathbb{P}(X = l \mid \xi = k)
\]</span></p>
<p><em>i.e.</em>, the probability that an original score <span class="math inline">\(\xi = k\)</span> is changed into the score <span class="math inline">\(X = l\)</span>. These so called transition probabilities are defined for all <span class="math inline">\(k, l = 1, ..., K\)</span>.<br>
Using these transition probabilities as entries of a <span class="math inline">\(K \times K\)</span> matrix, we obtain a Markov matrix that we will call the PRAM-matrix, denoted by <span class="math inline">\(\mathbf{P}\)</span>.</p>
<p>Applying PRAM now means that, given the score <span class="math inline">\(\xi = k\)</span> for record <span class="math inline">\(r\)</span>, the score <span class="math inline">\(X\)</span> for that record is drawn from the probability distribution <span class="math inline">\(p_{k1},\ldots,p_{kK}\)</span>. For each record in the original file, this procedure is performed independently of the other records.</p>
<p>To illustrate the ideas, suppose that the variable <span class="math inline">\(\xi\)</span> is gender, with scores <span class="math inline">\(\xi =1\)</span> if male and <span class="math inline">\(\xi = 2\)</span> if female. Applying PRAM with <span class="math inline">\(p_{11} = p_{22} = 0.9\)</span> on a microdata file with 110 males and 90 females, would yield a perturbed microdata file with <em>in expectation</em>, 108 males and 92 females. However, in expectation, 9 of these males were originally female, and similarly, 11 of the females were originally male.</p>
<p><em>Correcting analyses</em><br>
More generally, the effect of PRAM on one-dimensional frequency tables is that</p>
<p><span class="math display">\[
\mathbb{E}(T_{X} \mid \xi) = \mathbf{P}^t T_{\xi}
\]</span></p>
<p>where <span class="math inline">\(T_{\xi} = (T_{\xi}(1),\ldots,T_{\xi}(K))^T\)</span> denotes the frequency table according to the original microdata file and <span class="math inline">\(T_X\)</span> the frequency table according to the perturbed microdata file. A conditionally unbiased estimator of the frequency table in the original file is then given by</p>
<p><span class="math display">\[
{\hat{T}}_{\xi} = \left( \mathbf{P}^{- 1} \right)^t T_{X}
\]</span></p>
<p>This can be extended to two-dimensional frequency tables, by vectorizing such tables. The corresponding PRAM-matrix is then given by the Kronecker product of the PRAM-matrices of the individual dimensions.</p>
<p>Alternatively, one could use the two-dimensional frequency tables<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> <span class="math inline">\(T_{\xi\eta}\)</span> for the original data and <span class="math inline">\(T_{XY}\)</span> for the perturbed data directly in matrix notation:</p>
<p><span class="math display">\[
\hat{T}_{\xi\eta} = \left( \mathbf{P}_{X}^{- 1} \right)^t T_{XY}\mathbf{P}_{Y}^{- 1}
\]</span></p>
<p>where <span class="math inline">\(\mathbf{P}_{X}\)</span> denotes the PRAM-matrix corresponding to the categorical variable <span class="math inline">\(X\)</span> and <span class="math inline">\(\mathbf{P}_{Y}\)</span> denotes the PRAM-matrix corresponding to the categorical variable <span class="math inline">\(Y\)</span>.</p>
<p>For more information about correction methods for statistical analyses applied to data that have been protected with PRAM, we refer to <em>e.g.</em>, Gouweleeuw <em>et al.</em> (1998a) and Van den Hout (1999 and 2004).</p>
<p><em>Choice of PRAM-matrix</em><br>
The exact choice of transition probabilities influences both the amount of information loss as well as the amount of disclosure limitation. Moreover, in certain situations ‘illogical’ changes could occur, <em>e.g.</em> changing the gender of a female respondent with ovarian cancer to male. These kind of changes would attract the attention of a possible intruder which should be avoided.</p>
<p>It is thus important to choose the transition probabilities in an appropriate way. Illogical changes could be avoided by appointing a probability of 0 to the illogical scores. In the example given above, PRAM should not be applied to the variable gender individually, but to the crossing of the variables gender and diseases. In that case, each transition probability of changing a score into the score (male, ovarian cancer) should be set equal to 0.</p>
<p>The choice of the transition probabilities in relation to the disclosure limitation and the information loss is more delicate. An empirical study on these effects is given in De Wolf and Van Gelder (2004). A theoretical discussion on the possibility to choose the transition probabilities in an optimal way (in some sense) is given in Cator <em>et al.</em> (2005).</p>
</div>
</div>
</div>
<div class="no-row-height column-margin column-container"><p><sup>1</sup>&nbsp;When <span class="math inline">\(X\)</span> has <span class="math inline">\(K\)</span> categories and <span class="math inline">\(Y\)</span> has <span class="math inline">\(L\)</span> categories, the 2-dimensional frequency table <span class="math inline">\(T_{XY}\)</span> is a <span class="math inline">\(K\times L\)</span> matrix.</p></div></section>
<section id="when-to-use-pram" class="level4" data-number="3.4.6.2">
<h4 data-number="3.4.6.2" class="anchored" data-anchor-id="when-to-use-pram"><span class="header-section-number">3.4.6.2</span> When to use PRAM</h4>
<p>In certain situations methods like global recoding, local suppression and top-coding would yield too much loss of detail in order to produce a safe microdata file. In these circumstances, PRAM is an alternative. Using PRAM, the amount of detail is preserved whereas the level of disclosure control is achieved by introducing uncertainty in the scores on identifying variables.</p>
<p>However, in order to make adequate inferences on a microdata file to which PRAM has been applied, the statistician needs to include sophisticated changes to the standard methods. This demands a good knowledge of both PRAM and the statistical analysis that is to be applied.</p>
<p>In case a researcher is willing to make use of a remote execution facility, PRAM might be used to produce a microdata file with the same structure as the original microdata file, but with some kind of synthetic data. Such microdata files might be used as a ‘test’ microdata file on which a researcher can try her scripts before sending these scripts to the remote execution facility. Since the results of the script are not used directly, the amount of distortion of the original microdata file can be chosen to be quite large. That way a safe microdata file is produced that still exhibits the same structure (and amount of detail) as the original microdata file.</p>
<p>In other situations, PRAM might produce a microdata file that is safe and leaves certain statistical characteristics of that file (more or less) unchanged. In that case, a researcher might perform his research on that microdata file in order to get an idea on the eventually needed research strategy. Once that strategy has been determined, the researcher might come to an on-site facility in order to perform the analyses once more on the original microdata hence reducing the amount of time that she has to be at the on-site facility.</p>
</section>
<section id="references-on-pram" class="level4" data-number="3.4.6.3">
<h4 data-number="3.4.6.3" class="anchored" data-anchor-id="references-on-pram"><span class="header-section-number">3.4.6.3</span> References on PRAM</h4>
<p>Gross, B., Guiblin,Ph, and K. Merrett (2004), <em>Implementing the Post Randomisation method To the Individual Sample of Anonymised Records (SAR) from the 2001 Census</em>, Office for National Statistics.<br>
(<a href="https://doc.ukdataservice.ac.uk/doc/7208/mrdoc/pdf/7208_implementing_the_post_randomisation_method_to_the_individual_sar_from_the_2001_census.pdf">https://doc.ukdataservice.ac.uk/doc/7208/mrdoc/pdf/7208_implementing_the_post<wbr>_randomisation_method_to_the_individual_sar_from_the_2001_census.pdf</a>)</p>
<p>Cator, E., Hensbergen A. and Y. Rozenholc (2005), <em>Statistical Disclosure Control using PRAM</em>, Proceedings of the 48th European Study Group Mathematics with Industry, Delft, The Netherlands, 15-19 March 2004. Delft University Press, 2005, p.&nbsp;23 – 30.</p>
<p>Gouweleeuw, J.M., P. Kooiman, L.C.R.J. Willenborg and P.P. de Wolf (1998a), <em>Post Randomisation for Statistical Disclosure Control: Theory and Implementation</em>, Journal of Official Statistics, Vol. 14, 4, pp.&nbsp;463 – 478.</p>
<p>Gouweleeuw, J.M., P. Kooiman, L.C.R.J. Willenborg and P.P. de Wolf (1998b), <em>The post randomisation method for protecting microdata</em>, Qüestiió, Quaderns d’Estadística i Investigació Operativa, Vol. 22, 1, pp.&nbsp;145 – 156.</p>
<p>Van den Hout, A. (2000), <em>The analysis of data perturbed by PRAM</em>, Delft University Press, ISBN 90-407-2014-2.</p>
<p>Van den Hout, A. (2004), <em>Analyzing misclassified data: randomized response and post randomization</em>, Ph.D.&nbsp;thesis, Utrecht University.</p>
<p>De Wolf, P.P. and I. Van Gelder (2004), <em>An empirical evaluation of PRAM</em>, Discussion paper 04012, Statistics Netherlands. This paper can also be found on the CASC-Website (<a href="https://research.cbs.nl/casc/Related/discussion-paper-04012.pdf">https://research.cbs.nl/casc/Related/discussion-paper-04012.pdf</a>)</p>
<p>De Wolf, P.P., J.M. Gouweleeuw, P. Kooiman and L.C.R.J. Willenborg (1998), <em>Reflections on PRAM</em>, Proceedings of the conference “Statistical Data Protection”, March 25-27 1998, Lisbon, Portugal. This paper can also be found on the CASC-Website (<a href="https://research.cbs.nl/casc/Related/Sdp_98_2.pdf">https://research.cbs.nl/casc/Related/Sdp_98_2.pdf</a>)</p>
</section>
</section>
<section id="sec-synthetic-data" class="level3" data-number="3.4.7">
<h3 data-number="3.4.7" class="anchored" data-anchor-id="sec-synthetic-data"><span class="header-section-number">3.4.7</span> Synthetic microdata</h3>
<p>Publication of synthetic —<em>i.e.</em> simulated— data was proposed long ago as a way to guard against statistical disclosure. The idea is to randomly generate data with the constraint that certain statistics or internal relationships of the original dataset should be preserved.</p>
<p>We next review some approaches in the literature to synthetic data generation and then proceed to discuss the global pros and cons of using synthetic data.</p>
<section id="a-forerunner-data-distortion-by-probability-distribution" class="level4" data-number="3.4.7.1">
<h4 data-number="3.4.7.1" class="anchored" data-anchor-id="a-forerunner-data-distortion-by-probability-distribution"><span class="header-section-number">3.4.7.1</span> A forerunner: data distortion by probability distribution</h4>
<p>Data distortion by probability distribution was proposed in 1985&nbsp;(Liew, Choi and Liew, 1985) and is not usually included in the category of synthetic data generation methods. However, its operating principle is to obtain a protected dataset by randomly drawing from the underlying distribution of the original dataset. Thus, it can be regarded as a forerunner of synthetic methods.</p>
<p>This method is suitable for both categorical and continuous variables and consists of three steps:</p>
<ol type="1">
<li>Identify the density function underlying to each of the confidential variables in the dataset and estimate the parameters associated with that density function.</li>
<li>For each confidential variable, generate a protected series by randomly drawing from the estimated density function.</li>
<li>Map the confidential series to the protected series and publish the protected series instead of the confidential ones.</li>
</ol>
<p>In the identification and estimation stage, the original series of the confidential variable (<em>e.g.</em> salary) is screened to determine which of a set of predetermined density functions fits the data best. Goodness of fit can be tested by the Kolmogorov-Smirnov test. If several density functions are acceptable at a given significance level, selecting the one yielding the smallest value for the Kolmogorov-Smirnov statistics is recommended. If no density in the predetermined set fits the data, the frequency imposed distortion method can be used. With the latter method, the original series is divided into several intervals (somewhere between 8 and 20). The frequencies within the interval are counted for the original series, and become a guideline to generate the distorted series. By using a uniform random number generating subroutine, a distorted series is generated until its frequencies become the same as the frequencies of the original series. If the frequencies in some intervals overflow, they are simply discarded.</p>
<p>Once the best-fit density function has been selected, the generation stage feeds its estimated parameters to a random value generating routine to produce the distorted series.</p>
<p>Finally, the mapping and replacement stage is only needed if the distorted variables are to be used jointly with other non-distorted variables. Mapping consists of ranking the distorted series and the original series in the same order and replacing each element of the original series with the corresponding distorted element.</p>
<p>It must be stressed here that the approach described in&nbsp;(Liew, Choi and Liew, 1985) was for one variable at a time. One could imagine a generalization of the method using multivariate density functions. However such a generalization: i) is not trivial, because it requires multivariate ranking-mapping; and ii) can lead to very poor fitting.</p>
<div class="callout callout-style-simple callout-note">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-body-container">
<p><strong>Example</strong> A distribution fitting software&nbsp;(Crystal.Ball, 2004) has been used on the original (ranked) data set 186, 693, 830, 1177, 1219, 1428, 1902, 1903, 2496, 3406. Continuous distributions tried were normal, triangular, exponential, lognormal, Weibull, uniform, beta, gamma, logistic, Pareto and extreme value; discrete distributions tried were binomial, Poisson, geometric and hypergeometric. The software allowed for three fitting criteria to be used: Kolmogorov-Smirnov, <span class="math inline">\(\chi^{2}\)</span> and Anderson-Darling. According to the first criterion, the best fit happened for the extreme value distribution with modal and scale parameters 1105.78 and 732.43, respectively; the Kolmogorov statistic for this fit was 0.1138. Using the fitted distribution, the following (ranked) dataset was generated and used to replace the original one: 425.60, 660.97, 843.43, 855.76, 880.68, 895.73, 1086.25, 1102.57, 1485.37, 2035.34.</p>
</div>
</div>
</div>
</section>
<section id="synthetic-data-by-multiple-imputation" class="level4" data-number="3.4.7.2">
<h4 data-number="3.4.7.2" class="anchored" data-anchor-id="synthetic-data-by-multiple-imputation"><span class="header-section-number">3.4.7.2</span> Synthetic data by multiple imputation</h4>
<p>Rubin (1993) suggested creating an entirely synthetic dataset based on the original survey data and multiple imputations. Rubin’s proposal was more completely developed in Raghunathan, Reiter, and Rubin (2003). A simulation study of it was given in Reiter (2002). In Reiter (2005) inference on synthetic data is discussed and in Reiter (2005b) an application is given.</p>
<p>We next sketch the operation of the original proposal by Rubin. Consider an original microdata set <span class="math inline">\(X\)</span> of size <span class="math inline">\(n\)</span> records drawn from a much larger population of <span class="math inline">\(N\)</span> individuals, where there are background variables <span class="math inline">\(A\)</span>, non-confidential variables <span class="math inline">\(B\)</span> and confidential variables <span class="math inline">\(C\)</span>. Background variables are observed and available for all <span class="math inline">\(N\)</span> individuals in the population, whereas <span class="math inline">\(B\)</span> and <span class="math inline">\(C\)</span> are only available for the <span class="math inline">\(n\)</span> records in the sample <span class="math inline">\(X\)</span>. The first step is to construct from <span class="math inline">\(X\)</span> a multiply-imputed population of <span class="math inline">\(N\)</span> individuals. This population consists of the <span class="math inline">\(n\)</span> records in <span class="math inline">\(X\)</span> and <span class="math inline">\(M\)</span> (the number of multiple imputations, typically between 3 and 10) matrices of <span class="math inline">\((B,C)\)</span> data for the <span class="math inline">\(N - n\)</span> non-sampled individuals. The variability in the imputed values ensures, theoretically, that valid inferences can be obtained on the multiply-imputed population. A model for predicting <span class="math inline">\((B,C)\)</span> from <span class="math inline">\(A\)</span> is used to multiply-impute <span class="math inline">\((B,C)\)</span> in the population. The choice of the model is a nontrivial matter. Once the multiply-imputed population is available, a sample <span class="math inline">\(Z\)</span> of <span class="math inline">\(n'\)</span> records can be drawn from it whose structure looks like the one a sample of <span class="math inline">\(n'\)</span> records drawn from the original population. This can be done <span class="math inline">\(M\)</span> times to create <span class="math inline">\(M\)</span> replicates of <span class="math inline">\((B,C)\)</span> values. The results are <span class="math inline">\(M\)</span> multiply-imputed synthetic datasets. To make sure no original data are in the synthetic datasets, it is wise to draw the samples from the multiply-imputed population excluding the <span class="math inline">\(n\)</span> original records from it.</p>
</section>
<section id="synthetic-data-by-bootstrap" class="level4" data-number="3.4.7.3">
<h4 data-number="3.4.7.3" class="anchored" data-anchor-id="synthetic-data-by-bootstrap"><span class="header-section-number">3.4.7.3</span> Synthetic data by bootstrap</h4>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-15-contents" aria-controls="callout-15" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Expert level
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-15" class="callout-15-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Fienberg (1994) proposed generating synthetic microdata by using bootstrap methods. Later, in Fienberg, Makov and Steele (1998), this approach was used for categorical data.</p>
<p>The bootstrap approach bears some similarity to the data distortion by probability distribution and the multiple-imputation methods described above. Given an original microdata set <span class="math inline">\(X\)</span> with <span class="math inline">\(p\)</span> variables, the data protector computes its empirical <span class="math inline">\(p\)</span>-variate cumulative distribution function (c.d.f.) <span class="math inline">\(F\)</span>. Now, rather than distorting the original data to obtain masked data, the data protector alters (or “smoothes”) the c.d.f. <span class="math inline">\(F\)</span> to derive a similar c.d.f. <span class="math inline">\(F'\)</span>. Finally, <span class="math inline">\(F'\)</span> is sampled to obtain a synthetic microdata set <span class="math inline">\(Z\)</span>.</p>
</div>
</div>
</div>
</section>
<section id="synthetic-data-by-latin-hypercube-sampling" class="level4" data-number="3.4.7.4">
<h4 data-number="3.4.7.4" class="anchored" data-anchor-id="synthetic-data-by-latin-hypercube-sampling"><span class="header-section-number">3.4.7.4</span> Synthetic data by Latin Hypercube Sampling</h4>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-16-contents" aria-controls="callout-16" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Expert level
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-16" class="callout-16-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Latin Hypercube Sampling (LHS) appears in the literature as another method for generating multivariate synthetic datasets. In Huntington and Lyrintzis (1998), the LHS updated technique of Florian (1992) was improved, but the proposed scheme is still time-intensive even for a moderate number of records. In Dandekar, Cohen and Kirkendall (2002) LHS is used along with a rank correlation refinement to reproduce both the univariate (<em>i.e.</em> mean and covariance) and multivariate structure (in the sense of rank correlation) of the original dataset. In a nutshell, LHS-based methods rely on iterative refinement, are time-intensive and their running time does not only depend on the number of values to be reproduced, but on the starting values as well.</p>
</div>
</div>
</div>
</section>
<section id="sec-IPSO" class="level4" data-number="3.4.7.5">
<h4 data-number="3.4.7.5" class="anchored" data-anchor-id="sec-IPSO"><span class="header-section-number">3.4.7.5</span> Partially synthetic data by Cholesky decomposition</h4>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-17-contents" aria-controls="callout-17" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Expert level
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-17" class="callout-17-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Generating plausible synthetic values for all variables in a database may be difficult in practice. Thus, several authors have considered mixing actual and synthetic data.</p>
<p>In Burridge (2004) a family of methods known as IPSO (Information Preserving Statistical Obfuscation) is proposed for generation of partially synthetic data. It consists of three methods that are described next.</p>
<p><em>Method A: The basic IPSO procedure</em><br>
The basic form of IPSO will be called here Method A. Informally, suppose two sets of variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, where the former are the confidential outcome variables and the latter are quasi-identifier variables. Then <span class="math inline">\(X\)</span> are taken as independent and <span class="math inline">\(Y\)</span> as dependent variables. A multiple regression of <span class="math inline">\(Y\)</span> on <span class="math inline">\(X\)</span> is computed and fitted <span class="math inline">\(Y_{A}'\)</span> variables are computed. Finally, variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y_{A}'\)</span> are released in place of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>.</p>
<p>More formally, let <span class="math inline">\(y\)</span>and <span class="math inline">\(x\)</span> be two data matrices, with rows representing respondents and columns representing variables; the row vectors <span class="math inline">\(y_{i}\)</span> and <span class="math inline">\(x_{i}\)</span> will represent the data for the <span class="math inline">\(i\)</span>-th respondent, for <span class="math inline">\(i = 1,\cdots,n\)</span>. The column vector <span class="math inline">\(u_{j}\)</span> will represent the quasi-identifier variable <span class="math inline">\(j\)</span>, for <span class="math inline">\(j = 1,\cdots,p\)</span>; in other words, the <span class="math inline">\(u_{j}\)</span> are the columns of quasi-identifier matrix <span class="math inline">\(Y\)</span>. Conditionally on the specific values for confidential variables, quasi-identifier variables for different respondents are assumed to be independent. Conditional on the specific confidential variables <span class="math inline">\(x_{i}\)</span>, the quasi-identifier variables <span class="math inline">\(Y_{i}\)</span> are assumed to follow a multivariate normal distribution with covariance matrix <span class="math inline">\(\Sigma = \left\{ \sigma_{jk} \right\}\)</span> and a mean vector <span class="math inline">\(x_{i}B\)</span>, where <span class="math inline">\(B\)</span> is an <span class="math inline">\(m\times p\)</span> matrix with columns <span class="math inline">\(\beta_{j}\)</span>. Thus a separate univariate normal multiple regression model is assumed for each column of <span class="math inline">\(Y\)</span> with regression parameter equal to the corresponding column of <span class="math inline">\(B\)</span>, that is, <span class="math inline">\(U_{j} \sim  N\left( x\beta_{j},\sigma_{jj}I \right)\)</span>.</p>
<p>Let <span class="math inline">\(\hat{B}\)</span> and <span class="math inline">\(\hat{\Sigma}\)</span> be the maximum likelihood estimates of <span class="math inline">\(B\)</span> and <span class="math inline">\(\Sigma\)</span> derived from the complete dataset <span class="math inline">\((y,x)\)</span>. These estimates are a pair of sufficient statistics for the regression model. We denote in what follows the vectors of fitted values and residuals for <span class="math inline">\(u_{j}\)</span> as <span class="math inline">\({\hat{\mu}}_{j}\)</span> and <span class="math inline">\({\hat{r}}_{j}\)</span>, respectively. Thus, <span class="math inline">\(\hat{\mu}\)</span>, <span class="math inline">\(\hat{r}\)</span> and <span class="math inline">\(\hat{\Sigma}\)</span> will denote the matrices <span class="math inline">\(x\hat{B}\)</span>, <span class="math inline">\(y - x\hat{B}\)</span> and <span class="math inline">\(n^{- 1}{\hat{r}}^t\hat{r}\)</span>, respectively.</p>
<p>The output of IPSO Method A is <span class="math inline">\(y'_{A} = x\hat{B}\)</span>.</p>
<p><em>Method B: IPSO preserving <span class="math inline">\(\hat{B}\)</span></em><br>
If a user fits a multiple regression model to <span class="math inline">\(\left( y_{A}',x \right)\)</span>, she will get estimates <span class="math inline">\({\hat{B}}_{A}\)</span> and <span class="math inline">\({\hat{\Sigma}}_{A}\)</span> which, in general, are different from the estimates <span class="math inline">\(\hat{B}\)</span> and <span class="math inline">\(\hat{\Sigma}\)</span> obtained when fitting the model to the original data <span class="math inline">\((y,x)\)</span>.</p>
<p>IPSO Method B modifies <span class="math inline">\(y_{A}'\)</span> into <span class="math inline">\(y_{B}'\)</span> in such a way that the estimate <span class="math inline">\({\hat{B}}_{B}\)</span> obtained by multiple linear regression from <span class="math inline">\(\left( y_{B}',x \right)\)</span> satisfies <span class="math inline">\({\hat{B}}_{B} = \hat{B}\)</span>.</p>
<p>Suppose that <span class="math inline">\(\tilde{y}\)</span> is a new, artificial, set of quasi-identifier values. These can be any set of numbers initially, <em>e.g.</em> an i.i.d. normal random sample or a deterministically chosen set. For each component new residuals <span class="math inline">\({\tilde{r}}_{j}\)</span> are calculated by fitting the above multivariate multiple regression to the new “data” <span class="math inline">\(\tilde{y}\)</span>. Define</p>
<p><span class="math display">\[
y_{B}' = \hat{\mu} + \tilde{r} = x\hat{B} + \tilde{r}
\]</span></p>
<p>The following information preservation result holds for IPSO-B.</p>
<p><strong>Lemma 3.3.7.1.</strong> <em>Regardless of the initial choice <span class="math inline">\(\tilde{y}\)</span>, <span class="math inline">\(\left( y_{B}',x \right)\)</span> preserves the sufficient statistic <span class="math inline">\(\hat{B}\)</span>.</em></p>
<p><strong>Proof:</strong> We have that <span id="eq-y_B_prime"><span class="math display">\[
y_{B}' = x\hat{B} + \tilde{r} = x\hat{B} + \left( \tilde{y} - x\tilde{B} \right)
\tag{3.5}\]</span></span> where <span class="math inline">\(\hat{B}\)</span> is the MLE estimate of <span class="math inline">\(B\)</span> obtained from <span class="math inline">\(\left( \tilde{y},x \right)\)</span>. Now, the expressions of <span class="math inline">\(\hat{B}\)</span> and <span class="math inline">\(\tilde{B}\)</span> are, respectively, <span class="math display">\[
\hat{B} = \left( x^{t}x \right)^{- 1}x^{t}y
\]</span> and <span class="math display">\[
\tilde{B} = \left( x^{t}x \right)^{- 1}x^{t}\tilde{y}
\]</span> Analogously, the expression of the MLE estimate of <span class="math inline">\({\hat{B}}_{B}\)</span> obtained from <span class="math inline">\(\left( y_{B}',x \right)\)</span> is <span class="math display">\[
{\hat{B}}_{B} = \left( x^{t}x \right)^{- 1}x^{t}y_{B}'
\]</span> Substituting expression (<a href="#eq-y_B_prime" class="quarto-xref"><span>3.5</span></a>) for <span class="math inline">\(y_{B}'\)</span> in the equation above, we get <span class="math display">\[
{\hat{B}}_{B} = \left( x^{t}x \right)^{- 1}\left( x^{t}x \right)\hat{B} + \left( x^{t}x \right)^{- 1}x^{t}(\tilde{y} - x\tilde{B}) = \hat{B} + \tilde{B} - \tilde{B} = \hat{B}
\]</span><br>
<br>
<em>Method C: IPSO preserving <span class="math inline">\(\hat{B}\)</span> and <span class="math inline">\(\hat{\Sigma}\)</span></em><br>
A more ambitious goal is to come up with a data matrix <span class="math inline">\(y_{C}'\)</span> such that, when a multivariate multiple regression model is fitted to <span class="math inline">\(\left( y_{C}',x \right)\)</span>, <em>both</em> sufficient statistics <span class="math inline">\(\hat{B}\)</span> and <span class="math inline">\(\hat{\Sigma}\)</span> obtained on the original data <span class="math inline">\((y,x)\)</span> are preserved.</p>
<p>The algorithm proposed in Burridge (2004) to get <span class="math inline">\(y_{C}'\)</span> is as follows</p>
<ol type="1">
<li>Generate provisional new “data” <span class="math inline">\(\tilde{y}\)</span> (this will be an <span class="math inline">\(n\times p\)</span> matrix).</li>
<li>Calculate provisional new residuals <span class="math inline">\(\tilde{r}\)</span> by fitting the multiple regression model to each column of <span class="math inline">\(\tilde{y}\)</span>.</li>
<li>Define new residuals <span class="math inline">\({\tilde{r}}'\)</span> as a transformation of <span class="math inline">\(\tilde{r}\)</span> so that <span class="math inline">\({\tilde{r}}^t{\tilde{r}}' = n\hat{\Sigma}\)</span>. This is easily done as follows:
<ol type="a">
<li>Let <span class="math inline">\(L\)</span> and <span class="math inline">\(L_{O}\)</span> be the lower triangular matrices in the Cholesky factorizations <span class="math inline">\(n\hat{\Sigma} = LL^{t}\)</span> and <span class="math inline">\({\tilde{r}}^t\tilde{r} = L_{O}^{\strut}L_{O}^t\)</span>.</li>
<li>Define <span class="math inline">\({\tilde{r}}' = \tilde{r}\left(L_{O}^{-1}\right)^t L^t\)</span>. It is easily verified that <span class="math inline">\(({\tilde{r}}')^t {\tilde{r}}' = n\hat{\Sigma}\)</span>.</li>
</ol></li>
</ol>
<p>Information preservation in IPSO-C is as follows.</p>
<p>Define <span class="math display">\[
y_{C}' = x\hat{B} + {\tilde{r}}'
\]</span></p>
<p><strong>Lemma 3.3.7.2.</strong> <em><span class="math inline">\(\left( y_{C}',x \right)\)</span> preserves the sufficient statistics <span class="math inline">\(\hat{B}\)</span> and <span class="math inline">\(\hat{\Sigma}\)</span>.</em></p>
<p><strong>Proof:</strong> The expression of the MLE estimate of <span class="math inline">\(\hat{B}\)</span> obtained from <span class="math inline">\(\left( y_{C}',x \right)\)</span> is</p>
<p><span class="math display">\[\begin{align}
{\hat{B}}_{C} &amp;= \left( x^t x \right)^{- 1}x^t y_{C}' = \left( x^t x \right)^{- 1}x^t \left( x\hat{B} + {\tilde{r}}' \right) \\
&amp;= \hat{B} + \left( x^t x \right)^{- 1}x^t \tilde{r}L_{O}^t L^t = \hat{B} + \left( x^t x \right)^{- 1}x^t \left( \tilde{y} - x\tilde{B} \right)L_{O}^t L^t \\
&amp;= \hat{B} + \left( \tilde{B} - \tilde{B} \right)L_{O}^t L^t = \hat{B} \\
\end{align}\]</span></p>
<p>Using that <span class="math inline">\({\hat{B}}_{C} = \hat{B}\)</span>, the expression of the MLE estimate of <span class="math inline">\(\hat{\Sigma}\)</span> obtained from <span class="math inline">\(\left( y_{C}',x \right)\)</span> is</p>
<p><span class="math display">\[\begin{align}
{\hat{\Sigma}}_{C} &amp;= \frac{\left( y_{C}',x\hat{B} \right)^t \left( y_{C}',x\hat{B} \right)}{n}\\
&amp;= \frac{\left( x\hat{B} + {\tilde{r}}' - x\hat{B} \right)^t \left( x\hat{B} + {\tilde{r}}' - x\hat{B} \right)}{n} \\
&amp;= \frac{{\tilde{r}}^t {\tilde{r}}'}{n} \\
&amp;= \hat{\Sigma}
\end{align}\]</span></p>
<p>where in the last equality we have used the property required on <span class="math inline">\({\tilde{r}}'\)</span>.</p>
<p><em>Using IPSO to get entirely synthetic microdata</em><br>
In&nbsp;Mateo-Sanz, Martínez-Ballesté and Domingo-Ferrer (2004), a non-iterative method for generating entirely synthetic continuous microdata through Cholesky decomposition is proposed. This can be viewed as a special case of IPSO. In a single step of computation, the method exactly reproduces the means and the covariance matrix of the original dataset. The running time grows linearly with the number of records. Exact preservation of the original covariance matrix implies that variances and Pearson correlations are also exactly preserved in the synthetic dataset.</p>
<p>The idea of the method is as follows. A dataset <span class="math inline">\(X\)</span> is viewed as a <span class="math inline">\(n\times m\)</span> matrix, where rows are records and columns are variables. First, the covariance matrix <span class="math inline">\(C\)</span> of <span class="math inline">\(X\)</span> is computed (covariance is defined between variables, <em>i.e.</em> between columns). Then, a random <span class="math inline">\(n\times m\)</span> matrix <span class="math inline">\(A\)</span> is generated, whose covariance matrix is the identity matrix. Next, the Cholesky decomposition of <span class="math inline">\(C\)</span> is computed, <em>i.e.</em>, an upper triangular matrix <span class="math inline">\(U\)</span> is found such that <span class="math inline">\(C=U^t U\)</span>. Finally, the synthetic microdata set <span class="math inline">\(Z\)</span> is an <span class="math inline">\(n\times m\)</span> matrix <span class="math inline">\(Z = A U\)</span>.</p>
</div>
</div>
</div>
</section>
<section id="other-partially-synthetic-microdata-approaches" class="level4" data-number="3.4.7.6">
<h4 data-number="3.4.7.6" class="anchored" data-anchor-id="other-partially-synthetic-microdata-approaches"><span class="header-section-number">3.4.7.6</span> Other partially synthetic microdata approaches</h4>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-18-contents" aria-controls="callout-18" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Expert level
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-18" class="callout-18-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>The multiple imputation approach described in Rubin (1993) for creating entirely synthetic microdata can be extended for partially synthetic microdata. As a result, multiply-imputed, partially synthetic datasets are obtained that contain a mix of actual and imputed (synthetic) values. The idea is to multiply-impute confidential values and release non-confidential values without perturbation. This approach was first applied to protect the US Survey of Consumer Finances&nbsp;(Kennickell, 1999), (Kennickell, 1999b). In Abowd and Woodcock (2001) and Abowd and Woodcock (2004), this technique was adopted to protect longitudinal linked data, that is, microdata that contain observations from two or more related time periods (successive years, etc.). Methods for valid inference on this kind of partial synthetic data were developed in Reiter (2003) and a non-parametric method was presented in&nbsp;Reiter (2003b) to generate multiply-imputed, partially synthetic data.</p>
<p>Closely related to multiply imputed, partially synthetic microdata is model-based disclosure protection&nbsp;(Franconi and Stander, 2002), (Polettini, Franconi, and Stander, 2002). In this approach, a set of confidential continuous outcome variables is regressed on a disjoint set non-confidential variables; then the fitted values are released for the confidential variables instead of the original values.</p>
</div>
</div>
</div>
</section>
<section id="muralidhar-sarathy-hybrid-generator" class="level4" data-number="3.4.7.7">
<h4 data-number="3.4.7.7" class="anchored" data-anchor-id="muralidhar-sarathy-hybrid-generator"><span class="header-section-number">3.4.7.7</span> Muralidhar-Sarathy hybrid generator</h4>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-19-contents" aria-controls="callout-19" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Expert level
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-19" class="callout-19-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Hybrid data are a mixture of original data and synthetic data. Let <span class="math inline">\(V\)</span> an original data set whose attributes are numerical and fall into confidential attributes <span class="math inline">\(X (=X_1\dots X_L)\)</span> and non-confidential attributes <span class="math inline">\(Y (=Y_1\dots Y_M)\)</span>. Let <span class="math inline">\(V'\)</span> be a hybrid data set obtained from <span class="math inline">\(V\)</span>, whose attributes are <span class="math inline">\(X (=X'_1\dots X'_L)\)</span> (hybrid versions of <span class="math inline">\(X\)</span>) and <span class="math inline">\(Y\)</span>.</p>
<p>Muralidhar and Sarathy (2008) proposed a procedure (called MS in the sequel) for generating hybrid data as follows <span class="math display">\[
X'_j = \gamma + X_j\alpha^t + Y_j\beta^t + e_i, \quad j = 1, \dots, n
\]</span> MS can yield hybrid data preserving the means and covariances of original data. To that end, the following equalities must be satisfied:</p>
<p><span class="math display">\[\begin{align}
\beta^t &amp;= \Sigma_{YY}^{-1} \Sigma_{YX}^{\strut} (I-\alpha^t) \\
\gamma &amp;= (I-\alpha) \bar{X} - \beta \bar{Y} \\
\Sigma_{ee} &amp;= (\Sigma_{XX}^{\strut} - \Sigma_{XY}^{\strut}\Sigma_{YY}^{-1}\Sigma_{YX}^{\strut}) - \alpha (\Sigma_{XX}^{\strut} - \Sigma_{XY}^{\strut}\Sigma_{YY}^{-1}\Sigma_{YX}^{\strut}) \alpha^t
\end{align}\]</span></p>
<p>where <span class="math inline">\(I\)</span> is the identity matrix and <span class="math inline">\(\Sigma_{ee}\)</span> is the covariance matrix of the noise terms <span class="math inline">\(e\)</span>.</p>
<p>Thus, <span class="math inline">\(\alpha\)</span> completely specifies the procedure. The authors of MS admit that <span class="math inline">\(\alpha\)</span> must be selected carefully to ensure that <span class="math inline">\(\Sigma_{ee}\)</span> is positive semidefinite. They consider three options for specifying the <span class="math inline">\(\alpha\)</span> matrix:</p>
<ol type="1">
<li>Take <span class="math inline">\(\alpha\)</span> as a diagonal matrix with all values in the diagonal being equal. In this case, <span class="math inline">\(\Sigma_{ee}\)</span> is positive semidefinite and the value of the hybrid attribute <span class="math inline">\(X_i'\)</span> depends only on <span class="math inline">\(X_i\)</span>, but not on <span class="math inline">\(X_j\)</span> for <span class="math inline">\(j \neq i\)</span>. All confidential attributes <span class="math inline">\(X_i\)</span> are perturbed at the same level.</li>
<li>Take <span class="math inline">\(\alpha\)</span> as a diagonal matrix, with values in the diagonal being not all equal. In this case, <span class="math inline">\(X_i'\)</span> still depends only on <span class="math inline">\(X_i\)</span>, but not on <span class="math inline">\(X_j\)</span> for <span class="math inline">\(j \neq i\)</span>. The differences are that the confidential attributes are perturbed at different levels and there is no guarantee that <span class="math inline">\(\Sigma_{ee}\)</span> is positive semidefinite, so it may be necessary to try several values of <span class="math inline">\(\alpha\)</span> until positive semidefiniteness is achieved.</li>
<li>Taking <span class="math inline">\(\alpha\)</span> as a non-diagonal matrix does not guarantee positive semidefiniteness either and the authors of MS do not see any advantage in it, although it would be the only way to have <span class="math inline">\(X_i'\)</span> depend on several attributes among <span class="math inline">\((X_1 \dots X_L)\)</span>. With <span class="math inline">\(\mathbb{R}\)</span>-Microhybrid, the dependence of <span class="math inline">\(X_i'\)</span> on the original confidential attributes is the one provided by the underlying IPSO method.</li>
</ol>
</div>
</div>
</div>
</section>
<section id="microaggregation-based-hybrid-data" class="level4" data-number="3.4.7.8">
<h4 data-number="3.4.7.8" class="anchored" data-anchor-id="microaggregation-based-hybrid-data"><span class="header-section-number">3.4.7.8</span> Microaggregation-based hybrid data</h4>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-20-contents" aria-controls="callout-20" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Expert level
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-20" class="callout-20-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>In (Domingo-Ferrer and González-Nicolás, 2009) an alternative procedure to generate hybrid data based on microaggregation was proposed. Let <span class="math inline">\(V\)</span> be an original data set consisting of <span class="math inline">\(n\)</span> records. On input an integer parameter <span class="math inline">\(k \in \{1,\dots,n\}\)</span>, the procedure described in this section generates a hybrid data set <span class="math inline">\(V'\)</span>. The greater <span class="math inline">\(k\)</span>, the more synthetic is <span class="math inline">\(V'\)</span>. Extreme cases are: i) <span class="math inline">\(k\)</span> = 1, which yields <span class="math inline">\(V' = V\)</span> (the output data are exactly the original input data); and ii) <span class="math inline">\(k = n\)</span>, which yields a completely synthetic output data set <span class="math inline">\(V'\)</span>.</p>
<p>The procedure calls two algorithms:</p>
<ul>
<li>A generic synthetic data generator <span class="math inline">\(S(C,C', \text{parms})\)</span>, that is, an algorithm which, given an original data (sub)set <span class="math inline">\(C\)</span>, generates a synthetic data (sub)set <span class="math inline">\(C'\)</span> preserving the statistics or parameters or models of <span class="math inline">\(C\)</span> specified in <span class="math inline">\(\text{parms}\)</span>.</li>
<li>A microaggregation heuristic, which, on input of a set of <span class="math inline">\(n\)</span> records and parameter <span class="math inline">\(k\)</span>, partitions the set of records into clusters containing between <span class="math inline">\(k\)</span> and <span class="math inline">\(2k − 1\)</span> records. Cluster creation attempts to maximize intra-cluster homogeneity.</li>
</ul>
<p><strong>Procedure 1 (<em>Microhybrid</em> (<span class="math inline">\(V\)</span>,<span class="math inline">\(V'\)</span>, <span class="math inline">\(\text{parms}\)</span>, <span class="math inline">\(k\)</span>))</strong></p>
<ol type="1">
<li>Call microaggregation(<span class="math inline">\(V\)</span>, <span class="math inline">\(k\)</span>). Let <span class="math inline">\(C_1,\dots,C_k\)</span> for some <span class="math inline">\(k\)</span> be the resulting clusters of records.</li>
<li>For <span class="math inline">\(i = 1, \dots, k\)</span> call <span class="math inline">\(S(C_i,C_{i}', \text{parms})\)</span>.</li>
<li>Output a hybrid dataset <span class="math inline">\(V'\)</span> whose records are those in the clusters <span class="math inline">\(C_{1}',\dots,C_{k}'\)</span> .</li>
</ol>
<p>At Step 1 of procedure Microhybrid above, clusters containing between <span class="math inline">\(k\)</span> and <span class="math inline">\(2k −1\)</span> records are created. Then at Step 2, a synthetic version of each cluster is generated. At Step 3, the original records in each cluster are replaced by the records in the corresponding synthetic cluster (instead of replacing them with the average record of the cluster, as done in conventional microaggregation).</p>
<p>The Microhybrid procedure bears some resemblance to the condensation approach proposed by (Aggarwal and Yu, 2004); however, Microhybrid is more general because:</p>
<ul>
<li>It can be applied to any data type (condensation is designed for numerical data only);</li>
<li>Clusters do not need to be all of size <span class="math inline">\(k\)</span> (their sizes can vary between <span class="math inline">\(k\)</span> and <span class="math inline">\(2k − 1\)</span>);</li>
<li>Any synthetic data generator (chosen to preserve certain pre-selected statistics or models) can be used by Microhybrid;</li>
<li>Instead of using an ad hoc clustering heuristic like condensation, Microhybrid can use any of the best microaggregation heuristics cited above, which should yield higher within-cluster homogeneity and thus less information loss.</li>
</ul>
<p><em>Role of parameter <span class="math inline">\(k\)</span></em><br>
We justify here the role of parameter <span class="math inline">\(k\)</span> in Microhybrid:</p>
<ul>
<li>If <span class="math inline">\(k = 1\)</span>, and <span class="math inline">\(\text{parms}\)</span> include preserving the mean of each attribute in the original clusters, the output is the same original data set, because the procedure creates <span class="math inline">\(n\)</span> clusters (as many as the number of original records). With <span class="math inline">\(k = 1\)</span>, even variable-size heuristics will yield all clusters of size 1, because the maximum intra-cluster similarity is obtained when clusters consist all of a single record.</li>
<li>If <span class="math inline">\(k = n\)</span>, the output is a single synthetic cluster: the procedure is equivalent to calling the synthetic data generator <span class="math inline">\(S\)</span> once for the entire data set.</li>
<li>For intermediate values of <span class="math inline">\(k\)</span>, several clusters are obtained at Step 1, whose parameters <span class="math inline">\(\text{parms}\)</span> are preserved by the synthetic clusters generated at Step 2. As <span class="math inline">\(k\)</span> decreases, the number of clusters (whose parameters are preserved in the data output at Step 3) increases, which causes the output data to look more and more like the original data. Each cluster can be regarded as a constraint on the synthetic data generation: the more constraints, the less freedom there is for generating synthetic data, and the output resembles more the original data. This is why the output data can be called hybrid.</li>
</ul>
<p>It must be noted here that, depending on the synthetic generator used, there may be a lower bound for <span class="math inline">\(k\)</span> higher than 1. For example, if using IPSO (see <a href="#sec-IPSO" class="quarto-xref"><span>Section 3.4.7.5</span></a>) with <span class="math inline">\(|X|\)</span> confidential attributes and <span class="math inline">\(|Y|\)</span> non-confidential attributes, it turns out that <span class="math inline">\(k\)</span> must be at least <span class="math inline">\(2|X|+|Y|+1\)</span>; otherwise there are not enough degrees of freedom for the generator to work.</p>
<p>Note that the choice of parameter <span class="math inline">\(k\)</span> is more straightforward than the choice of <span class="math inline">\(\alpha\)</span> in the MS procedure above. Also, for the case of numerical microdata, Microhybrid can offer, in addition to mean and covariance exact preservation, approximate preservation of third-order and fourth-order moments, and also approximate preservation of all moments up to order four in randomly chosen subdomains of the dataset. Details are given in the above-referenced paper describing Microhybrid.</p>
</div>
</div>
</div>
</section>
<section id="other-hybrid-microdata-approaches" class="level4" data-number="3.4.7.9">
<h4 data-number="3.4.7.9" class="anchored" data-anchor-id="other-hybrid-microdata-approaches"><span class="header-section-number">3.4.7.9</span> Other hybrid microdata approaches</h4>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-21-contents" aria-controls="callout-21" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Expert level
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-21" class="callout-21-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>A different approach called hybrid masking was proposed in&nbsp;Dandekar, Domingo-Ferrer and Sebé (2002). The idea is to compute masked data as a combination of original and synthetic data. Such a combination allows better control than purely synthetic data over the individual characteristics of masked records. For hybrid masking to be feasible, a rule must be used to pair one original data record with one synthetic data record. An option suggested in&nbsp;Dandekar, Domingo-Ferrer and Sebé (2002) is to go through all original data records and pair each original record with the nearest synthetic record according to some distance. Once records have been paired,&nbsp;Dandekar, Domingo-Ferrer, and Sebé (2002) suggest two possible ways for combining one original record <span class="math inline">\(X\)</span> with one synthetic record <span class="math inline">\(X_{S}\)</span>: additive combination and multiplicative combination. Additive combination yields</p>
<p><span class="math display">\[
Z = \alpha X + (1 - \alpha)X_{S}
\]</span></p>
<p>and multiplicative combination yields</p>
<p><span class="math display">\[
Z = X^{\alpha} \cdot X_{s}^{(1 - \alpha)}
\]</span></p>
<p>where <span class="math inline">\(\alpha\)</span> is an input parameter in <span class="math inline">\([0,1]\)</span> and <span class="math inline">\(Z\)</span> is the hybrid record. The authors present empirical results comparing the hybrid approach with rank swapping and microaggregation masking (the synthetic component of hybrid data is generated using Latin Hypercube Sampling&nbsp;by Dandekar, Cohen, and Kirkendall, 2002).</p>
<p>Post-masking optimization is another approach to combining original and synthetic microdata is proposed in Sebé <em>et al.</em> (2002). The idea here is to first mask an original dataset using a masking method. Then a hill-climbing optimization heuristic is run which seeks to modify the masked data to preserve the first and second-order moments of the original dataset as much as possible without increasing the disclosure risk with respect to the initial masked data. The optimization heuristic can be modified to preserve higher-order moments, but this significantly increases computation. Also, the optimization heuristic can use take as initial dataset a random dataset instead of a masked dataset; in this case, the output dataset is purely synthetic.</p>
</div>
</div>
</div>
</section>
<section id="pros-and-cons-of-synthetic-and-hybrid-microdata" class="level4" data-number="3.4.7.10">
<h4 data-number="3.4.7.10" class="anchored" data-anchor-id="pros-and-cons-of-synthetic-and-hybrid-microdata"><span class="header-section-number">3.4.7.10</span> Pros and cons of synthetic and hybrid microdata</h4>
<p>Synthetic data are appealing in that, at a first glance, they seem to circumvent the re-identification problem: since published records are invented and do not derive from any original record, it might be concluded that no individual can complain from having been re-identified. At a closer look this advantage is less clear. If, by chance, a published synthetic record matches a particular citizen’s non-confidential variables (age, marital status, place of residence, etc.) and confidential variables (salary, mortgage, etc.), re-identification using the non-confidential variables is easy and that citizen may feel that his confidential variables have been unduly revealed. In that case, the citizen is unlikely to be happy with or even understand the explanation that the record was synthetically generated.</p>
<p>On the other hand, limited data utility is another problem of synthetic data. Only the statistical properties explicitly captured by the model used by the data protector are preserved. A logical question at this point is why not directly publish the statistics one wants to preserve rather than release a synthetic microdata set.</p>
<p>One possible justification for synthetic microdata would be if valid analyses could be obtained on a number of subdomains, <em>i.e.</em> similar results were obtained in a number of subsets of the original dataset and the corresponding subsets of the synthetic dataset. Partially synthetic or hybrid microdata are more likely to succeed in staying useful for subdomain analysis. However, when using partially synthetic or hybrid microdata, we lose the attractive feature of purely synthetic data that the number of records in the protected (synthetic) dataset is independent from the number of records in the original dataset.</p>
</section>
<section id="references-4" class="level4" data-number="3.4.7.11">
<h4 data-number="3.4.7.11" class="anchored" data-anchor-id="references-4"><span class="header-section-number">3.4.7.11</span> References</h4>
<p>Abowd, J. M., and Woodcock, S. D. (2001). <em>Disclosure limitation in longitudinal linked tables</em>. In P. Doyle, J. I. Lane, J. J. Theeuwes, and L. V. Zayatz, editors, Confidentiality, Disclosure and Data Access: Theory and Practical Applications for Statistical Agencies, pages 215–278, Amsterdam, 2001. North-Holland.</p>
<p>Abowd, J. M. and Woodcock, S. D. (2004). <em>Multiply-imputing confidential characteristics and file links in longitudinal linked data</em>. In J. Domingo-Ferrer and V. Torra, editors, Privacy in Statistical Databases, volume 3050 of LNCS, pages 290–297, Berlin Heidelberg, 2004. Springer.</p>
<p>Aggarwal, C. C., and Yu, P. S. (2004). <em>A condensation approach to privacy preserving data mining</em>. In E. Bertino, S. Christodoulakis, D. Plexousakis, V. Christophides, M. Koubarakis, K. Böhm, and E. Ferrari, editors, Advances in Database Technology - EDBT 2004, volume 2992 of Lecture Notes in Computer Science, pages 183–199, Berlin Heidelberg, 2004.</p>
<p>Burridge, J. (2004). <em>Information preserving statistical obfuscation.</em> Statistics and Computing, 13:321–327, 2003.</p>
<p>Crystal.Ball. <a href="http://www.aertia.com/en/productos.asp?pid=245">http://www.aertia.com/en/productos.asp?pid=245</a>.</p>
<p>Dandekar, R., Cohen, M., and Kirkendall, N. (2002). <em>Sensitive micro data protection using latin hypercube sampling technique.</em> In J. Domingo-Ferrer, editor, Inference Control in Statistical Databases, volume 2316 of LNCS, pages 245–253, Berlin Heidelberg, Springer.</p>
<p>Dandekar, R., Domingo-Ferrer, J., and Sebé, F. (2002). <em>LHS-based hybrid microdata vs.&nbsp;rank swapping and microaggregation for numeric microdata protection.</em> In J. Domingo-Ferrer, editor, Inference Control in Statistical Databases, volume 2316 of LNCS, pages 153–162, Berlin Heidelberg. Springer.</p>
<p>Domingo-Ferrer, J., and González-Nicolás, Ú. (2009). <em>Hybrid Microdata Using Microaggregation</em>. Manuscript.</p>
<p>Fienberg, S. E. (1994). <em>A radical proposal for the provision of micro-data samples and the preservation of confidentiality</em>. Technical Report 611, Carnegie Mellon University Department of Statistics.</p>
<p>Fienberg, S.E., Makov, U. E., and Steele, R. J. (1998). <em>Disclosure limitation using perturbation and related methods for categorical data</em>. Journal of Official Statistics, 14(4):485–502.</p>
<p>Florian, A. (1992). <em>An efficient sampling scheme: updated latin hypercube sampling</em>. Probabilistic Engineering Mechanics, 7(2):123–130.</p>
<p>Franconi, L., and Stander, J. (2002). <em>A model based method for disclosure limitation of business microdata</em>. Journal of the Royal Statistical Society D - Statistician, 51:1–11.</p>
<p>Huntington, D. E., and Lyrintzis, C. S. (1998). <em>Improvements to and limitations of latin hypercube sampling</em>. Probabilistic Engineering Mechanics, 13(4):245–253.</p>
<p>Kennickell, A. B. (1999). <em>Multiple imputation and disclosure control: the case of the 1995 survey of consumer finances.</em> In Record Linkage Techniques, pages 248–267, Washington DC, 1999. National Academy Press.</p>
<p>Kennickell, A. B. (1999b). <em>Multiple imputation and disclosure protection: the case of the 1995 survey of consumer finances.</em> In J. Domingo-Ferrer, editor, Statistical Data Protection, pages 248–267, Luxemburg, 1999. Office for Official Publications of the European Communities.</p>
<p>Liew, C. K., Choi, U. J., and Liew, C. J. (1985). <em>A data distortion by probability distribution.</em> ACM Transactions on Database Systems, 10:395–411, 1985.</p>
<p>Mateo-Sanz, J. M., Martínez-Ballesté, A., and Domingo-Ferrer, J. (2004). <em>Fast generation of accurate synthetic microdata.</em> In J. Domingo-Ferrer and V. Torra, editors, Privacy in Statistical Databases, volume 3050 of LNCS, pages 298–306, Berlin Heidelberg, Springer.</p>
<p>Muralidhar, K, and Sarathy, R, (2008). <em>Generating sufficiency-based nonsynthetic perturbed data</em>. Transactions on Data Privacy, 1(1):17–33, 2008. <a href="https://www.tdp.cat/issues/tdp.a005a08.pdf">https://www.tdp.cat/issues/tdp.a005a08.pdf</a>.</p>
<p>Polettini, S., Franconi, L., and Stander, J. (2002). <em>Model based disclosure protection.</em> In J. Domingo-Ferrer, editor, Inference Control in Statistical Databases, volume 2316 of LNCS, pages 83–96, Berlin Heidelberg. Springer.</p>
<p>Raghunathan, T. J., Reiter, J. P., and Rubin, D. (2003). <em>Multiple imputation for statistical disclosure limitation.</em> Journal of Official Statistics, 19(1):1–16.</p>
<p>Reiter, J. P. (2002). Satisfying disclosure restrictions with synthetic data sets. <em>Journal of Official Statistics</em>, 18(4):531–544.</p>
<p>Reiter, J. P. (2003). Inference for partially synthetic, public use microdata sets. <em>Survey Methodology</em>, 29:181–188.</p>
<p>Reiter, J. P. (2003b). <em>Using CART to generate partially synthetic public use microdata, 2003</em>. Duke University working paper.</p>
<p>Reiter, J. P. (2005). <em>Releasing multiply-imputed, synthetic public use microdata: An illustration and empirical study.</em> Journal of the Royal Statistical Society, Series A, 168:185–205.</p>
<p>Reiter, J. P. (2005b). <em>Significance tests for multi-component estimands from multiply-imputed, synthetic microdata</em>. Journal of Statistical Planning and Inference, 131(2):365–377.</p>
<p>Rubin, D. E. (1993). <em>Discussion of statistical disclosure limitation.</em> Journal of Official Statistics, 9(2):461–468.</p>
<p>Sebé, F., Domingo-Ferrer, J., Mateo-Sanz, J. M. and Torra, V. (2002). <em>Post-masking optimization of the tradeoff between information loss and disclosure risk in masked microdata sets.</em> In J. Domingo-Ferrer, editor, Inference Control in Statistical Databases, volume 2316 of LNCS, pages 163–171, Berlin Heidelberg, Springer.</p>
<!-- ## Information loss in microdata protection {#sec-informationloss-microdata} -->
</section>
</section>
</section>
<section id="sec-informationloss-microdata" class="level2 page-columns page-full" data-number="3.5">
<h2 data-number="3.5" class="anchored" data-anchor-id="sec-informationloss-microdata"><span class="header-section-number">3.5</span> Measurement of information loss</h2>
<section id="concepts-and-types-of-information-loss-and-its-measures" class="level3" data-number="3.5.1">
<h3 data-number="3.5.1" class="anchored" data-anchor-id="concepts-and-types-of-information-loss-and-its-measures"><span class="header-section-number">3.5.1</span> Concepts and types of information loss and its measures</h3>
<p>The application of SDC methods entails the loss of some information. It arises as a result <em>e.g.</em> from gaps occurring in data when non-perturbative SDC methods are used, or perturbations when perturbative SDC tools are used. Because of this loss the analytical worth of the disclosed data for the user decreases, which means there is a possibility that results of computations and analyses based on such data will be inadequate (<em>e.g.</em> the precision of estimation could be much worse).</p>
<p>A strict evaluation of information loss must be based on the data uses to be supported by the protected data. The greater the differences between the results obtained on original and protected data for those uses, the higher the loss of information. However, very often microdata protection cannot be performed in a data use specific manner, for the following reasons:</p>
<ul>
<li>Potential data uses are very diverse and it may be even hard to identify them all at the moment of data release by the data protector.</li>
<li>Even if all data uses can be identified, issuing several versions of the same original dataset so that the <span class="math inline">\(i\)</span>-th version has an information loss optimized for the <span class="math inline">\(i\)</span>-th data use may result in unexpected disclosure by combining the differently protected datasets.</li>
</ul>
<p>Since that data often must be protected with no specific data use in mind, generic information loss measures are desirable to guide the data protector in assessing how much harm is being inflicted to the data by a particular SDC technique.</p>
<p>Defining what a generic information loss measure is can be a tricky issue. Roughly speaking, it should capture the amount of information loss for a reasonable range of data uses. We will say there is little information loss if the protected dataset is analytically valid and interesting according to the following definitions by Winkler (1998):</p>
<ul>
<li>A protected microdata set is an <em>analytically valid</em> microdata set if it approximately preserves the following with respect to the original data (some conditions apply only to continuous variables):
<ul>
<li>Means and covariances on a small set of subdomains (subsets of records and/or variables)</li>
<li>Marginal values for a few tabulations of the data (the information loss in this approach concerns mainly tables created on the basis of microdata and therefore it will be discussed in <a href="04-magnitude-tabular-data.html">Chapter 4</a> and <a href="05-frequency-tables.html">Chapter 5</a>)</li>
<li>At least one distributional characteristic</li>
</ul></li>
<li>A microdata set is an <em>analytically interesting microdata set</em>, if six variables on important subdomains are provided that can be validly analyzed.<br>
More precise conditions of analytical validity and analytical interest cannot be stated without taking specific data uses into account. As imprecise as they may be, the above definitions suggest some possible measures:
<ul>
<li>Compare raw records in the original and the protected dataset. The more similar the SDC method to the identity function, the less the impact (but the higher the disclosure risk! ). This requires pairing records in the original dataset and records in the protected dataset. For masking methods, each record in the protected dataset is naturally paired to the record in the original dataset it originates from. For synthetic protected datasets, pairing is more artificial. Dandekar, Domingo-Ferrer and Sebé (2002) proposed to pair a synthetic record to the nearest original record according to some distance.</li>
<li>Compare some statistics computed on the original and the protected datasets. The above definitions list some statistics which should be preserved as much as possible by an SDC method.</li>
</ul></li>
</ul>
<p>Taking the aforementioned premises into account, for microdata the information loss can concern the differences in distributions, in diversification and in shape and power of connections between various features. Therefore, the following types of measures of information loss are distinguished:</p>
<ol type="1">
<li>Measures of distribution disturbance – measures based on distances between original and perturbed values of variables (<em>e.g.</em> mean, mean of relative distances, complex distances, etc.),</li>
<li>Measures of impact on variance of estimation – computed using distances between variances for averages of continuous variables before and after SDC or multi-factor ANOVA for a selected dependent variable in relation to selected independent categorical variables (in this case, the measure of information loss involves a comparison of components of coefficients of determination <span class="math inline">\(R^2\)</span> - in terms of within-group and inter-group variance - for relevant models based on original and perturbed values (cf.&nbsp;Hundepool et al.&nbsp;(2012))),</li>
<li>Measures of impact on the intensity of connections – comparisons of measures of direction and intensity of connections between original continuous variables and between relevant perturbed ones; such measures can be <em>e.g.</em> correlation coefficients or test of independence.</li>
</ol>
</section>
<section id="information-loss-measures-for-categorical-data" class="level3" data-number="3.5.2">
<h3 data-number="3.5.2" class="anchored" data-anchor-id="information-loss-measures-for-categorical-data"><span class="header-section-number">3.5.2</span> Information loss measures for categorical data</h3>
<p>Straightforward computation of measures based on basic arithmetic operations like addition, subtraction, multiplication and division on categorical data is not possible. Neither is the use of most descriptive statistics like Euclidean distance, mean variance, correlation, etc. The following alternatives are considered in Domingo-Ferrer and Torra (2001):</p>
<ul>
<li>Direct comparison of categorical values</li>
<li>Comparison of contingency tables</li>
<li>Entropy-based measures</li>
</ul>
<p>Below we will describe examples for each of such types of measures.</p>
<section id="sec-direct-comparison" class="level4" data-number="3.5.2.1">
<h4 data-number="3.5.2.1" class="anchored" data-anchor-id="sec-direct-comparison"><span class="header-section-number">3.5.2.1</span> Direct comparison of categorical values</h4>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-22-contents" aria-controls="callout-22" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Expert level
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-22" class="callout-22-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Comparison of matrices <span class="math inline">\(X\)</span> and <span class="math inline">\(X^{\prime}\)</span> for categorical data requires the definition of a distance for categorical variables. Definitions consider only the distances between pairs of categories that can appear when comparing an original record and its protected version (see discussion above on pairing original and protected records).</p>
<p>For a nominal variable <span class="math inline">\(V\)</span> (a categorical variable taking values over an unordered set), the only permitted operation is comparison for equality. This leads to the following distance definition: <span class="math display">\[
d_V(c,c')=\begin{cases}
0, &amp; \text{if } c=c' \\
1, &amp; \text{if } c \neq c'
\end{cases}
\]</span> where <span class="math inline">\(c\)</span> is a category in an original record and <span class="math inline">\(c'\)</span> is the category which has replaced <span class="math inline">\(c\)</span> in the corresponding protected record.</p>
<p>For an ordinal variable <span class="math inline">\(V\)</span> (a categorical variable taking values over a totally ordered set), let <span class="math inline">\(\leq V\)</span> be the total order operator over the range <span class="math inline">\(D(V)\)</span> of <span class="math inline">\(V\)</span>. Define the distance between categories <span class="math inline">\(c\)</span> and <span class="math inline">\(c^{\prime}\)</span> as the number of categories between the minimum and the maximum of <span class="math inline">\(c\)</span> and <span class="math inline">\(c^{\prime}\)</span> divided by the cardinality of the range:</p>
<p><span id="eq-dist_categ"><span class="math display">\[
\text{dc}\left(c,c^{\prime}\right)=\frac{\left|c^{\prime\prime}\text{:min}\left(c,c^{\prime}\right)\leq c^{\prime\prime}&lt;\text{max}\left(c,c^{\prime}\right)\right|}{\left|D(V)\right|}
\tag{3.6}\]</span></span></p>
</div>
</div>
</div>
</section>
<section id="sec-contingency-table" class="level4" data-number="3.5.2.2">
<h4 data-number="3.5.2.2" class="anchored" data-anchor-id="sec-contingency-table"><span class="header-section-number">3.5.2.2</span> Comparison of contingency tables</h4>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-23-contents" aria-controls="callout-23" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Expert level
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-23" class="callout-23-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>An alternative to directly comparing the values of categorical variables is to compare their contingency tables. Given two datasets <span class="math inline">\(F\)</span> and <span class="math inline">\(G\)</span> (the original and the protected set, respectively) and their corresponding <span class="math inline">\(t\)</span>-dimensional contingency tables for <span class="math inline">\(t \leq K\)</span>, we can define a contingency table-based information loss measure <span class="math inline">\(CTBIL\)</span> for a subset <span class="math inline">\(W\)</span> of variables as follows: <span id="eq-CTBIL"><span class="math display">\[
CTBIL(F,G;W,K)=\sum_{\{V_{ji}\cdots V_{jt}\} f\subseteq W\atop|\{V_{ji}\cdots V_{jt}\}|\leq K}\sum_{i_1\cdots i_t}|x^F_{i_1\cdots i_t}-x^G_{i_1\cdots i_t} |
\tag{3.7}\]</span></span> where <span class="math inline">\(x_{\text{subscripts}}^{\text{file}}\)</span> is the entry of the contingency table of <span class="math inline">\(\text{file}\)</span> at position given by <span class="math inline">\(\text{subscripts}\)</span>.</p>
<p>Because the number of contingency tables to be considered depends on the number of variables <span class="math inline">\(|W|\)</span>, the number of categories for each variable, and the dimension <span class="math inline">\(K\)</span>, a normalized version of (<a href="#eq-CTBIL" class="quarto-xref"><span>3.7</span></a>) may be desirable. This can be obtained by dividing expression (<a href="#eq-CTBIL" class="quarto-xref"><span>3.7</span></a>) by the total number of cells in all considered tables.</p>
<p>Distance between contingency tables generalizes some of the information loss measures used in the literature. For example, the <span class="math inline">\(\mu\)</span>‑ARGUS software (see <em>e.g.</em> Hundepool et al., 2014) measures information loss for local suppression by counting the number of suppressions. The distance between two contingency tables of dimension one returns twice the number of suppressions. This is because, when category <span class="math inline">\(A\)</span> is suppressed for one record, two entries of the contingency table are changed: the count of records with category <span class="math inline">\(A\)</span> decreases and the count of records with the “missing” category increases.</p>
</div>
</div>
</div>
</section>
<section id="entropy-based-measures" class="level4" data-number="3.5.2.3">
<h4 data-number="3.5.2.3" class="anchored" data-anchor-id="entropy-based-measures"><span class="header-section-number">3.5.2.3</span> Entropy-based measures</h4>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-24-contents" aria-controls="callout-24" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Expert level
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-24" class="callout-24-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>In De Waal and Willenborg (1999), Kooiman, Willenborg and Gouweleeuw (1998) and Willenborg and De Waal (2001), the use of Shannon’s entropy to measure information loss is discussed for the following methods: local suppression, global recoding and PRAM. Entropy is an information-theoretic measure, but can be used in SDC if the protection process is modelled as the noise that would be added to the original dataset in the event of it being transmitted over a noisy channel.</p>
<p>As noted earlier, PRAM is a method that generalizes noise addition, suppression and recoding methods. Therefore, our description of the use of entropy will be limited to PRAM.</p>
<p>Let <span class="math inline">\(V\)</span> be a variable in the original dataset and <span class="math inline">\(V'\)</span> be the corresponding variable in the PRAM-protected dataset. Let <span class="math inline">\(\mathbf{P}_{V,V'} = \left\{\mathbb{P}\left( V' = j \mid V = i \right) \right\}\)</span> be the PRAM Markov matrix. Then, the conditional uncertainty of <span class="math inline">\(V\)</span> given that <span class="math inline">\(V' = j\)</span> is: <span id="eq-conditional-uncertainty"><span class="math display">\[
H\left( V \mid V' = j \right) = - \sum\limits_{i = 1}^{n}\mathbb{P}\left( V = i \mid V' = j \right)\log \mathbb{P}\left( V = i \mid V' = j \right)
\tag{3.8}\]</span></span></p>
<p>The probabilities in (<a href="#eq-conditional-uncertainty" class="quarto-xref"><span>3.8</span></a>) can be derived from <span class="math inline">\(\mathbf{P}_{V,V'}\)</span> using Bayes’ formula. Finally, the entropy-based information loss measure <span class="math inline">\(EBIL\)</span> is obtained by accumulating expression (<a href="#eq-conditional-uncertainty" class="quarto-xref"><span>3.8</span></a>) for all individuals <span class="math inline">\(r\)</span> in the protected dataset <span class="math inline">\(G\)</span> <span class="math display">\[
EBIL\left( \mathbf{P}_{V,V'},G \right) = \sum\limits_{r \in G}^{}{H\left( V \mid V' = j_{r} \right)}
\]</span> where <span class="math inline">\(j_{r}\)</span> is the value taken by <span class="math inline">\(V'\)</span> in record <span class="math inline">\(r\)</span>.</p>
<p>The above measure can be generalized for multivariate datasets if <span class="math inline">\(V\)</span> and <span class="math inline">\(V^{\prime}\)</span> are taken as being multidimensional variables (<em>i.e.</em> representing several one-dimensional variables).</p>
<p>While using entropy to measure information loss is attractive from a theoretical point of view, its interpretation in terms of data utility loss is less obvious than for the previously discussed measures.</p>
</div>
</div>
</div>
</section>
</section>
<section id="information-loss-measures-for-continuous-data" class="level3" data-number="3.5.3">
<h3 data-number="3.5.3" class="anchored" data-anchor-id="information-loss-measures-for-continuous-data"><span class="header-section-number">3.5.3</span> Information loss measures for continuous data</h3>
<p>Assume a microdata set with <span class="math inline">\(n\)</span> individuals (records) <span class="math inline">\(I_{1},I_{2},\cdots,I_{n}\)</span> and <span class="math inline">\(p\)</span> continuous variables <span class="math inline">\(Z_{1},Z_{2},\cdots,Z_{p}\)</span>. Let <span class="math inline">\(X\)</span> be the matrix representing the original microdata set (rows are records and columns are variables). Let <span class="math inline">\(X^{'}\)</span> be the matrix representing the protected microdata set. The following tools are useful to characterize the information contained in the dataset:</p>
<ul>
<li>Covariance matrices <span class="math inline">\(V\)</span> (on <span class="math inline">\(X\)</span>) and <span class="math inline">\(V^{'}\)</span> (on <span class="math inline">\(X^{'}\)</span>).</li>
<li>Correlation matrices <span class="math inline">\(R\)</span> and <span class="math inline">\(R^{'}\)</span>.</li>
<li>Correlation matrices <span class="math inline">\(RF\)</span> and <span class="math inline">\({RF}^{'}\)</span> between the <span class="math inline">\(p\)</span> variables and the <span class="math inline">\(p\)</span> factors principal components <span class="math inline">\({PC}_{1},{PC}_{2},\cdots,{PC}_{p}\)</span> obtained through principal components analysis.</li>
<li>Communality between each of the <span class="math inline">\(p\)</span> variables and the first principal component <span class="math inline">\({PC}_{1}\)</span> (or other principal components <span class="math inline">\({PC}_{i}\)</span>’s). Communality is the percent of each variable that is explained by <span class="math inline">\({PC}_{1}\)</span> (or <span class="math inline">\({PC}_{i}\)</span>). Let <span class="math inline">\(C\)</span> be the vector of communalities for <span class="math inline">\(X\)</span> and <span class="math inline">\(C^{'}\)</span> the corresponding vector for <span class="math inline">\(X^{'}\)</span>.</li>
<li>Matrices <span class="math inline">\(F\)</span> and <span class="math inline">\(F^{'}\)</span>containing the loadings of each variable in <span class="math inline">\(X\)</span> on each principal component. The <span class="math inline">\(i\)</span>-th variable in <span class="math inline">\(X\)</span> can be expressed as a linear combination of the principal components plus a residual variation, where the <span class="math inline">\(j\)</span>-th principal component is multiplied by the loading in <span class="math inline">\(F\)</span> relating the <span class="math inline">\(i\)</span>-th variable and the <span class="math inline">\(j\)</span>-th principal component (Chatfield and Collins, 1980). <span class="math inline">\(F^{'}\)</span>is the corresponding matrix for <span class="math inline">\(X^{'}\)</span>.</li>
</ul>
<p>There does not seem to be a single quantitative measure which completely reflects those structural differences. Therefore, we proposed in Domingo-Ferrer, Mateo-Sanz, and Torra (2001) and Domingo-Ferrer and Torra (2001) to measure information loss through the discrepancies between matrices <span class="math inline">\(X\)</span>, <span class="math inline">\(V\)</span>, <span class="math inline">\(R\)</span>, <span class="math inline">\({RF}\)</span>, <span class="math inline">\(C\)</span> and <span class="math inline">\(F\)</span> obtained on the original data and the corresponding <span class="math inline">\(X^{'}\)</span>, <span class="math inline">\(V^{'}\)</span>, <span class="math inline">\(R^{'}\)</span>, <span class="math inline">\({RF}^{'}\)</span>, <span class="math inline">\(C^{'}\)</span> and <span class="math inline">\(F^{'}\)</span> obtained on the protected dataset. In particular, discrepancy between correlations is related to the information loss for data uses such as regressions and cross tabulations.</p>
<p>Matrix discrepancy can be measured in at least three ways:</p>
<p><strong>Mean square error</strong> Sum of squared componentwise differences between pairs of matrices, divided by the number of cells in either matrix.</p>
<p><strong>Mean absolute error</strong> Sum of absolute componentwise differences between pairs of matrices, divided by the number of cells in either matrix.</p>
<p><strong>Mean variation</strong> Sum of absolute percent variation of components in the matrix computed on protected data with respect to components in the matrix computed on original data, divided by the number of cells in either matrix. This approach has the advantage of not being affected by scale changes of variables.</p>
<p><a href="#tbl-loss-information" class="quarto-xref">Table&nbsp;<span>3.7</span></a> summarizes the measures proposed in Domingo-Ferrer, Mateo-Sanz and Torra (2001) and Domingo-Ferrer and V. Torra (2001). In this table, <span class="math inline">\(p\)</span> is the number of variables, <span class="math inline">\(n\)</span> the number of records, and components of matrices are represented by the corresponding lowercase letters (<em>e.g.</em> <span class="math inline">\(x_{\text{ij}}\)</span> is a component of matrix <span class="math inline">\(X\)</span>). Regarding <span class="math inline">\(X - X^{'}\)</span> measures, it makes also sense to compute those on the averages of variables rather than on all data (call this variant <span class="math inline">\(\overline{X^{\phantom{'}}} - \overline{X^{'}}\)</span>). Similarly, for <span class="math inline">\(V - V^{'}\)</span>measures, it would also be sensible to use them to compare only the variances of the variables, <em>i.e.</em> to compare the diagonals of the covariance matrices rather than the whole matrices (call this variant <span class="math inline">\(S - S^{'}\)</span>).</p>
<div id="tbl-loss-information" class="quarto-float anchored">
<figure class="quarto-float quarto-float-tbl figure">
<div aria-describedby="tbl-loss-information-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="table">
<colgroup>
<col style="width: 5%">
<col style="width: 29%">
<col style="width: 29%">
<col style="width: 35%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: center;"></th>
<th style="text-align: center;">Mean square error</th>
<th style="text-align: center;">Mean abs. error</th>
<th style="text-align: center;">Mean variation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><span class="math inline">\(X-X'\)</span></td>
<td style="text-align: center;"><span class="math inline">\(\frac{\sum\limits_{j=1}^{p}\sum\limits_{i=1}^{n}(x_{ij} - x_{ij}')^2}{np}\)</span></td>
<td style="text-align: center;"><span class="math inline">\(\frac{\sum\limits_{j=1}^{p}\sum\limits_{i=1}^{n}|x_{ij} - x_{ij}'|}{np}\)</span></td>
<td style="text-align: center;"><span class="math inline">\(\frac{\sum\limits_{j=1}^{p}\sum\limits_{i=1}^{n}\frac{|x_{ij} - x_{ij}'|}{|x_{ij}|}}{np}\)</span></td>
</tr>
<tr class="even">
<td style="text-align: center;"><span class="math inline">\(V-V'\)</span></td>
<td style="text-align: center;"><span class="math inline">\(\frac{\sum\limits_{j=1}^{p}\sum\limits_{1 \leq i \leq j}(v_{ij} - v_{ij}')^2}{p(p+1)/2}\)</span></td>
<td style="text-align: center;"><span class="math inline">\(\frac{\sum\limits_{j=1}^{p}\sum\limits_{1 \leq i \leq j}|v_{ij} - v_{ij}'|}{p(p+1)/2}\)</span></td>
<td style="text-align: center;"><span class="math inline">\(\frac{\sum\limits_{j=1}^{p}\sum\limits_{1 \leq i \leq j}\frac{|v_{ij} - v_{ij}'|}{|v_{ij}|}}{p(p+1)/2}\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: center;"><span class="math inline">\(R-R'\)</span></td>
<td style="text-align: center;"><span class="math inline">\(\frac{\sum\limits_{j=1}^{p}\sum\limits_{1 \leq i &lt; j}(r_{ij} - r_{ij}')^2}{p(p-1)/2}\)</span></td>
<td style="text-align: center;"><span class="math inline">\(\frac{\sum\limits_{j=1}^{p}\sum\limits_{1 \leq i &lt; j}|r_{ij} - r_{ij}'|}{p(p-1)/2}\)</span></td>
<td style="text-align: center;"><span class="math inline">\(\frac{\sum\limits_{j=1}^{p}\sum\limits_{1 \leq i &lt; j}\frac{|r_{ij} - r_{ij}'|}{|r_{ij}|}}{p(p-1)/2}\)</span></td>
</tr>
<tr class="even">
<td style="text-align: center;"><span class="math inline">\(RF-RF'\)</span></td>
<td style="text-align: center;"><span class="math inline">\(\frac{\sum\limits_{j=1}^{p}w_j\sum\limits_{i=1}^{p}(rf_{ij} - rf_{ij}')^2}{p^2}\)</span></td>
<td style="text-align: center;"><span class="math inline">\(\frac{\sum\limits_{j=1}^{p}w_j\sum\limits_{i=1}^{p}|rf_{ij} - rf_{ij}'|}{p^2}\)</span></td>
<td style="text-align: center;"><span class="math inline">\(\frac{\sum\limits_{j=1}^{p} w_j \sum\limits_{i=1}^{p}\frac{|rf_{ij} - rf_{ij}'|}{|rf_{ij}|}}{p^2}\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: center;"><span class="math inline">\(C-C'\)</span></td>
<td style="text-align: center;"><span class="math inline">\(\frac{\sum\limits_{i=1}^{p}(c_i - c_i')^2}{p}\)</span></td>
<td style="text-align: center;"><span class="math inline">\(\frac{\sum\limits_{i=1}^{p}|c_i - c_i'|}{p}\)</span></td>
<td style="text-align: center;"><span class="math inline">\(\frac{\sum\limits_{i=1}^{p}\frac{|c_i - c_{i}'|}{|c_i|}}{p}\)</span></td>
</tr>
<tr class="even">
<td style="text-align: center;"><span class="math inline">\(F-F'\)</span></td>
<td style="text-align: center;"><span class="math inline">\(\frac{\sum\limits_{j=1}^{p}w_j\sum\limits_{i=1}^{p}(f_{ij} - f_{ij}')^2}{p^2}\)</span></td>
<td style="text-align: center;"><span class="math inline">\(\frac{\sum\limits_{j=1}^{p}w_j\sum\limits_{i=1}^{p}|f_{ij} - f_{ij}'|}{p^2}\)</span></td>
<td style="text-align: center;"><span class="math inline">\(\frac{\sum\limits_{j=1}^{p} w_j \sum\limits_{i=1}^{p}\frac{|f_{ij} - f_{ij}'|}{|f_{ij}|}}{p^2}\)</span></td>
</tr>
</tbody>
</table>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-tbl" id="tbl-loss-information-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;3.7: Information loss measures for continuous microdata. Source: Domingo-Ferrer, Mateo-Sanz and Torra (2001).
</figcaption>
</figure>
</div>
<p>In&nbsp;Yancey, Winkler and Creecy (2002), it is observed that dividing by <span class="math inline">\(x_{\text{ij}}\)</span> causes the <span class="math inline">\(X - X^{'}\)</span>mean variation to rise sharply when the original value <span class="math inline">\(x_{\text{ij}}\)</span> is close to 0. This dependency on the particular original value being undesirable in an information loss measure, Yancey, Winkler and Creecy (2002) propose to replace the mean variation of <span class="math inline">\(X - X^{'}\)</span> by the more stable measure IL1 given by <span class="math display">\[
  \frac{1}{np}\sum_{j=1}^p\sum_{i=1}^n\frac{|x_{ij}-x'_{ij}|}{\sqrt{2} S_j}
\]</span> where <span class="math inline">\(S_{j}\)</span> is the standard deviation of the <span class="math inline">\(j\)</span>-th variable in the original dataset. This measure was incorporated into the sdcMicro R package. The IL1 measure, in turn, is highly sensitive to small disturbances and weak differentiation of feature values - it may take too high values for variables with low differentiation, and too low - when the differentiation is significant. In practice, if <span class="math inline">\(S_j\)</span> is very close to zero, we obtain as a results INF (infinity). In this case, the measure becomes really useless, because it will not allow to compare the loss of information in several microdata sets with statistical confidentiality protected in various ways - if for each of such sets the IL1 measure will be equal to INF.</p>
<p>Trottini (2003) argues that, since information loss is to be traded off for disclosure risk and the latter is bounded —there is no risk higher than 100%—, upper bounds should be enforced for information loss measures. In practice, the proposal in Trottini (2003) is to limit those measures in <a href="#tbl-loss-information" class="quarto-xref">Table&nbsp;<span>3.7</span></a> based on the mean variation to a predefined maximum value.</p>
<p>Młodak (2020) proposed a new measure of information loss for continuous variables in terms of assesment of impact on the intensity of connections, which was slighthly improved by Młodak, Pietrzak and Józefowski (2022). It is based on on diagonal entries of inversed correlation matrices for continuous variables in the original (<span class="math inline">\(R^{-1}\)</span>) and perturbed (<span class="math inline">\({R^{\prime}}^{-1}\)</span>) data sets, <em>i.e.</em> <span class="math inline">\(\rho_{jj}^{(-1)}\)</span> and <span class="math inline">\({\rho_{jj}^{\prime}}^{(-1)}\)</span>, <span class="math inline">\(j=1,2,\ldots,m_c\)</span> (where <span class="math inline">\(m_c\)</span> is the number of continuous variables): <span id="eq-info-loss-Mlodak"><span class="math display">\[
\gamma=\frac{1}{\sqrt{2}}\sqrt{\sum_{j=1}^{m_c}{\left(\frac{\rho_{jj}^{(-1)}}{\sqrt{\sum_{l=1}^m{\left(\rho_{ll}^{(-1)}\right)^2}}}-\frac{{\rho_{jj}^{\prime}}^{(-1)}}{\sqrt{\sum_{l=1}^m{\left({\rho_{ll}^{\prime}}^{(-1)}\right)^2}}}\right)^2}}\in [0,1].
\tag{3.9}\]</span></span></p>
<p>Values of (<a href="#eq-info-loss-Mlodak" class="quarto-xref"><span>3.9</span></a>) are also easily interpretable - it can be understood as the expected loss of information about connections between variables. As one can easily see, the result can be expressed in %. Of course, both matrices - <span class="math inline">\(R\)</span> and <span class="math inline">\(R'\)</span> - must be based on the same correlation coefficient. The most obvious choice in this respect is the Pearson’s index. However, when tau-Kendall correlation matrix is used, one can also apply it to ordinal variables. The method will be not applicable if the correlation matrix is singular. The main advantage of the measure <span class="math inline">\(\gamma\)</span> is that it treats all variables as an inseparable whole and takes all connections between analysed variables, even those hard to observe, into account. <span class="math inline">\(\gamma\)</span> can be computed in the sdcMicro R package using the function <code>IL_correl()</code>.</p>
</section>
<section id="complex-measures-of-information-loss" class="level3" data-number="3.5.4">
<h3 data-number="3.5.4" class="anchored" data-anchor-id="complex-measures-of-information-loss"><span class="header-section-number">3.5.4</span> Complex measures of information loss</h3>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-25-contents" aria-controls="callout-25" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Expert level
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-25" class="callout-25-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>The above presented concepts of information loss prompt the question whether it is possible to construct complex measure of information loss taking variables of all measurement scales into account. The relevant proposal was formulated by Młodak (2020) and applied by Młodak, Pietrzak and Józefowski (2022) to the case of microdata from the Polish survey of accidents at work. For categorical variables it is based on the approaches <a href="#sec-direct-comparison" class="quarto-xref"><span>3.5.2.1</span></a> <!--(3.5.1)--> and <a href="#sec-contingency-table" class="quarto-xref"><span>3.5.2.2</span></a> <!--(3.5.2)-->, <em>i.e.</em> if the variable <span class="math inline">\(X_j\)</span> is nominal, then (treating NA as a separate level) <span id="eq-distance-nominal"><span class="math display">\[
d(x_{ij}^{\prime},x_{ij})=\begin{cases}
1&amp;\text{if}\; x_{ij}^{\prime}=x_{ij},\cr
0&amp;\text{if}\; x_{ij}^{\prime}\ne x_{ij}.
\end{cases}
\tag{3.10}\]</span></span> <!--3.5.7--></p>
<p>If <span class="math inline">\(X_j\)</span> is ordinal (assuming for simplification and without loss of generality that categories are numbered from 1 to <span class="math inline">\(\mathfrak{r}_j\)</span>, where <span class="math inline">\(\mathfrak{r}_j\)</span> is the number of categories), then (NA is treated as a separate, lowest category) <span id="eq-distance-ordinal"><span class="math display">\[
d(x_{ij}^{\prime},x_{ij})=\frac{\mathfrak{r}(x_{ij}^{\prime},x_{ij})}{\mathfrak{r}_j-1},
\tag{3.11}\]</span></span> <!--3.5.8--></p>
<p>where <span class="math inline">\(\mathfrak{r}(x_{ij}^{\prime},x_{ij})\)</span> is the absolute difference in categories between <span class="math inline">\(x_{ij}^{\prime}\)</span> and <span class="math inline">\(x_{ij}\)</span>. These partial distances take always values from [0,1]. There are, however, some problems with using them, especially if recoding is applied. The number of categories of a recoded variable in the original set and in the set after SDC will be different. Therefore, in the first place, it should be ensured that the numbers of the categories left unchanged are identical in both variants. For example, if before recoding the variable <span class="math inline">\(X_j\)</span> had <span class="math inline">\(\mathfrak{r}_j\)</span>=8 categories marked as 1,2,3,4,5,6,7,8 and as a result of recoding categories 2 and 3 and 6 and 7 were combined, then the new categories should have respectively numbers 1,2,4,5,6,8. Then the option (<a href="#eq-distance-ordinal" class="quarto-xref"><span>3.11</span></a>) for categorical variables applies in this case as well.</p>
<p>Much more complicated situation occurs for continuous variables. Młodak (2020) proposed several options is this respect, <em>e.g.</em> normalized absolute value or normalized square of difference between <span class="math inline">\({x_{ij}}^{\prime}\)</span> and <span class="math inline">\(x_{ij}\)</span>, <em>i.e.</em> <span id="eq-distance-continuous1"><span class="math display">\[
d(x_{ij}^{\prime},x_{ij})=|x_{ij}^{\prime}-x_{ij}| / \max_{k=1,2,\ldots,n}⁡|x_{kj}^{\prime}-x_{kj}|,
\tag{3.12}\]</span></span> <!--(3.5.9)--> or <span id="eq-distance-continuous2"><span class="math display">\[
d(x_{ij}^{\prime},x_{ij})=(x_{ij}^{\prime}-x_{ij})^2 / \max_{k=1,2,\dots,n}⁡(x_{kj}^{\prime}-x_{kj})^2,
\tag{3.13}\]</span></span> <!--(3.5.10)--> <span class="math inline">\(i=1,2,\ldots,n\)</span>, <span class="math inline">\(j=1,2,\ldots,m_c\)</span>, where <span class="math inline">\(n\)</span> is the number of records and <span class="math inline">\(m_c\)</span> - the number of continuous variables.</p>
<p>Measures (<a href="#eq-distance-nominal" class="quarto-xref"><span>3.10</span></a>) and (<a href="#eq-distance-ordinal" class="quarto-xref"><span>3.11</span></a>) also have another significant weakness. The measure of information loss should be an increasing function due to individual partial information losses. This means that, for example, if for some <span class="math inline">\(i\in\{1,2,\ldots,n\}\)</span> the value <span class="math inline">\(|x_{ij}^{\prime}-x_{ij}|\)</span> will increase and all <span class="math inline">\(|x_{hj}^{\prime}-x_{hj}|\)</span> for <span class="math inline">\(h\ne i\)</span> remain the same, the value of the distance should increase. Meanwhile, in the case of formulas (<a href="#eq-distance-continuous1" class="quarto-xref"><span>3.12</span></a>) and (<a href="#eq-distance-continuous2" class="quarto-xref"><span>3.13</span></a>), this will not be the case. If, for the same, the indicated absolute difference (or the square of the difference, respectively) between the original value and the value after SDC reaches a maximum, then the partial loss of information for <span class="math inline">\(i\)</span> will remain unchanged - it will be 1, and for the others it will turn out to be smaller. As a result, we get a smaller metric value, while the information loss actually increased.</p>
<p>Taking the aforementioned observations into account Młodak (2020) proposed in the discussed case the distance of the form: <span id="eq-distance-continuous3"><span class="math display">\[
d(x_{ij}^{\prime},x_{ij})=\frac{2}{\pi}\arctan|x_{ij}^{\prime}-x_{ij}|.
\tag{3.14}\]</span></span> <!--3.5.11--> The arcus tangens (arctan) function was used to ensure that the distance between original and perturbed values takes values from <span class="math inline">\([0,1]\)</span>. To achieve this, an ascending function bounded on both sides (both from the top and from the bottom) should be applied. The arctan seems to be a good solution and is also easy to compute. Of course – like any function of this type – it is not perfect: for larger absolute differences between original and perturbed values it tends to be close to <span class="math inline">\(\frac{\pi}{2}\)</span> (and, in consequence, <span class="math inline">\(d(x_{ij}^{\prime},x_{ij})\)</span> to be close to 1). On the other hand, owing to this property it exhibits more clearly all information losses due to perturbation.</p>
<p>The complex measure of distribution disturbance is given by (cf.&nbsp;Młodak, Pietrzak and Józefowski (2022)): <span id="eq-lambda-distance"><span class="math display">\[
\lambda=\sum_{j=1}^m{\sum_{i=1}^n{\frac{d(x_{ij}^{\prime},x_{ij})}{mn}}}\in [0,1],
\tag{3.15}\]</span></span> <!--3.5.12--> where <span class="math inline">\(d(\cdot,\cdot)\in [0,1]\)</span> is measure of distance according to the formulas (<a href="#eq-distance-nominal" class="quarto-xref"><span>3.10</span></a>), (<a href="#eq-distance-ordinal" class="quarto-xref"><span>3.11</span></a>) or (<a href="#eq-distance-continuous3" class="quarto-xref"><span>3.14</span></a>) according to the measurement scale of a given value.</p>
<p>Authors of the aforementioned paper indicated also than one can measure the contribution of particular variables <span class="math inline">\(X_j\)</span> to total information loss as follows<br>
<span id="eq-lambda-infoloss"><span class="math display">\[
\lambda_j=\sum_{i=1}^n{\frac{d(x_{ij}^{\prime},x_{ij})}{n}}\in [0,1],
\tag{3.16}\]</span></span> <!--3.5.13--> <span class="math inline">\(j=1,2,\ldots,m\)</span>.</p>
<p>An additional problem occurs if non-perturbative SDC tools are used. In this case the original values are either suppressed or remained unchanged. How to proceed in this case during computation of the measures (<a href="#eq-distance-continuous2" class="quarto-xref"><span>3.13</span></a> and (<a href="#eq-distance-continuous3" class="quarto-xref"><span>3.14</span></a>) also depends on the measurement scale of the variables. If the used <span class="math inline">\(X_j\)</span> is nominal, then if <span class="math inline">\(x_{ij}^{\prime}\)</span> is hidden then one should assume <span class="math inline">\(d(x_{ij}^{\prime},x_{ij})=1\)</span>; if <span class="math inline">\(X_j\)</span> is ordinal, then we assign <span class="math inline">\(x_{ij}^{\prime}:=1\)</span> if <span class="math inline">\(x_{ij}\)</span> is closer to <span class="math inline">\(\mathfrak{r}_j\)</span> or <span class="math inline">\(x_{ij}^{\prime}:=\mathfrak{r}_j\)</span> if <span class="math inline">\(X_j\)</span> is closer to 1; if <span class="math inline">\(X_j\)</span> is continuous, then <span class="math display">\[
x_{ij}^{\prime}:=\begin{cases}
\max\limits_{h=1,2,\ldots,n}{x_{hj}}&amp;\text{if}\quad x_{ij}\le\operatorname*{med}\limits_{h=1,2,\ldots,n}{x_{hj}},\\
\min\limits_{h=1,2,\ldots,n}{x_{hj}}&amp;\text{if}\quad x_{ij}&gt;\operatorname*{med}\limits_{h=1,2,\ldots,n}{x_{hj}}.
\end{cases}
\]</span></p>
<p>The measures (<a href="#eq-lambda-distance" class="quarto-xref"><span>3.15</span></a>) and (<a href="#eq-lambda-infoloss" class="quarto-xref"><span>3.16</span></a>) can be expressed as a percentages and show total information loss and contribution of particular variables to it, respectively. The greater the value of <span class="math inline">\(\lambda/\lambda_j\)</span>, the bigger the loss/contribution. In this way users obtain clear and easily understandable information about expected information loss owing to the application of SDC. These measures were implemented to the sdcMicro R package and are computed by the function <code>IL_variables</code>.</p>
</div>
</div>
</div>
</section>
<section id="practical-realization-of-trade-off-between-safety-and-utility-of-microdata" class="level3" data-number="3.5.5">
<h3 data-number="3.5.5" class="anchored" data-anchor-id="practical-realization-of-trade-off-between-safety-and-utility-of-microdata"><span class="header-section-number">3.5.5</span> Practical realization of trade-off between safety and utility of microdata</h3>
<p>Achieving the optimal balance between minimization of dislosure risk and minimization of the information loss is not easy. It is very hard (if even possible) to take all aspects deciding on level of these quantities (especially in the case of risk) into account. Moreover, both risk and information loss can be assessed from various point of views. Thus, first one should establish the possible factors which may decide on the type and level of dislosure risk and the most preferred direction of data use by the user. In the case of risk, one should assess not only internal risk (including different types of variables and their relationships) but also assess what alternative data sources the interested data user could have access to due to his place of employment and position held (such information is usually provided in official data access request). The priorities in measurement of information loss preferred by the user should be a basis for establishment of used measure in this context. For instance, if the users prefers comparison of distributions of some phenomena, then the measures of distribution disturbance should have much higher priority than others. On the other hand, if the subject of interest of an user are connections between some features, then for categorical variables the information loss should be assessed using the measures for contingency tables (as they are in fact frequency tables, this problem is discussed in <a href="05-frequency-tables.html">Chapter 5</a>). For continuous variables the aforementioned measures of impact on the intensity of connections can be, of course, applied.</p>
<p>Similarly as <em>e.g.</em> in the case of significance and loss in testing of statistical hypotheses, the most obvious and easy approach to obtain reasonable compromise between these two expectations is to apply one of two following ways:</p>
<ul>
<li>establishing arbitrarily maximum allowable level of disclosure risk and minimize the information loss in this situation - it defends, first of all, the data confidentiality and trust to data holder in terms of privacy protection,</li>
<li>establishing arbitrarily maximum allowable level of information loss and minimize the disclosure risk in this situation - it defends, first of all, the data utility for users and data provider as a source of reliable, creadible and useful data.</li>
</ul>
<p>In practice, the data holder (<em>e.g.</em> official statistics) prefers rather the first approach as the strict protection of data privacy is usually an obligation imposed by valid law regulations. So, assurance of the safety of confidential information is very important.</p>
</section>
<section id="example" class="level3 page-columns page-full" data-number="3.5.6">
<h3 data-number="3.5.6" class="anchored" data-anchor-id="example"><span class="header-section-number">3.5.6</span> Example</h3>
<p>The manner of assessing disclosure risk and information loss owing to the application of SDC methods was demonstrated using data from a case study published on the website of International Household Survey Network (IHSN)<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> Statistical Disclosure Control for Microdata: A Practice Guide - Case Study Data and R Script, being a supplement to the book by Benschop, Machingauta and Welch (2022). Use was made of part of the code from the first study of this type, in which the authors applied SDC measures to a set of farms using the sdcMicro package.</p>
<div class="no-row-height column-margin column-container"><p><sup>2</sup>&nbsp;<a href="https://www.ihsn.org/software/disclosure-control-toolbox">https://www.ihsn.org/software/disclosure-control-toolbox</a></p></div><p>The following categorical variables were selected as key variables: REGION, URBRUR (area of residence), HHSIZE (household size), OWNAGLAND (agricultural land ownership), RELIG (religion of household head). The authors of the case study applied local data suppression to these variables.</p>
<p>SDC was also applied to quantitative variables concerning 1) expenditure: TFOODEXP (total food expenditure), TALCHEXP (total alcohol expenditure), TCLTHEXP (total expenditure on clothing and footwear), THOUSEXP (total expenditure on housing), TFURNEXP (total expenditure on furnishing ), THLTHEXP (total expenditure on health), TTRANSEXP (total expenditure on transport), TCOMMEXP (total expenditure on communications), TRECEXP (total expenditure on recreation), TEDUEXP (total expenditure on education), TRESTHOTEXP (total expenditure on restaurants and hotel ), TMISCEXP (total miscellaneous expenditure); 2) income: INCTOTGROSSHH (total gross household income – annual), INCRMT (total amount of remittances received from remittance sending members), INCWAGE (wage and salaries – annual), INCFARMBSN (gross income from household farm businesses – annual), INCNFARMBSN (gross income from household non-farm businesses – annual), INCRENT (rental income – annual), INCFIN (financial income from savings, loans, tax refunds, maturity payments on insurance), INCPENSN (pension and other social assistance – annual), INCOTHER (other income – annual), and 3) land size: LANDSIZEHA (land size owned by household in ha). 1% noise was added to the variables relating to all components of expenditure and income; 5% noise was added to outliers. Values of the LANDSIZEHA variable were rounded (1 digit for plots smaller than 1 and to no digits for plots larger than 1) and grouped (values in intervals 5-19 to 13, and values in intervals 20-39 to 30, values larger than 40 to 40).</p>
<p>In the case study, the PRAM method was applied to variables describing apartment equipment: ROOF (roof type), WATER (main source of water), TOILET (main toilet facility), ELECTCON (electricity), FUELCOOK (main cooking fuel), OWNMOTORCYCLE (ownership of motorcycle), CAR (ownership of car), TV (ownership of television), LIVESTOCK (number of large-sized livestock owned). The data were stratified by REGION variable making sure that variants of the transformed variables were not modified in 80% of cases.</p>
<p>The set of data anonymised in the manner described above was used as the starting point for the assessment of the risk of disclosure and information loss. Tables <a href="#tbl-example-individual-risk" class="quarto-xref">Table&nbsp;<span>3.8</span></a> and <a href="#tbl-example-global-risk" class="quarto-xref">Table&nbsp;<span>3.9</span></a> shows descriptive statistics for the risk of disclosure in the case of key variables before and after applying local suppression. While the risk was significantly reduced, one must bear in mind that the risk of disclosure was already relatively low in the original dataset. The maximum value of individual risk dropped from 5.5% in the original dataset to 0.3% after applying local suppression. The global risk in the original set was on average equal to 0.05%, which means that the expected number of disclosed units was 0.99; after applying local suppression, the global risk dropped to less than 0.02, which means that the expected number of disclosed units was 0.35.</p>
<p>As regards the assessment of disclosure risk for quantitative variables, an interval of [0.0%, 83.5%] was chosen, where the upper limit represents the worst case scenario in which the intruder is sure that each nearest neighbour is in fact the correct linkage.</p>
<p>Several of the measures mentioned above have been developed to assess the loss of information. Based on distances between values of variables that were to be anonymised in the original set and the their values in the anonymised set, <span class="math inline">\(\lambda\)</span> measures were calculated. Table <a href="#tbl-example-information-loss" class="quarto-xref">Table&nbsp;<span>3.10</span></a> shows the general value of <span class="math inline">\(\lambda\)</span> and its values for individual variables (<span class="math inline">\(\lambda_k\)</span>). The overall loss of information for the anonymised variables is 14.3%. The greatest loss is observed for quantitative variables to which noise was added; in the case of INCTOTGROSSHH, the loss of information measured by <span class="math inline">\(\lambda\)</span> reaches 83.4%. The loss of information was much lower in the case of key variables subjected to local suppression and those modified with the PRAM method: the maximum loss was 9.7% and 9.4%, respectively.</p>
<p>Overall information loss was determined using two measures described above: <span class="math inline">\(IL1\)</span> and <span class="math inline">\(\lambda\)</span>. <span class="math inline">\(IL1\)</span> was equal to 79.4, which indicates relatively large standard deviations of anonymised values of quantitative variables from standard deviations of the original variables. The value of the second measure, which is based on correlation coefficients, is 0.6%, which indicates a slight loss of information regarding correlations between the quantitative variables. Nevertheless, it should be stressed that as a result of to numerous cases of non-response in the quantitative variables, the value of <span class="math inline">\(\lambda\)</span> was calculated on the basis of only 111 observations, <em>i.e.</em> less than 6% of all units.</p>
<p>The above assessment was conducted using the R sdcMicro package. Because some of the information loss measures described above are not implemented in this package, they were not used in the assessment.</p>
<div id="tbl-example-individual-risk" class="quarto-float anchored">
<figure class="quarto-float quarto-float-tbl figure">
<div aria-describedby="tbl-example-individual-risk-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="table">
<colgroup>
<col style="width: 36%">
<col style="width: 26%">
<col style="width: 36%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;"><strong>Statistic</strong></th>
<th style="text-align: right;"><strong>Original values</strong></th>
<th style="text-align: right;"><strong>Values after anonymisation</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Min</td>
<td style="text-align: right;">0.0007</td>
<td style="text-align: right;">0.0007</td>
</tr>
<tr class="even">
<td style="text-align: left;">Q1</td>
<td style="text-align: right;">0.0021</td>
<td style="text-align: right;">0.0021</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Me</td>
<td style="text-align: right;">0.0067</td>
<td style="text-align: right;">0.0059</td>
</tr>
<tr class="even">
<td style="text-align: left;">Q3</td>
<td style="text-align: right;">0.0213</td>
<td style="text-align: right;">0.0161</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Max</td>
<td style="text-align: right;">5.5434</td>
<td style="text-align: right;">0.3225</td>
</tr>
<tr class="even">
<td style="text-align: left;">Mean</td>
<td style="text-align: right;">0.0502</td>
<td style="text-align: right;">0.0176</td>
</tr>
</tbody>
</table>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-tbl" id="tbl-example-individual-risk-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;3.8: Descriptive statistics of individual risk measures for quantitative variables
</figcaption>
</figure>
</div>
<div id="tbl-example-global-risk" class="quarto-float anchored">
<figure class="quarto-float quarto-float-tbl figure">
<div aria-describedby="tbl-example-global-risk-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="table">
<colgroup>
<col style="width: 36%">
<col style="width: 26%">
<col style="width: 36%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;"><strong>Statistic</strong></th>
<th style="text-align: right;"><strong>Original values</strong></th>
<th style="text-align: right;"><strong>Values after anonymisation</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Risk %</td>
<td style="text-align: right;">0.0502</td>
<td style="text-align: right;">0.0176</td>
</tr>
<tr class="even">
<td style="text-align: left;">Expected number of disclosures</td>
<td style="text-align: right;">0.9895</td>
<td style="text-align: right;">0.3476</td>
</tr>
</tbody>
</table>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-tbl" id="tbl-example-global-risk-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;3.9: Global risk measures for quantitative variables
</figcaption>
</figure>
</div>
<div id="tbl-example-information-loss" class="quarto-float anchored">
<figure class="quarto-float quarto-float-tbl figure">
<div aria-describedby="tbl-example-information-loss-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="table">
<thead>
<tr class="header">
<th style="text-align: left;"><strong>Variable</strong></th>
<th style="text-align: right;"><span class="math inline">\(\lambda\)</span> (%)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>OVERALL</strong></td>
<td style="text-align: right;"><strong>14.3</strong></td>
</tr>
<tr class="even">
<td style="text-align: left;">URBRUR</td>
<td style="text-align: right;">0.5</td>
</tr>
<tr class="odd">
<td style="text-align: left;">REGION</td>
<td style="text-align: right;">0.2</td>
</tr>
<tr class="even">
<td style="text-align: left;">OWNAGLAND</td>
<td style="text-align: right;">2.5</td>
</tr>
<tr class="odd">
<td style="text-align: left;">RELIG</td>
<td style="text-align: right;">1.1</td>
</tr>
<tr class="even">
<td style="text-align: left;">LANDSIZEHA</td>
<td style="text-align: right;">9.7</td>
</tr>
<tr class="odd">
<td style="text-align: left;">TANHHEXP</td>
<td style="text-align: right;">50.7</td>
</tr>
<tr class="even">
<td style="text-align: left;">TFOODEXP</td>
<td style="text-align: right;">38.3</td>
</tr>
<tr class="odd">
<td style="text-align: left;">TALCHEXP</td>
<td style="text-align: right;">12.6</td>
</tr>
<tr class="even">
<td style="text-align: left;">TCLTHEXP</td>
<td style="text-align: right;">8.4</td>
</tr>
<tr class="odd">
<td style="text-align: left;">THOUSEXP</td>
<td style="text-align: right;">14.6</td>
</tr>
<tr class="even">
<td style="text-align: left;">TFURNEXP</td>
<td style="text-align: right;">6.0</td>
</tr>
<tr class="odd">
<td style="text-align: left;">THLTHEXP</td>
<td style="text-align: right;">12.3</td>
</tr>
<tr class="even">
<td style="text-align: left;">TTRANSEXP</td>
<td style="text-align: right;">18.5</td>
</tr>
<tr class="odd">
<td style="text-align: left;">TCOMMEXP</td>
<td style="text-align: right;">9.2</td>
</tr>
<tr class="even">
<td style="text-align: left;">TRECEXP</td>
<td style="text-align: right;">4.5</td>
</tr>
<tr class="odd">
<td style="text-align: left;">TEDUEXP</td>
<td style="text-align: right;">41.4</td>
</tr>
<tr class="even">
<td style="text-align: left;">TRESTHOTEXP</td>
<td style="text-align: right;">16.6</td>
</tr>
<tr class="odd">
<td style="text-align: left;">TMISCEXP</td>
<td style="text-align: right;">6.4</td>
</tr>
<tr class="even">
<td style="text-align: left;">INCTOTGROSSHH</td>
<td style="text-align: right;">73.6</td>
</tr>
<tr class="odd">
<td style="text-align: left;">INCRMT</td>
<td style="text-align: right;">32.1</td>
</tr>
<tr class="even">
<td style="text-align: left;">INCWAGE</td>
<td style="text-align: right;">71.0</td>
</tr>
<tr class="odd">
<td style="text-align: left;">INCFARMBSN</td>
<td style="text-align: right;">15.1</td>
</tr>
<tr class="even">
<td style="text-align: left;">INCNFARMBSN</td>
<td style="text-align: right;">24.0</td>
</tr>
<tr class="odd">
<td style="text-align: left;">INCRENT</td>
<td style="text-align: right;">10.4</td>
</tr>
<tr class="even">
<td style="text-align: left;">INCFIN</td>
<td style="text-align: right;">1.3</td>
</tr>
<tr class="odd">
<td style="text-align: left;">INCPENSN</td>
<td style="text-align: right;">17.1</td>
</tr>
<tr class="even">
<td style="text-align: left;">INCOTHER</td>
<td style="text-align: right;">17.7</td>
</tr>
<tr class="odd">
<td style="text-align: left;">ROOF</td>
<td style="text-align: right;">6.0</td>
</tr>
<tr class="even">
<td style="text-align: left;">TOILET</td>
<td style="text-align: right;">7.6</td>
</tr>
<tr class="odd">
<td style="text-align: left;">WATER</td>
<td style="text-align: right;">9.4</td>
</tr>
<tr class="even">
<td style="text-align: left;">ELECTCON</td>
<td style="text-align: right;">1.7</td>
</tr>
<tr class="odd">
<td style="text-align: left;">FUELCOOK</td>
<td style="text-align: right;">4.3</td>
</tr>
<tr class="even">
<td style="text-align: left;">OWNMOTORCYCLE</td>
<td style="text-align: right;">3.3</td>
</tr>
<tr class="odd">
<td style="text-align: left;">CAR</td>
<td style="text-align: right;">1.5</td>
</tr>
<tr class="even">
<td style="text-align: left;">TV</td>
<td style="text-align: right;">7.3</td>
</tr>
<tr class="odd">
<td style="text-align: left;">LIVESTOCK</td>
<td style="text-align: right;">1.3</td>
</tr>
</tbody>
</table>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-tbl" id="tbl-example-information-loss-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;3.10: Loss of information due to anonymisation, overall and for individual variables
</figcaption>
</figure>
</div>
</section>
<section id="references-5" class="level3" data-number="3.5.7">
<h3 data-number="3.5.7" class="anchored" data-anchor-id="references-5"><span class="header-section-number">3.5.7</span> References</h3>
<p>Benschop, T., Machingauta, C., and Welch, M. (2022). <em>Statistical Disclosure Control: A Practice Guide</em>, <a href="https://readthedocs.org/projects/sdcpractice/downloads/pdf/latest/">https://readthedocs.org/projects/sdcpractice/downloads/pdf/latest/</a></p>
<p>Chatfield, C., and Collins, A. J., (1980). <em>Introduction to Multivariate Analysis</em>, Chapman and Hall, London, 1980.</p>
<p>Dandekar, R., Domingo-Ferrer, J., and Sebé, F., (2002). <em>LHS-based hybrid microdata vs.&nbsp;rank swapping and microaggregation for numeric microdata protection.</em> In J.&nbsp;Domingo-Ferrer, editor, Inference Control in Statistical Databases, volume 2316 of LNCS, pages 153–162, Berlin Heidelberg, 2002. Springer.</p>
<p>De Waal, A. G., and Willenborg, L.&nbsp;C. R.&nbsp;J. (1999). <em>Information loss through global recoding and local suppression.</em> Netherlands Official Statistics, 14:17–20, 1999. special issue on SDC.</p>
<p>Domingo-Ferrer, J., Mateo-Sanz, J. M., and Torra, V. (2001). <em>Comparing sdc methods for microdata on the basis of information loss and disclosure risk</em>. In Pre-proceedings of ETK-NTTS’2001 (vol.&nbsp;2), pages 807–826, Luxemburg, 2001. Eurostat.</p>
<p>Domingo-Ferrer, J., and Torra, V. (2001). <em>Disclosure protection methods and information loss for microdata.</em> In P.&nbsp;Doyle, J.&nbsp;I. Lane, J.&nbsp;J.&nbsp;M. Theeuwes, and L.&nbsp;Zayatz, editors, Confidentiality, Disclosure and Data Access: Theory and Practical Applications for Statistical Agencies, pages 91–110, Amsterdam, 2001. North-Holland. <a href="https://crises-deim.urv.cat/webCrises/publications/bcpi/cliatpasa01Disclosure.pdf">https://crises-deim.urv.cat/webCrises/publications/bcpi/cliatpasa01Disclosure.pdf</a>.</p>
<p>Hundepool, A., Van de&nbsp;Wetering, A., Ramaswamy, R., Franconi, L., Capobianchi, A., De Wolf, P.P., Domingo-Ferrer, J., Torra, V., Brand, R:, and Giessing, S. (2005). <em><span class="math inline">\(\mu\)</span>-ARGUS version 5.1 Software and User’s Manual</em>. Statistics Netherlands, Voorburg NL, 2014. <a href="https://research.cbs.nl/casc/Software/MUmanual5.1.3.pdf">https://research.cbs.nl/casc/Software/MUmanual5.1.3.pdf</a>.</p>
<p>Hundepool, A., Domingo–Ferrer, J., Franconi, L., Giessing, S., Nordholt, E. S., Spicer, K., &amp; de Wolf, P. (2012). <em>Statistical Disclosure Control</em>. John Wiley &amp; Sons, Ltd.</p>
<p>Kooiman, P. L., Willenborg, L. and Gouweleeuw, J. (1998). <em>PRAM: A method for disclosure limitation of microdata.</em> Technical report, Statistics Netherlands (Voorburg, NL), 1998.</p>
<p>Młodak, A. (2020). Information loss resulting from statistical disclosure control of output data. <em>Wiadomości Statystyczne. The Polish Statistician</em>, 65 (9), 7–27. (in Polish)</p>
<p>Młodak, A., Pietrzak, M., &amp; Józefowski, T. (2022). The trade–off between the risk of disclosure and data utility in SDC: A case of data from a survey of accidents at work. <em>Statistical Journal of the IAOS</em>, 38 (4), 1503–1511.</p>
<p>Trottini, M. (2003) . <em>Decision models for data disclosure limitation</em>. PhD thesis, Carnegie Mellon University, 2003.</p>
<p>Willenborg, L., and De Waal, T., (2001). <em>Elements of Statistical Disclosure Control</em>. Springer-Verlag, New York, 2001.</p>
<p>Winkler, W. E. (1998). <em>Re-identification methods for evaluating the confidentiality of analytically valid microdata.</em> In J.&nbsp;Domingo-Ferrer, editor, Statistical Data Protection, Luxemburg, 1999. Office for Official Publications of the European Communities. (Journal version in Research in Official Statistics, vol.&nbsp;1, no. 2, pp.&nbsp;50-69, 1998).</p>
<p>Yancey, W. E., Winkler, W. E., and Creecy, R. H. (2002). <em>Disclosure risk assessment in perturbative microdata protection.</em> In J.&nbsp;Domingo-Ferrer, editor, Inference Control in Statistical Databases, volume 2316 of LNCS, pages 135–152, Berlin Heidelberg, 2002. Springer.</p>
</section>
</section>
<section id="sec-software" class="level2" data-number="3.6">
<h2 data-number="3.6" class="anchored" data-anchor-id="sec-software"><span class="header-section-number">3.6</span> Software</h2>
<section id="sec-mu-argus" class="level3" data-number="3.6.1">
<h3 data-number="3.6.1" class="anchored" data-anchor-id="sec-mu-argus"><span class="header-section-number">3.6.1</span> <span class="math inline">\(\mu\)</span>-ARGUS</h3>
<p>The <span class="math inline">\(\mu\)</span>-ARGUS software has been developed to facilitate statisticians, mainly in the NSI’s, to apply the SDC-methods described above to create safe micro data files. It is a tool to apply the SDC-methodology, not a black-box that will create a safe file without knowing the background of the SDC-methodology. The development of <span class="math inline">\(\mu\)</span>‑ARGUS has started at Statistics Netherlands by implementing the Dutch methods and rules. With this software as a starting point many other methods have been added. Several of these methods have been developed and/or actually implemented during the CASC-project.</p>
<p>In this section we will just a short overview of <span class="math inline">\(\mu\)</span>-ARGUS, as an extensive manual is available, fully describing the software.</p>
<p>The starting point of <span class="math inline">\(\mu\)</span>-ARGUS has been implementation of the threshold rules for identifying unsafe records and procedures for global recoding and local suppression.</p>
<p><strong>Data:</strong> <span class="math inline">\(\mu\)</span>-ARGUS can both protect fixed and free format ASCII files.</p>
<div id="fig-mu-argus-overview" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-mu-argus-overview-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Images/media/mu_argus_overview.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-mu-argus-overview-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.2: Overview of <span class="math inline">\(\mu\)</span>-ARGUS
</figcaption>
</figure>
</div>
<p>Many of the methods described previously in the methodology section can be applied with <span class="math inline">\(\mu\)</span>-ARGUS to a dataset. It is our aim to include other methods as well in the near future, if time permits</p>
<p><span class="math inline">\(\mu\)</span>-ARGUS is a flexible interactive program that will guide you through the process of data protection. In a typically <span class="math inline">\(\mu\)</span>-ARGUS run you will typically go through the following steps, given that the microdata set is ready.</p>
<ol type="1">
<li><strong>Meta data</strong>. <span class="math inline">\(\mu\)</span>-ARGUS needs to know the structure of the data set. Not only the general aspects but also additional SDC-specific information. As there is till now no suitable flexible standard for meta data allowing us to specify also the SDC-specific parts of the meta data, we have to rely on the ARGUS meta data format.<br>
This can be prepared (partially) externally or it can be specified interactively during a <span class="math inline">\(\mu\)</span>-ARGUS session.</li>
<li><strong>Threshold-rule/risk models.</strong> Selection and computation of frequency tables on which several SDC-methods (like risk models, threshold rule) are based</li>
<li><strong>Global recoding.</strong> Selection of possible recodings and inspection of the results.</li>
<li>Selection and application of <strong>other protection methods</strong> like:
<ul>
<li>Microaggregation (<a href="#sec-microaggregation" class="quarto-xref"><span>3.4.2.3</span></a>)</li>
<li>PRAM (<a href="#sec-PRAM" class="quarto-xref"><span>3.4.6</span></a>)</li>
<li>Rounding (<a href="#sec-rounding" class="quarto-xref"><span>3.4.2.5</span></a>)</li>
<li>Top and bottom coding (<a href="#sec-top-and-bottom-coding" class="quarto-xref"><span>3.4.3.3</span></a>)</li>
<li>Rank swapping (<a href="#sec-data-rank-swapping" class="quarto-xref"><span>3.4.2.4</span></a>)</li>
<li>Noise addition (<a href="#sec-noise-addition" class="quarto-xref"><span>3.4.2.1</span></a>)</li>
</ul></li>
<li><strong>Risk model:</strong> selection of the risk-level</li>
<li><strong>Generate the safe micro file.</strong> During this process all data transformations specified above. This is also the moment that all remaining unsafe combinations will be protected by local suppressions. Also an extensive report will be generated.</li>
</ol>
<p>When the above scheme has been followed a safe microdata file has been generated. <span class="math inline">\(\mu\)</span>‑ARGUS is capable of handling very large datasets. Only during the first phase, when the datafile is explored and the frequency tables are computed some heavy computations are performed. This might take some time depending on the size of the datafile. However all the real SDC-work (global recoding and the other methods named under 4 and 5 above) are done at the level of the information prepared during this first phase. This will be done very quickly. Only in the final phase when the protected datafile is made, the operation might be time consuming depending on the size of the datafile.</p>
<p>This architecture of <span class="math inline">\(\mu\)</span>‑ARGUS has the advantage that all real SDC-work, that will be done interactively, will have a very quick response time. Inspecting the results of various recodings is easy and simple.</p>
<p>The most recent release of <span class="math inline">\(\mu\)</span>-ARGUS can be found on GitHub (<a href="https://github.com/sdcTools/muargus/releases">https://github.com/sdcTools/muargus/releases</a>).</p>
</section>
<section id="sec-sdcmicro" class="level3" data-number="3.6.2">
<h3 data-number="3.6.2" class="anchored" data-anchor-id="sec-sdcmicro"><span class="header-section-number">3.6.2</span> sdcMicro</h3>
<p><code>sdcMicro</code> (<a href="https://github.com/sdcTools/sdcMicro">https://github.com/sdcTools/sdcMicro</a>) is an <a href="https://r-project.org">R</a> package implementing almost all methods discussed in <a href="#sec-microdata-protection-methods" class="quarto-xref"><span>Section 3.4</span></a>. The required steps to use the package are essentially the same as outlined in <a href="#sec-mu-argus" class="quarto-xref"><span>Section 3.6.1</span></a> and are quickly summarized below as well.</p>
<ol type="1">
<li><p><strong>Definition of a problem</strong> The first step is always to create an object that defines the current sdc problem. This task can be achieved by calling function <code>createSdcObj()</code>. In this function, quite a few parameters can be set. The most important ones are:</p>
<ul>
<li><strong>Data</strong>: the input data set needs to be a <code>data.frame</code> / <code>data.table</code> but it should be noted that any functionality from <code>R</code> can be used to create such objects from a variety of files exported or generated from other tools such as SAS, SPSS or STATA among using plain text-files (such as <code>.csv)</code> or other structured formats like .<code>json</code> or <code>.xml</code> as long those can be converted to a rectangular data structures. It is of course also possible to use queries to database systems in order to create suitable input objects.</li>
<li><strong>Key variables for risk assessment</strong>: the user is required to specify a set of categorical key variables. These variables are automatically used when computing risk measures (see also <a href="#sec-concepts" class="quarto-xref"><span>Section 3.3.3</span></a>).</li>
<li><strong>Numerical key variables</strong>: It is also possible (but optional) to specify a set of numerical variables that are deemed important. Such variables can (automatically) be used to apply suitable perturbation methods (such as e.g masking by noise) to it.</li>
<li><strong>Weights</strong>: In case the underlying microdata step from a survey sample, a variable holding suitable weights can be specified. This is required in order to make sure that risk measures are computed correctly.</li>
<li><strong>Strata</strong>: Sometimes it is useful if a specific anonymization approach is applied independently to specific strata of the underlying population. In <code>sdcMicro</code> this can be achieved by defining a variable that holds different values for different groups of the population.</li>
<li><strong>Ghost-variables</strong>: This allows to link variables to (categorical key) variables in a sense that modifications to the relevant key-variable (e.g suppression) are transferred and applied to the dependent variables that are referred to as <em>“ghost”</em> variables</li>
<li><strong>Excluding direct identifiers</strong>: In statistical practice microdata files often contain direct identifiers which can be identified already on creation of an input object. If such variables have been defined, they will be removed prior to any computations.</li>
</ul>
<p>It should be noted that while it is very convenient to work with an object created with <code>createSdcObj()</code>, it is perfectly possible to apply all implemented methods of the package also to simpler data-structures like a <code>data.frame</code>.</p></li>
<li><p><strong>Application of SDC-methods</strong> Once a problem instance has been created, some helpful summary statistics such as the number of observations violating <span class="math inline">\(k\)</span>-anonymity or (global) risk measures such as the expected number of re-identifications given the defined risk-scenario, are readily available and are shown by simply printing out the object.</p>
<p>The next step is then to interactively apply SDC techniques to the object and re-assess the impact of its application both on risk-measures as well as on data-utility. If the application yields unexpected or bad results, the implemented <code>undo()</code>-method can be used to revert to the state before application of the specific methods. This allows to quickly try out different parameter settings and makes the process of applying SDC methods quite interactive.</p>
<p>The package allows to (for example) add stochastic noise to numerical variables (<a href="#sec-noise-addition" class="quarto-xref"><span>3.4.2.1</span></a>) using <code>addNoise()</code>, post-randomize values in categorically scaled variables (<a href="#sec-PRAM" class="quarto-xref"><span>3.4.6</span></a>) with function <code>pram()</code>, create synthetic microdata (<a href="#sec-synthetic-data" class="quarto-xref"><span>3.4.7</span></a>) with method <code>dataGen()</code> or perform global recoding (<a href="#sec-global-recoding" class="quarto-xref"><span>3.4.3.2</span></a>) by using <code>globalRecode()</code>. Furthermore it is possible to apply rank-swapping (<a href="#sec-data-rank-swapping" class="quarto-xref"><span>3.4.2.4</span></a>) with function <code>rankSwap()</code>, compute SUDA-scores (<a href="#sec-SUDA" class="quarto-xref"><span>3.3.7</span></a>) using <code>suda2()</code>, compute individual risk estimates (<a href="#sec-argus-individual-risk" class="quarto-xref"><span>3.3.5</span></a>) with <code>indivRisk()</code> and <code>freqCalc()</code> as well as make a set of categorical key variables fulfill <span class="math inline">\(k\)</span>-anonymity using <code>kAnon()</code>. In the current versions of the package, TRS (targeted record swapping, <a href="05-frequency-tables.html#sec-TRS" class="quarto-xref"><span>5.6</span></a>) is implemented and can be called using the <code>recordSwap()</code> function. A detailed discussion and overview is available in a custom vignette (<a href="https://sdctools.github.io/sdcMicro/articles/recordSwapping.html">https://sdctools.github.io/sdcMicro/articles/recordSwapping.html</a>).</p></li>
<li><p><strong>Exporting Results</strong> Once the interactive process has been finished, the package allows to quickly write out a safe dataset that contains all applied techniques also respecting any settings defined when initializing the object itself such as <em>“ghost-variables”</em> using function <code>writeSafeFile()</code>.</p>
<p>Further more there is a <code>report()</code> functionality available that can be applied to an sdc-object at any time. This method can be called to either generate an internal or external report summarizing the process. The difference between the internal and the external report is the level of detail. While the external report is targeted for public consumption and does not contain any (sensitive) values such as specific parameter settings, the internal report lists any techniques that have been applied to protect the microdata in great detail. Both variants result in a <code>html</code> file that can easily be shared.</p></li>
<li><p><strong>Graphical User-Interface</strong> Creating safe, protected microdata files is often a challenging task. Also having to dive into <code>R</code> and write code to perform several steps of the procedure can be a hurdle for non-experts in <code>R</code>. In order to mitigate this problem and to facilitate the use of <code>sdcMicro</code>, the package comes with an interactive, shiny-based graphical user-interface (Meindl, 2019). The interface can be started using the <code>sdcApp</code> function and its functionality is explained in detail in a custom vignette (<a href="https://sdctools.github.io/sdcMicro/articles/sdcMicro.html">https://sdctools.github.io/sdcMicro/articles/sdcMicro.html</a>).</p></li>
</ol>
</section>
</section>
<section id="sec-intro-example" class="level2" data-number="3.7">
<h2 data-number="3.7" class="anchored" data-anchor-id="sec-intro-example"><span class="header-section-number">3.7</span> Introductory example: rules at Statistics Netherlands</h2>
<p>As has been shown in the previous sections there are many sophisticated ways of making a safe protected microdata set. And it is far from a simple straightforward task to select the most appropriate method for the Disclosure Protection of a microdata set. This requires a solid knowledge of the survey in question as well as a good overview of all the methods described in the previous sections.</p>
<p>However as an introduction we will describe here a method/set of rules inspired by those currently applied at Statistics Netherlands for making both microdata files for researchers as well as public use files. This approach can be easily applied, as it is readily available in <span class="math inline">\(\mu\)</span>-ARGUS. These rules are based on the ARGUS threshold-rule in combination with global recoding and local suppression (see <a href="#sec-global-recoding" class="quarto-xref"><span>Section 3.4.3.2</span></a> and <a href="#sec-local-suppression" class="quarto-xref"><span>3.4.3.4</span></a>). This rule only concentrates on the identifying variables or key-variables, as these are the starting point for an intrusion. There rules have primarily been developed for microdata about persons.</p>
<p><em>Microdata for researchers</em><br>
For the microdata for researchers one could use the following set of rules:</p>
<ol type="1">
<li>Direct identifiers should not be released and therefore should be removed from the microdata set.</li>
<li>The indirect identifiers are subdivided into extremely identifying variables, very identifying variables and identifying variables. Only direct regional variables are considered to be extremely identifying. Very identifying variable are very visible variables like gender, ethnicity etc. Each combination of values of an extremely identifying variable, a very identifying variable and an identifying variable should occur at least 100 times in the population.</li>
<li>The maximum level of detail for occupation, firm and level of education is determined by the most detailed direct regional variable. This rule does not replace rule 2, but is instead a practical extension of that rule.</li>
<li>A region that can be distinguished in the microdata should contain at least 10&nbsp;000 inhabitants.</li>
<li>If the microdata concern panel data direct regional data should not be released. This rule prevents the disclosure of individual information by using the panel character of the microdata.</li>
</ol>
<p>If these rules are violated, global recoding and local suppression are applied to achieve a safe file. Both global recoding and local suppression lead to information loss, because either less detailed information is provided or some information is not given at all. A balance between global recoding and local suppression should always be found in order to make the information loss due to the statistical disclosure control measures as low as possible. It is recommended to start by recoding some variables globally until the number of unsafe combinations that has to be protected is sufficiently low. Then the remaining unsafe combinations have to be protected by local suppressions.</p>
<p>For business microdata these rules are not appropriate. Opposite to personal microdata business data tends to be much more skewed. Each business is much more visible in a microdata set. This makes it very hard to make a safe business micro dataset.</p>
<p><em>Microdata for the general public</em><br>
The software package <span class="math inline">\(\mu\)</span>-ARGUS (see <em>e.g.</em> Hundepool et al, 2014) is also of help in producing public use microdata files. For public use microdata files one could use the following set of rules:</p>
<ol type="1">
<li>The microdata must be at least one year old before they may be released.</li>
<li>Direct identifiers should not be released. Also direct regional variables, nationality, country of birth and ethnicity should not be released.</li>
<li>Only one kind of indirect regional variables (<em>e.g.</em> the size class of the place of residence) may be released. The combinations of values of the indirect regional variables should be sufficiently scattered, <em>i.e.</em> each area that can be distinguished should contain at least 200&nbsp;000 persons in the target population and, moreover, should consist of municipalities from at least six of the twelve provinces in the Netherlands. The number of inhabitants of a municipality in an area that can be distinguished should be less than 50&nbsp;% of the total number of inhabitants in that area.</li>
<li>The number of identifying variables in the microdata is at most 15.</li>
<li>Sensitive variables should not be released.</li>
<li>It should be impossible to derive additional identifying information from the sampling weights.</li>
<li>At least 200&nbsp;000 persons in the population should score on each value of an identifying variable.</li>
<li>At least 1&nbsp;000 persons in the population should score on each value of the crossing of two identifying variables.</li>
<li>For each household from which more than one person participated in the survey we demand that the total number of households that correspond to any particular combination of values of household variables is at least five in the microdata.</li>
<li>The records of the microdata should be released in random order.</li>
</ol>
<p>According to this set of rules the public use files are protected much more severely than the microdata for research. Note that for the microdata for research it is necessary to check certain trivariate combinations of values of identifying variables and for the public use files it is sufficient to check bivariate combinations, but the thresholds are much higher. However, for public use files it is not allowed to release direct regional variables. When no direct regional variable is released in a microdata set for research, then only some bivariate combinations of values of identifying variables should be checked according to the statistical disclosure control rules. For the corresponding public use files all the bivariate combinations of values of identifying variables should be checked.</p>
</section>
<section id="sec-further-example" class="level2 page-columns page-full" data-number="3.8">
<h2 data-number="3.8" class="anchored" data-anchor-id="sec-further-example"><span class="header-section-number">3.8</span> Further examples</h2>
<p>In this section we provide examples based on real surveys of the various steps described in the previous sections in order to describe a possible process of microdata anonymisation in practice. The two surveys analysed are one from the social domain, the Labour Force Survey (LFS), and one on business data, the Community Innovation Survey (CIS). Notice how the complexity of the reasoning on business data may rise sharply as compared to social microdata.</p>
<section id="labour-force-survey" class="level3 page-columns page-full" data-number="3.8.1">
<h3 data-number="3.8.1" class="anchored" data-anchor-id="labour-force-survey"><span class="header-section-number">3.8.1</span> Labour Force Survey</h3>
<p>The Labour Force Survey<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> is one of the surveys subject to Regulation EC 831/2002 on access to microdata for scientific purposes. The Labour Force Survey (LFS) is the main data source for the analysis of the labour market, employment, unemployment as well as the conditions and the level of involvement at the labour market. Some of the main observed variables are:</p>
<div class="no-row-height column-margin column-container"><p><sup>3</sup>&nbsp;The European Union Labour Force Survey (EU LFS)? is conducted in the 25 Member States of the European Union and 3 countries of the European Free Trade Association (EFTA) in accordance with Council Regulation (EEC) No.&nbsp;577/98 of 9 March 98: <a href="https://eur-lex.europa.eu/legal-content/EN/ALL/?uri=celex%3A31998R0577">https://eur-lex.europa.eu/legal-content/EN/ALL/?uri=celex%3A31998R0577</a></p></div><ol type="1">
<li>demographic variable: (gender, year of birth, marital status, relationship to reference person, place of residence);</li>
<li>labour status: (labour status during the reference week, etc.);</li>
<li>employment characteristics of the main job: (professional status, economic activity of local unit, country of place of work, etc.);</li>
<li>education and training;</li>
<li>income;</li>
<li>technical item relating to the interview.</li>
</ol>
<p>The sampling designs in the EU-LFS are extremely varied. Most NSIs employ some kind of multistage stratified random sampling design, especially those that do not have central population register available.</p>
<p>In the document Eurostat (2004) a proposal for the anonymisation of the LFS is presented. Here we show a possible approach to disclosure scenario definition that leads to the definition of the identifying variables.</p>
<p><em>Disclosure scenarios</em><br>
A spontaneous identification (see <a href="#sec-disclosure-risk-scenarios" class="quarto-xref"><span>Section 3.3.2</span></a>) can happen when an intruder has a direct knowledge of some statistical units belonging to the sample and, whether such units assume extremely particular values for some variables or for some combinations. In the Labour Force data set there are several variables that may lead to a spontaneous identification. Some of these variables are: professional status, number of persons working at local unit, income, economic activity, etc. To avoid a possible spontaneous identification such variables are usually checked to see whether there are unusual patterns or very rare keys and, if necessary some recoding or suppression may be suggested.</p>
<p>Also the external register scenario could be considered for the LFS data. The two external archives taken as example in the Italian study are: (i) the Electoral roll for the Individual archive scenario, and (ii) the Population register for the Household archive scenario (see <a href="#sec-disclosure-risk-scenarios" class="quarto-xref"><span>Section 3.3.2</span></a>). The <em>Electoral roll</em> is a register containing information on people having electoral rights, mainly demographic variables (gender, age, place of residence and birth) and sometimes variables such as, marital status, professional and/or educational information. The key variables considered as reliable for re‑identification attempts under this scenario are: gender, age and residence. Place of birth is removed from the MF and the others are not considered as their quality was not deemed sufficient for re-identification purposes. The <em>Population register</em> maybe a public register containing demographic information at individual and household level. Particularly, the set of key variables considered for the household archive scenario comprises the variables: gender, age, place of residence and marital status as individual information, the household size and parental relationship as household information.</p>
<p><em>Risk assessment</em><br>
The definition of the identifying variables allows for a definition of risk assessment. This can be performed as in Eurostat (2004) following the reasoning of <a href="#sec-intro-example" class="quarto-xref"><span>Section 3.7</span></a> or other risk measures can be used. If the survey employs a complex multi-staged stratified random sample design possibly with calibration, then the ARGUS individual risk may be used especially when hierarchical information on the household need to be released.</p>
<p><em>Protection</em><br>
The risk assessment procedure will show the keys at risk and based on this information a strategy for microdata protection needs to be adopted. If the number of keys at risk is very large then some variables are too detailed and some global recoding is advisable in order to avoid the application of a high percentage of local suppressions. If the keys at risk are particularly concentrated on certain values of an identifying variable a local recoding of such variable could be sufficient to solve the problem.</p>
</section>
<section id="community-innovation-survey" class="level3 page-columns page-full" data-number="3.8.2">
<h3 data-number="3.8.2" class="anchored" data-anchor-id="community-innovation-survey"><span class="header-section-number">3.8.2</span> Community Innovation Survey</h3>
<p>The Community Innovation Survey is one of the surveys subject to Regulation CE 831/2002 on access to microdata for scientific purpose. A lot of effort has been put in anonymising this microdata set, see for example Eurostat (2006).</p>
<p>In this section we propose a study of disclosure scenarios to define identifying variables and a risk assessment analysis to single out the records at risk for the Community Innovation Survey (CIS) based on Ichim (2006). A protection stage is then outlined giving different choices. The interested reader is referred to that paper for more information on the whole process.</p>
<p>CIS provides information on the characteristics of innovation activity at enterprise level<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a>. The CIS statistical population is determined by the size of the enterprise (all enterprises with 10 or more employees) and its principal economic activity.</p>
<div class="no-row-height column-margin column-container"><p><sup>4</sup>&nbsp;Some of the main observed variables in the CIS3 are : principal economic activity, geographical information, number of employees in 1998 and 2000, turnover in 1998 and 2000, exports in 1998 and 2000, gross investment in tangible goods: 2000, number of valid patents at end of 2000, number of employees with higher education (in line with the number of employees in 2000), expenditure in intramural RD (in line with the turnover in 2000), expenditure in extramural RD (in line with the turnover in 2000), expenditure in acquisition of machinery (in line with the turnover in 2000), expenditure in other external knowledge (in line with the turnover in 2000), expenditure in training, market (in line with the turnover in 2000), total innovation expenditure (in line with the turnover in 2000), number of persons involved in intra RD (in line with the number of employees in 2000).</p></div><p><em>Disclosure scenario</em><br>
Since generally business registers are publicly available, it is supposed that an intruder could use such information to identify an enterprise. Public business registers report general information on name, address, turnover (TURN), number of employees (EMP), principal activity of an enterprise (NACE), region (NUTS). Therefore, the identifying variables of the hypothesized disclosure scenario are: economic classification (NACE), region (NUTS), number of employees (EMP) and turnover (TURN). The information content of these variables must be somehow reduced in order to increase the intruder uncertainty. An initial coding performed on such variables was: NACE at 2 digits, Nuts recoded at national level (no regional breakdown) and three enterprise size classes.</p>
<p>Additionally, in the CIS data set there are several confidential variables that may be subject to spontaneous identification. Some examples are total expenditure on innovation (RTOT), exports, number of persons involved in intra RD, etc. Such variables are never published in an external register, but they can assume extremely particular values on some units. Mere additional information would then clearly identify an enterprise. Special attention must be paid on these variables. A check performed by the <em>survey experts</em> is generally suggested. These assessments must be performed with respect to each combination of categorical identifying variables to be released. The analysis by the survey expert suggested to remove from the data to be released the variable Country of head office. With the given details on NACE, size class and NUTS all the other continuous variables were not deemed sufficiently spread to lead to a spontaneous identification of a unit. For this reason it maybe suggested to let them unchanged.</p>
<p><em>Risk assessment</em><br>
A unit is considered at risk if it is ‘recognisable’ either in the external register scenario or in the spontaneous identification scenario. It is assumed that an intruder may confuse a unit U with others when there is a sufficient number of units in a well-defined (and not too large) neighbourhood of U. The anonymisation proposal developed in Ichim (2006) is based on the idea that similarity based on clusters and confusion both express the same concept, although in different frameworks. : When a unit belongs to a cluster, it belongs to a high density (sufficient number of close units) subset of data. Hence the unit may be considered as being confused with others. The algorithms taking into account these two features (distance from each other and number of neighbours) are called density based algorithms and Ichim (2006) uses one of these algorithms to identify isolated units <em>i.e.</em> units at risk with respect to the identifying variables.</p>
<p><em>Protection by perturbation</em><br>
Once the units at risk have been identified, protection should be applied. Several different proposals in the field of data perturbation methods are possible. The proposal by Eurostat protection is achieved by the application to the main continuous variables in the data set of individual ranking and some local suppression of particular values. This microaggregation would be applied to the whole file irrespective to different economic classifications or size classes and without taking into account possible relationships between variables (for example turnover needs to be greater than export or expenditures). This strategy is perfectly acceptable if a slight modification of the data is deemed sufficient.</p>
<p>An alternative could be to apply a perturbation only to these records at risk (mainly the large size enterprises in single NACE 2 digits) whereas the rest of the file is released unchanged. Ichim (2006) suggests different perturbations of the points at risk whereas these are in the middle of the distribution of points (nearest cluster imputation) or if they are in the tail (microaggregation). A further adjustment is proposed in order to preserve turnover totals for each combination of categorical identifying variables. This is deemed important by users who need to compare results with published tables. A study of the information loss of this approach is presented in Ichim (2006).</p>
</section>
<section id="references-6" class="level3" data-number="3.8.3">
<h3 data-number="3.8.3" class="anchored" data-anchor-id="references-6"><span class="header-section-number">3.8.3</span> References</h3>
<p>Eurostat (2004). <em>Proposal on anonymised LFS microdata.</em> CSC 2004/B5/ item 2.2.2.</p>
<p>Eurostat (2006). <em>The CIS4. An amended version of the micro-data anonymisation method</em>. Doc. Eurostat/F4/STI/CIS/M2/8.</p>
<p>Ichim, D. (2006). <em>Microdata anonymisation of the Community Innovation Survey: a density based clustering approach for risk assessment.</em> Contribution Istat. Shortly available from <a href="https://www.istat.it/wp-content/uploads/2018/07/2007_2-1.pdf">https://www.istat.it/wp-content/uploads/2018/07/2007_2-1.pdf</a></p>
<p>Trottini, M., Franconi, L. and Polettini, S. (2006). <em>Italian Household Expenditure Survey: A proposal for Data Dissemination.</em> In Domingo Ferrer, J and Franconi, L. (eds) Privacy in Statistical Databases, CENEX-SDC Project International Conference, Rome, Italy, December 2006, 318-333.</p>


</section>
</section>


</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./02-regulations.html" class="pagination-link  aria-label=" &lt;span="" in="" legal="" acts="" and="" ethical="" codes&lt;="" span&gt;"="">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Confidentiality in legal acts and ethical codes</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./04-magnitude-tabular-data.html" class="pagination-link" aria-label="<span class='chapter-number'>4</span>&nbsp; <span class='chapter-title'>Magnitude tabular data</span>">
        <span class="nav-page-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Magnitude tabular data</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>