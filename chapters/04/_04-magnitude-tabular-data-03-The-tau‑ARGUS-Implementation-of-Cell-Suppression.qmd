## The $\tau$‑ARGUS Implementation of Cell Suppression {#sec-tau-argus-suppression}

The software package $\tau$‑ARGUS provides software tools for disclosure protection methods for tabular data. This can be achieved by modifying the table so that it contains less, or less detailed, information. $\tau$‑ARGUS allows for several modifications of a table: a table can be redesigned, meaning that rows and columns can be combined; sensitive cells can be suppressed and additional cells to protect these can be found in some optimal way (secondary cell suppression\index{cell suppression}). Several alternative algorithms for the selection of secondary suppressions are available in $\tau$‑ARGUS, *e.g.* the Hypercube/GHMiter method, a Modular and a Full optimal solution, and a method based on a network flow algorithm. Instead of cell suppression\index{cell suppression} one could also use other methods for disclosure control of tables. One of these alternative methods is Controlled Rounding. However, use of Controlled Rounding is more common for frequency tables.

![Overview of the $\tau$‑ARGUS architecture](Images/media/tau_argus_overview.png){#fig-tau-argus-overview}

We start this section by introducing the basic issues related to the question of how to properly set up a tabular data protection problem for a given set of tables that are intended to be published, discussing tabular data structures, and particularly tables with hierarchical structures. @sec-evaluation-of-secondary-cell-suppression-algorithms-offered-by-tau-argus provides an evaluation of algorithms for secondary cell suppression\index{cell suppression} offered by $\tau$-ARGUS, including a recommendation on which tool to use in which situation. After that, in @sec-efficient-protection-process we give some guidance on processing table protection efficiently, especially in the case of linked tables. We finish the section with an introductive example in @sec-introductive-example.

The latest version of $\tau$-ARGUS can be found on GitHub ([https://github.com/sdcTools/tauargus/releases](https://github.com/sdcTools/tauargus/releases)).

### Setting up a Tabular Data Protection Problem in Practice {#sec-setting-up-a-tabular-data-protection-problem-in-practice}

At the beginning of this section we explain the basic concept of tables in the context of tabular data protection, and complex structures like hierarchical and linked tables. In order to find a good balance between protection of individual response data and provision of information -- in other words, to take control of the loss of information that obviously cannot be avoided completely because of the requirements of disclosure control, it is necessary to somehow rate the information loss connected to a cell that is suppressed. We therefore complete this section by explaining the concept of cell costs to rate information loss.

\
***Table specification***\
For setting up the mathematical formulation of the protection problems connected to the release of tabulations for a certain data set, all the linear relations between the cells of those tabulations have to be considered. This leads us to a crucial question: What is a table anyway? In the absence of confidentiality concerns, a statistician creates a table in order to show certain properties of a data set, or to enhance comparison between different variables. So a single table might literally mix apples and oranges. Secondly, statisticians may wish to present a number of those 'properties', publishing multiple tables from a particular data set. Where does one table end and the next start? Is the ideal table one that fits nicely on a standard-size sheet of paper? With respect to tabular data protection, we have to think of tables in a different way:

Magnitude tables display sums of observations of a quantitative variable, the so-called '*response variable'*. The sums are displayed across all observations and/or within groups of observations. These groups can be formed by grouping the respondents according to certain criteria such as their economic activity, region, size class of turnover, legal form, etc. In that case the grouping of observations is defined by categorical variables observed for each individual respondent, the 'explanatory variables'. It also occurs that observations are grouped by categories of the response variable, for instance fruit production into apples, pears, cherries, etc.

The "*dimension*" of a table is given by the number of grouping variables used to specify the table. "*Margins*" or "*marginal cells*" of a table are those cells, which are specified by a smaller number of grouping variables. The smaller the number of grouping variables, the higher the "*level*" of a marginal cell. A two-dimensional table of some business survey may for instance provide sums of observations grouped by economic activity and company size classes. At the same time it may also display the sums of observations grouped by only economic activity or by only size classes. These are then margins/marginal cells of this table. If a sum across all observations is provided, we refer to it as the "*total*" or "*overall total*".

Note that tables presenting ratios or indexes typically do not define a tabular data protection problem, because there are no additive structures between the cells of such a table, and neither between a cell value, and the values of the contributing units. Think, for instance, of mean wages, where the cell values would be a sum of wages divided by a sum over the number of employees of several companies. However, there may be exceptions: if, for instance, it is realistic to assume that the denominators (say, the number of employees) can be estimated quite closely, both on the company level and on the cell level, then it might indeed be adequate to apply disclosure control methods based, for example, on the enumerator variable (in our instance: the wages).

\
***Hierachical, linked tables***\
Data collected within government statistical systems must meet the requirements of many users, who differ widely in the particular interest they take in the data. Some may need community-level data, while others need detailed data on a particular branch of the economy but no regional detail. As statisticians, we try to cope with this range of interest in our data by providing the data at several levels of detail. We usually combine explanatory variables in multiple ways, when creating tables for publication. If two tables presenting data on the same response variable share some categories of at least one explanatory variable, there will be cells which are presented in both tables -- those tables are said to be *linked* by the cells they have in common. In order to offer a range of statistical detail, we use elaborate classification schemes to categorize respondents. Thus, a respondent will often belong to various categories of the same classification scheme ‑ for instance a particular community within a particular county within a particular state ‑ and may thus fall into three categories of the regional classification.

The structure between the categories of hierarchical variables also implies sub-structure for the table. When, in the following, we talk about sub-tables without substructure, we mean a table constructed in the following way:

For any explanatory variable we pick one particular non-bottom-level category (the 'food production sector' for instance). Then we construct a 'sub-variable'. This sub-variable consists only of the category picked in the first step and those categories of the level immediately below belonging to this category (bakers, butchers, etc.). After doing that for each explanatory variable the table specified through a set of these sub-variables is free from substructure then, and is a sub-table of the original one. Any cell within the sub-table does also belong to the original table. Many cells of the original table will appear in more than one sub-table: The sub-tables are linked. Example 6 provides a simple instance of a one-dimensional table with hierarchical structure.

:::{.callout-note appearance="simple"}
**Example 6** 
Assume the categories A, AA, AB, AB1, AB2, and AB3 of some variable EXP resemble a hierarchical structure as depicted in @fig-hierarchical-structure below:

![Hierarchical structure](Images/media/chap4_hierarchical_structure.png){#fig-hierarchical-structure}

Let the full table present turnover by the categories of EXP. Then the two subtables of this table will be:

:::{layout="[[-1,1,-1,1,-1], [1]]"}

|       |        |          |
|:------|:-------|:---------|
| EXP   |        | Turnover |
| **A** |        |          |
|       | **AA** |          |
|       | **AB** |          |

|        |         |          |
|:-------|:--------|:---------|
| EXP    |         | Turnover |
| **AB** |         |          |
|        | **AB1** |          |
|        | **AB2** |          |

:::
Note that cell **AB** appears in both subtables

:::

Of course, when using cell suppression\index{cell suppression} methods to protect a set of linked tables, or sub-tables, it is not enough to select secondary suppressions for each table (or sub-table) separately. Otherwise it might for instance happen that the same cell is suppressed in one table because it is used as secondary suppression, while within another table it remains unsuppressed. A user comparing the two tables would then be able to disclose confidential cells in the first table. A common approach is to protect tables separately, but note any complementary suppression belonging also to one of the other tables; suppress it in this table as well, and repeat the cell suppression\index{cell suppression} procedure for this table. This approach is called a '*backtracking procedure*'. Although within a backtracking process for a hierarchical table the cell-suppression procedure will usually be repeated several times for each sub-table, the number of computations required for the process will be much smaller than when the entire table is protected all at once.

It must however be stressed, that a backtracking procedure is not global according to the denotation in Cox (2001). See Cox (2001) for discussion of problems related to non-global methods for secondary cell suppression\index{cell suppression}.

For mathematical formulation of linked tables structures see de Wolf (2007) and de Wolf, Giessing (2008).

\
***Cell costs***\
The challenge of tabular data protection is to preserve as much information in the table as possible, while creating the required uncertainty about the true values of the sensitive cells, as explained in the previous section. It is necessary to somehow rate the information content of data, in order to be able to express the task of selecting an 'optimal set' of secondary suppressions, or, alternatively, of adjusting a table in an optimal way, as mathematical programming problem. Information loss is expressed in these mathematical models as the sum of costs associated to the secondary suppressions, or non-sensitive cells subject to adjustment.

For cell suppression\index{cell suppression}, the idea of equating a minimum loss of information with the smallest number of suppressions is probably the most natural concept. This would be implemented technically by assigning identical costs to each cell. Yet experience has shown that this concept often yields a suppression pattern in which many large cells are suppressed, which is undesirable. In practice other cost functions, based on cell values, or power transformations thereof, or cell frequencies yield better results. Note that several criteria, other than the numeric value, may also have an impact on a user's perception of a particular cells importance, such as its situation within the table (totals and sub-totals are often rated as highly important), or its category (certain categories of variables are often considered to be of secondary importance). $\tau$-ARGUS offers special facilities to choose a cost function and to carry out power transformations of the cost function:

\

***Cost function***\
The cost function indicates the relative importance of a cell. In principle the user could attach a separate cost to each individual cell, but that will become a rather cumbersome operation. Therefore in $\tau$‑ARGUS a few standard cost functions are available[^7]:

-   Unity, *i.e.* all cells have equal weight (just minimising the number of suppressions)
-   The number of contributors (minimising the number of contributors to be hidden)
-   The cell value itself (preserving as much as possible the largest -- usually most important -- cells)
-   The cell value of another variable in the dataset (this will be the indication of the important cells).

[^7]: One of the secondary cell suppression\index{cell suppression} methods of $\tau$-ARGUS, the hypercube method, uses an elaborate internal weighting scheme in order to avoid to some extent the suppression of totals and subtotals which is essential for the performance of the method. In order to avoid that this internal weighting scheme is thrown out of balance, it has been decided to make the cost function facilities of t-ARGUS inoperational for this method.

:::{.callout-warning collapse="true"}
## Expert level
Especially in the latter two cases the cell value could give too much preference to the larger cells. Is a 10 times bigger cell really 10 times as important? Sometimes there is a need to be a bit more moderate. Some transformation of the cost function is desired. Popular transformations are square-root or a log-transformation. Box and Cox (1960) have proposed a system of power transformations:

$$
y = \frac{x^{\lambda} -1}{\lambda}
$$

In this formula $\lambda = 0$ yields to a log-transformation and the $-1$ is needed for making this formula continuous in $\lambda$.

This last aspect and the fact that this could lead to negative values we have introduced a simplified version of the lamda-transformation in $\tau$‑ARGUS

$$
y = \begin{cases}
x^{\lambda} \quad &\text{for }\lambda\neq0 \\ 
\text{log}(x)\quad &\text{for }\lambda=0
\end{cases}
$$

So choosing $\lambda = \frac{1}{2}$ will give the square-root.

:::

### Evaluation of Secondary Cell Suppression algorithms offered by $\tau$‑ARGUS {#sec-evaluation-of-secondary-cell-suppression-algorithms-offered-by-tau-argus}

The software package $\tau$−ARGUS offers a variety of algorithms to assign secondary suppressions which will be described in detail in @sec-methodology-secondary-suppression-tau-argus. While some of those algorithms are not yet fully developed, or not yet fully integrated into the package, respectively, two are indeed powerful tools for automated tabular data protection. In this section we focus on those latter algorithms. We compare them with respect to quality aspects of the protected tables, while considering also some practical issues. Based on this comparison, we finally give some recommendation on the use of those algorithms.

We begin this section by introduction of certain minimum protection standards PS1, and PS2. We explain why it is a 'must' for an algorithm to satisfy those standards in order to be useable in practice. For those algorithms that qualify 'useable' according to these criteria, we report some results of studies comparing algorithm performance with regard to information loss, and disclosure risk\index{disclosure risk}.

In @sec-secondary-protection-methods we have defined criteria for a safe suppression pattern. It is of course possible to design algorithms that will ensure that no suppression pattern is considered feasible unless all of these criteria are satisfied. In practice, however, such an algorithm would either require too much computer resource (in particular: CPU time) when applied to the large, detailed tabulations of official statistics, or the user would have to put up with a strong tendency for oversuppression. In the following, we therefore define minimum standards for a suppression pattern that is safe w.r.t. the safety rule adopted except for a residual disclosure risk\index{disclosure risk} that might be acceptable to an agency. It should be noted that those standards are definitely superior, or at least equal to what could be achieved by the traditional manual procedures for cell suppression\index{cell suppression}, even if carried out with utmost care! For the evaluation, we consider only those algorithms that ensure validity of the suppression pattern at least with respect to those relaxed standards.

\
***Minimum protection standards***\
For a table with hierarchical substructure, feasibility intervals computed on basis of the set of equations for the full table normally tend to be much closer than those computed on basis of separate sets of equations corresponding to sub-tables without any substructure. Moreover, making use of the additional knowledge of a respondent, who is the single respondent to a cell (a so called *'singleton'*), it is possible to derive intervals that are much closer than without this knowledge[^8].

[^8]: Consider for instance the table of example 3 in @sec-secondary-protection-methods. Without additional knowledge the bounds for cell (1,1) are $X_{11}^\text{min} = 3$ and $X_{11}\text{max} = 6$. However, if cell (1,2) were a singleton-cell (a cell with only a single contributor) then this single contributor could of course exactly compute the cell value of cell (1,1) by subtracting her own contribution from the row total of row 1.

Based on the assumption of a simple but probably more realistic disclosure scenario where intruders will deduce feasibility intervals separately for each sub-table, rather than taking the effort to consider the full table, and that single respondents, using their special additional knowledge, are more likely attempting to reveal other suppressed cell values when they are within the same row or column (more general: relation), the following minimum protection standards (PS) make sense:

> (PS1) *Protection against exact external disclosure*:\
> With a valid suppression pattern, it is not possible to disclose the value of a sensitive cell exactly, if no additional knowledge (like that of a singleton) is considered, and if subsets of table equations are considered separately, *i.e.* when feasibility intervals are computed separately for each sub-table.
>
> (PS2) *Protection against singleton disclosure*:\
> A suppression pattern, with only two suppressed cells within a row (or column) of a table is not valid, if each of the two corresponds to a single respondent who are not identical.
>
> (PS1\*) extension of (PS1) for inferential (instead of exact) disclosure,
>
> (PS2\*) extension of (PS2), covering the more general case where a single respondent can disclose another single respondent cell, not necessarily located within the same row (or column) of the table.

$\tau$-ARGUS offers basically two algorithms, Modular and Hypercube, both satisfying the above-mentioned minimum standards regarding disclosure risk\index{disclosure risk}. Both take a backtracking approach to protect hierarchical tables within an iterative procedure: they subdivide hierarchical tables into sets of linked, unstructured tables. The cell suppression\index{cell suppression} problem is solved for each subtable separately. Secondary suppressions are coordinated for overlapping subtables.

The Modular approach, a.k.a. HiTaS algorithm (see de Wolf, 2002), is based on the optimal solution as implemented by Fischetti and Salazar-González. By breaking down the hierarchical table in smaller non-structured subtables, protecting these subtables and linking the subtables together by a backtracking procedure the whole table is protected. For a description of the modular approach see @sec-modular, the optimal Fischetti/Salazar-González method is described in section @sec-optimal. See also Fischetti and Salazar-González (2000).

The Hypercube is based on a hypercube heuristic implemented by Repsilber (2002). See also the descriptions of this method in @sec-hypercube.

Note that in the current implementation of $\tau$-ARGUS, use of the optimization tools requires a license for additional commercial software (LP-solver), whereas use of the hypercube method is free. While Modular is only available for the Windows platform, of Hypercube there are also versions for Unix, IBM (OS 390) and SNI (BS2000).

Both Modular and Hypercube provide sufficient protection according to standards PS1\* (protection against inferential disclosure) and PS2 above. Regarding singleton disclosure, Hypercube even satisfies the extended standard PS2\*. However, simplifications of the heuristic approach of Hypercube cause a certain tendency for oversuppression. This evaluation therefore includes a relaxed variant that, instead of PS1\*, satisfies only the reduced standard PS1, *i.e.* zero protection against inferential disclosure. We therefore refer to this method as Hyper0 in the following. Hyper0 processing can be achieved simply by deactivating the option "Protection against inferential disclosure required" when running Hypercube out of $\tau$‑ARGUS.

::: {.content-visible unless-format="pdf"}
+:------------------------------------+:-----------------------+:--------------+:-----------+
|  **Algorithm**                      | **Modular**            | **Hypercube** | **Hyper0** |
+-------------------------------------+------------------------+---------------+------------+
| **Procedure for secondary**         | **Fischetti/Salazar**  |  **Hypercube**             |
|     **suppression**                 |    **optimization**    |  **heuristic**             |
+----------------+--------------------+------------------------+---------------+------------+
| **Protection** | **Interval/Exact** | PS1*                   |  PS1*         |  PS1       |
|  **standard**  |   **disclosure**   |                        |               |            |
|                +--------------------+------------------------+---------------+------------+
|                |    **Singleton**   | PS2                    |  PS2*         |  PS2*      |
+----------------+--------------------+------------------------+---------------+------------+

: Algorithms for practical use {#tbl-algorithms}

:::

::: {.content-visible when-format="pdf"}

\begin{table}
\begin{tabular}{|cc|c|cc|}
\hline
\multicolumn{2}{|c|}{Algorithm}                                                                                                                                              & Modular                                                                                    & \multicolumn{1}{c|}{Hypercube}                                & Hyper0                               \\ \hline
\multicolumn{2}{|c|}{\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Procedure for secondary\\ suppression\end{tabular}}}                                                         & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Fischetti/Salazar \\ optimization\end{tabular}} & \multicolumn{2}{c|}{\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Hypercube \\ heuristic\end{tabular}}} \\
\multicolumn{2}{|c|}{}                                                                                                                                                       &                                                                                            & \multicolumn{2}{c|}{}                                                                                \\ \hline
\multicolumn{1}{|c|}{\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Protection\\  standard\end{tabular}}} & \begin{tabular}[c]{@{}c@{}}Interval/Exact \\ discolsure\end{tabular} & PS1*                                                                                       & \multicolumn{1}{c|}{PS1*}                                     & PS1                                  \\ \cline{2-5} 
\multicolumn{1}{|c|}{}                                                                                & Singleton                                                            & PS2                                                                                        & \multicolumn{1}{c|}{PS2*}                                     & PS2                                  \\ \hline
\end{tabular}
\caption{Algorithms for practical use}
\label{tbl-algorithms}
\end{table}

:::

In the following, we briefly report results of two evaluations studies, in the following referred to as study A, and study B. For further detail on these studies see Giessing (2004) for study A, and Giessing et al. (2006) for study B. Both studies are based on tabulations of a strongly skewed magnitude variable.

For study A, we used a variety of hierarchical tables generated from the synthetic micro‑data set supplied as CASC deliverable 3-D3. In particular, we generated 2- and 3-dimensional tables, where one of the dimensions had a hierarchical structure. Manipulation of the depth of this hierarchy resulted in 7 different tables, with a total number of cells varying between 460 and 150,000 cells. Note that the Hyper0 method was not included in study A.

For study B, which was carried out on behalf of the German federal and state statistical offices, we used 2-dimensional tables of the German turnover tax statistics. Results have been derived for tables with 7-level hierarchical structure (given by the NACE economy classification) of the first dimension, and 4‑level structure of the second dimension given by the variable Region.

We compare the loss of information due to secondary suppression in terms of the number and cell values of secondary suppression, and analyze the disclosure risk\index{disclosure risk} of the protected tables. @tbl-results-study-A reports the results regarding the loss of information obtained in study A.

::: {.content-visible unless-format="pdf"}

+-------------+-----------+---------------------+---------------------+
| Table Hier. | No. Cells |  No. Suppressions   |    Added Value of   |
|    levels   |           |         (%)         |   Suppressions (%)  |
|             |           +-----------+---------+-----------+---------+
|             |           | Hypercube | Modular | Hypercube | Modular |
+=============+===========+===========+=========+===========+=========+
|  **2-dimensional tables**                                           |
+------+------+-----------+-----------+---------+-----------+---------+
|   1  |   3  |    460    |    6.96   |   4.35  |    0.18   |   0.05  |
+------+------+-----------+-----------+---------+-----------+---------+
|   2  |   4  |   1,050   |   10.95   |   8.29  |    0.98   |   0.62  |
+------+------+-----------+-----------+---------+-----------+---------+
|   3  |   6  |   8,230   |   14.92   |  11.48  |    6.78   |   1.51  |
+------+------+-----------+-----------+---------+-----------+---------+
|   4  |   7  |   16,530  |   14.97   |  11.13  |    8.24   |   2.12  |
+------+------+-----------+-----------+---------+-----------+---------+
|  **3-dimensional tables**                                           |
+------+------+-----------+-----------+---------+-----------+---------+
|   5  |   3  |   8,280   |   14.63   |  10.72  |    6.92   |   1.41  |
+------+------+-----------+-----------+---------+-----------+---------+
|   6  |   4  |   18,900  |   17.31   |  15.41  |   12.57   |   3.55  |
+------+------+-----------+-----------+---------+-----------+---------+
|   7  |   6  |  148,140  |   15.99   |  10.63  |   23.16   |   3.91  |
+------+------+-----------+-----------+---------+-----------+---------+

: Results of study A - Number and value of secondary suppressions {#tbl-results-study-A}

:::

::: {.content-visible when-format="pdf"}

\begin{table}
\begin{tabular}{crrrrrr}
\multicolumn{2}{c}{\multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}Table\\ Hier.\\  levels\end{tabular}}}} & \multirow{2}{*}{\textbf{No. Cells}} & \multicolumn{2}{c}{\textbf{No. Suppressions (\%)}} & \multicolumn{2}{c}{\textbf{\begin{tabular}[c]{@{}c@{}}Added Value of \\ Suppressions (\%)\end{tabular}}} \\ \cmidrule(lr){4-5} \cmidrule(lr){6-7}
\multicolumn{2}{c}{}                                                                                          &                                     & \textbf{Hypercube}        & \textbf{Modular}       & \textbf{Hypercube}                                   & \textbf{Modular}                                  \\ 
\multicolumn{7}{c}{2-dimensional tables}  \\
1                                                     & 3                                                     & 460                                 & 6.96                      & 4.35                   & 0.18                                                 & 0.05                                              \\
2                                                     & 4                                                     & 1,050                               & 10.95                     & 8.29                   & 0.98                                                 & 0.62                                              \\
3                                                     & 6                                                     & 8,230                               & 14.92                     & 11.48                  & 6.78                                                 & 1.51                                              \\
4                                                     & 7                                                     & 16,530                              & 14.97                     & 11.13                  & 8.24                                                 & 2.12                                              \\
\multicolumn{7}{c}{3-dimensional tables}                                                                                                                                                                                                                                                                            \\
5                                                     & 3                                                     & 8,280                               & 14.63                     & 10.72                  & 6.92                                                 & 1.41                                              \\
6                                                     & 4                                                     & 18,900                              & 17.31                     & 15.41                  & 12.57                                                & 3.55                                              \\
7                                                     & 6                                                     & 148,140                             & 15.99                     & 10.63                  & 23.16                                                & 3.91                                             
\end{tabular}
\caption{ Results of study A - Number and value of secondary suppressions }
\label{tbl-results-study-A}
\end{table}

:::


@tbl-res-B presents information loss results from study B on the two topmost levels of the second dimension variable Region, *e.g.* on the national, and state level.

::: {.content-visible unless-format="pdf"}

+-----------------+-------------------------------+-------------+
|  Algorithm      |  Number                       | Added Value |
|     for         +--------------+----------------+-------------+
|  secondary      |  State level | National level |   overall   |
| suppression     +-------+------+--------+-------+-------------+
|                 |  abs  |   %  |   abs  |   %   |  %          |
+=================+=======+======+========+=======+=============+
| **Modular**     | 1,675 | 10.0 |    7   |  0.6  |  2.73       |
+-----------------+-------+------+--------+-------+-------------+
| **Hyper0**      | 2,369 | 14.1 |    8   |  0.7  |  2.40       |
+-----------------+-------+------+--------+-------+-------------+
| **Hypercube**   | 2,930 | 17.4 |   22   |  1.9  |  5.69       |
+-----------------+-------+------+--------+-------+-------------+

: Results of study B - Number and value of secondary suppressions {#tbl-res-B}

:::

::: {.content-visible when-format="pdf"}

\begin{table}
\begin{tabular}{cccccc}
\multirow{3}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}Algorithm for \\ secondary \\ suppression\end{tabular}}} & \multicolumn{4}{c}{\textbf{Number}}                                                    & \textbf{Added Value} \\
                                                                                                            & \multicolumn{2}{c}{\textbf{State level}} & \multicolumn{2}{c}{\textbf{National level}} & \textbf{overall}     \\
                                                                                                            & \textbf{abs}        & \textbf{\%}        & \textbf{abs}          & \textbf{\%}         & \textbf{\%}          \\
Modular                                                                                                     & 1,675               & 10.0               & 7                     & 0.6                 & 2.73                 \\
Hyper0                                                                                                      & 2,369               & 14.1               & 8                     & 0.7                 & 2.40                 \\
Hypercube                                                                                                   & 2,930               & 17.4               & 22                    & 1.9                 & 5.69                
\end{tabular}
\caption{Results of study B - Number and value of secondary suppressions}
\label{tbl-res-B}
\end{table}

:::

Both studies show that best results are achieved by the modular method, while using either variant of the hypercube method leads to an often quite considerable increase in the amount of secondary suppression compared to the result of the modular method. The size of this increase varies between hierarchical levels of the aggregation: on the higher levels of a table the increase tends to be even larger than on the lower levels. Considering the results of study B presented in @tbl-res-B, on the national level we observe an increase of about 214% for Hypercube (14% for Hyper0), compared to an increase of 75% (41%) on the state level. In Giessing et al. (2006) we report also results on the district level. On that level the increase was about 28% on the higher NACE levels for Hypercube (9% for Hyper0), and 20 % for Hypercube (14% for Hyper0) on the lower levels. In study A we observed increases mostly around 30% for Hypercube, for the smallest 2‑di­mensional, and the largest 3-dimensional instance even of 50% and 60%, respectively. In particular, our impression is that the hypercube method tends to behave especially badly (compared to Modular), when tabulations involve many short equations, with only a few positions.

A first conclusion from the results of study B reported above was, because of the massive oversuppression, to exclude the Hypercube method in the variant that prevents inferential disclosure from any further experimentation. Although clearly only second-best performer, testing with Hyper0 was to be continued, because of technical and cost advantages mentioned above.

In experiments with state tables where the variable Region was taken down to the community level, we found that this additional detail in the table caused an increase in suppression at the district level of about 70 -- 80% with Hyper0 and about 10 -- 30% with Modular.

\
***Disclosure risk***\
As explained in @sec-secondary-protection-methods, in principle, there is a risk of inferential disclosure, when the bounds of the feasibility interval of a sensitive cell could be used to deduce bounds on an individual respondent's contribution that are too close according to the method employed for primary disclosure risk\index{disclosure risk} assessment. However, for large, complex structured tables, this risk is rather a risk potential, not comparable to the disclosure risk\index{disclosure risk} that would result from a publication of that cell. After all, the effort connected to the computation of feasibility intervals based on the full set of table equations is quite considerable. Moreover, in a part of the disclosure risk\index{disclosure risk} cases only insiders (other respondents) would actually be able to disclose the individual contribution.

Anyway, with this definition, and considering the full set of table equations, in study B we found between about 4% (protection by Modular) and about 6% (protection by Hyper0) of sensitive cells at risk in an audit step where we computed the feasibility intervals for a tabulation by NACE and state, and for two more detailed tables (down to the district level) audited separately for two of the states. In study A, we found up to about 5.5% (1.3%) cells at risk for the 2-dimensional tables protected by Modular (Hypercube, resp.), and for the 3-dimensional tables up to 2.6% (Modular) and 5.6% (Hypercube).

Of course, the risk potential is much higher for at-risk cells when the audit is carried out separately for each subtable without substructure, because the effort for this kind of analysis is much lower. Because Modular protects according to PS1\* standard, there are no such cells in tables protected by Modular. For Hyper0, in study B we found about 0.4% cells at risk, when computing feasibility intervals for several subtables of a detailed tabulation.

For those subtables, when protected by Modular, for about 0.08% of the singletons, it turned out that another singleton would be able to disclose its contribution. Hyper0 satisfies PS2\* standard, therefore of course no such case was found, when the table was protected by Hyper0.

\
***Recommendation***\
The methods Modular and Hypercube of $\tau$‑ARGUS are powerful tools for secondary cell suppression\index{cell suppression}. Concerning protection against external disclosure both satisfy the same protection standard. However, Modular gives much better results regarding information loss. Even compared to a variant of Hypercube (Hyper0) with relaxed protection standard, Modular performs clearly better. Although longer computation times for this method (compared to the fast hypercube method) can be a nuisance in practice, the results clearly justify this additional effort -- after all, it is possible, for instance, to protect a table with detailed hierarchical structure in two dimensions and more than 800,000 cells within just about an hour. Considering the total costs involved in processing a survey, it would neither be justified to use Hypercube (or Hyper0) only in order to avoid the costs for the commercial license which is necessary to run the optimization tools of Modular.

We also recommend use of Modular to assign secondary suppressions in 3-dimensional tables. However, when those tables are given by huge classifications, long computation times may become an actual obstacle. It took us, for instance, about 11 hours to run a 823 x 18 x 10 table (first dimension hierarchical). The same table was protected by Hyper0 within about 2 minutes. Unfortunately, replacing Modular by Hyper0 in that instance leads to about 28% increase in the number of secondary suppressions.

According to our findings, we would expect the Hypercube method (*i.e.* Hyper0) to be good alternative to Modular, if

-   Tabulations involve more than 2 dimensions and are very large, and
-   the majority of table equations (*e.g.* rows, columns, ...) are long, involving many positions, and
-   the distribution of the variable which is tabulated is rather even, not skewed, resulting in a lower rate of sensitive cells, and
-   protection against inferential disclosure is not an important issue, for instance when a minimum frequency rule instead of a concentration rule is employed to assess the primary disclosure risk\index{disclosure risk}.

In the next section we will give some hints on what to do, if according to these conditions use of the Modular method seems to not be an option, because table sizes are too large, in order to avoid being confined to the use of Hyper or Hyper0.

Concerning protection against singleton (insider) disclosure, the Hypercube method fulfils a higher standard. However, our empirical results confirm that in practice there are very few singleton cells left unprotected by Modular against this kind of disclosure risk\index{disclosure risk}. We are therefore confident that the problem can be fixed easily by a specialized audit step that would not even involve linear programming methods.

### Processing table protection efficiently {#sec-efficient-protection-process}

Within this section we give some guidance for how to facilitate disclosure analysis, and how to use the methods of $\tau$-ARGUS in an efficient way. For more tips and tricks to improve practical performance, see van der Meijden (2006).

Especially in large tables with detailed hierarchical structure we have observed that the choice of the parameter lambda ($\lambda$) may substantially affect the suppression pattern -- and the results may be unexpected. Normally we would expect that constant cell costs will lead to a solution with a (relatively) small number of secondary suppressions. In hierarchical tables, however, it may turn out that given this cost function the Modular method tends to suppress cells in the margins of subtables rather than inner cells which in turn require additional suppressions in a (linked) subtable. This effect may be so strong that in the end, a $\lambda$ close to zero yielding approximately constant costs may lead to a larger number of suppressions as if a larger $\lambda$ (say: 0.5) had been chosen. For larger tables it makes therefore sense to test the outcome with different values of $\lambda$.

\
***Table design***\
In @sec-setting-up-a-tabular-data-protection-problem-in-practice we already discussed how to set up tables for tabular data protection. If the concept for the publication is not yet fixed, some table-redesign may help to reduce the complexity of the problem.

\
***Table redesign***\
If a large number of sensitive cells are present in a table, it might be an indication that the spanning variables are too detailed. In that case one could consider combining certain rows and columns in the table. (This might not always be possible because of publication policy.) Otherwise the number of secondary cell suppressions might just be too enormous. It is a property of the sensitivity rules that a joint cell is safer than any of the individual cells. So as a result of this operation the number of unsafe cells is reduced. One can try to eliminate all unsafe combinations in this way, but that might lead to an unacceptably high information loss. Instead, one could stop at some point, and eliminate the remaining unsafe combinations by using other techniques such as cell suppression\index{cell suppression}.

In practice, it may also often be considered impractical and also not adequate to do the disclosure analysis for each table of the entire publication separately. A well-known simplification is to focus the analysis on certain lead variables. These lead tables are protected with $\tau$‑ARGUS as described above and the pattern found is then used for all the other tables. This strategy has the advantage that it is efficient and prevents the recalculation of suppressed cells by using the inherent relations between the different tables in the publication. The fact that for certain cells in the other tables not always all cells that should be considered unsafe according to the concentration rules will actually be considered unsafe, is considered a minor problem.

A compromise between the copying of the suppression pattern and protecting each table individually is the coordination of the primary suppressions only. This strategy is supported by the concept of shadow variables in $\tau$‑ARGUS. The shadow variable is used for the lead-table and only for finding the primary unsafe cells. After that the pattern is used for each individual table itself and the secondary cell suppression\index{cell suppression} is done in the traditional way.

\
***Shadow variables***\
The whole theory of primary unsafe cells is based on the idea that certain larger contributors in a cell might be at risk and need protection. The largest contributors therefore play an important role in the sensitivity rules.

Often the cell value itself is a very good estimate of the size of the contributors. But sometimes the cell value of a table is not always the best indicator of the size of the company. *E.g.* if the cell value is the investment in some commodity, the largest companies do not always have the largest values. Simply applying the sensitivity rules could yield a situation in which a rather small company will dominate the cell and force it to become unsafe. If we assume that this smaller company is not very well known and visible there is no reason to protect this cell.

If we want to protect the real large companies it could be a good idea to apply the sensitivity rules on another table (*shadow variable*) with the same spanning variables (*e.g.* turnover) and use the pattern of primary unsafe cells. Copy the pattern to the current table and compute the secondary unsafe cells in the normal way.

In $\tau$‑ARGUS we call the table where the primary unsafe cells are computed the shadow table.

Another reason for using a shadow variable could be the coordination of the pattern in different tables with the same spanning variables based on one dataset. See the remarks on periodical datasets below.

\
***Ignore response variable relations***\
A commonly used strategy to simplify disclosure analysis is not to reflect the full linear relationship between published tabulation cells in the table protection procedure. A typical case, where this would be justified is the following: Assume we publish salaries of seasonal workers from foreign countries ($S_{\text{foreigners}}$), and also salaries of all seasonal workers ($S_{\text{S-W}}$), by region, for instance. Normally, for table protection we should consider the relation

$$
S_{\text{S-W}} = S_{\text{foreigners}} + S_{\text{non-foreigners}} \ , \qquad \text{(R1)}
$$

even if we are not interested in publishing $S_{\text{non-foreigners}}$ data, and may not even have collected them. Typically $S_{\text{S-W}}$ data would also appear in other relations, for instance together with data for permanent workers as

$$
S_{\text{all}}= S_{\text{S-W}} + S_{\text{permanent}}  \ , \qquad \text{(R2)}
$$

so we would have to protect a set of linked tables. From a practical point of view this is of course not desirable. We assume now that $S_{\text{non-foreigners}}$ figures are for all regions much larger as $S_{\text{non-foreigners}}$ figures and that the $S_{\text{non-foreigners}}$ are not published. Then the $S_{\text{S-W}}$ figures cannot be used to estimate a sensitive $S_{\text{foreigners}}$ cell value. It is thus not necessary to coordinate the suppression pattern for $S_{\text{S-W}}$ and $S_{\text{foreigners}}$. This means that it is not necessary to carry out tabular data protection for the (R1) by Region tabulation. It is enough to protect two separate tables: $S_{\text{S-W}}$ or (R2) by Region, and $S_{\text{foreigners}}$ by Region.

This has been an instance of how to avoid a linked-tables structure. Those structures can, on the other hand, be very useful to make tabular data protection problems tractable in practice.

\
***Splitting detailed tables***\
Assume we have collected data on labour costs, including information on NACE category, Region, and Size class of the number of employees. We may want to release a 3-dimensional tabulation of labour costs by NACE, Region and Size Class. Looking at this table however, we may realize that while for a certain section (say: NACE$_1$) of the economy this amount of detail may be adequate, for the others (NACE$_2$, NACE$_3$, $\ldots$, NACE$_n$) it is not: In those sections many of the lower-level cells are either empty, or sensitive. As pointed out at the end of the section on information loss of @sec-evaluation-of-secondary-cell-suppression-algorithms-offered-by-tau-argus, more detail in a tabulation generally leads to more suppressions on the higher levels. This kind of oversuppression can be avoided by taking a splitting approach, splitting the table into a set of linked tables: Instead of one table, we protect three: 1.) Labour costs by Region, Size Class and NACE$_1$, 2.) Labour costs by Region and NACE, and 3.) Labour costs by size class and NACE. Even though we cannot then release any labour cost data for a particular NACE category, in a particular Region and in a particular Size Class unless the NACE category belongs to NACE$_1$, the additional unsuppressed information gained by means of the splitting approach (as compared to the simple approach dealing with a single 3-dimensional table only) is usually rated much higher.

\
***Use of hierarchical structures***\
Especially when working with the Modular method for secondary cell suppression\index{cell suppression}, using hierarchically structured variables usually helps a lot to reduce computational complexity, and hence computing time. Smaller numbers of categories per equation will make the program run faster. For the hypercube method, this is less of an issue. Here it is generally good to avoid structures with very few categories within a relation. The method has been developed for large tables, where the number of categories per relation is usually less than 20.

\
***How to process linked tables***\
We explain now a way how to process a set of linked tables, for instance $T_1$, $T_2$, $T_3$ using Modular for secondary cell suppression\index{cell suppression}. This process is normally an iterative process. The method described here is referred to as "*traditional method*" in de Wolf, Giessing (2008). De Wolf, Giessing (2008) suggest an extension of the $\tau$‑ARGUS Modular (referred to as "*adapted Modular*") method as a much more practical approach to deal with linked tables. Once this extension is available the method described in the following should be used only in cases where the adapted Modular method cannot be applied to the full set of linked tables (*e.g.* if the tables involve too many dimensions).

The question is then, which protection levels (cf. @sec-secondary-protection-methods) to assign to those 'manually unsafe cells' (in the $\tau$‑ARGUS terminology). A proper method would consist of using a CTA method (c.f. @sec-secondary-protection-methods) to find the closest adjusted table $T_1^*$ to $T_1$ that is consistent with the suppression pattern $S_{11}$ (that is, all cell values of $T_1^*$ cells that are not in $S_{11}$ must be identical to the original $T_1$ cell values), and the constraints imposed by the protection levels for the sensitive cells. We could then compute protection levels for $S_{11}$ cells on basis of the distances between adjusted and original cell value. In practice a much simpler approach is usually taken, by assigning a fixed percentage (the 'manual safety range' $\tau$-ARGUS terminology) of the cell value as protection level. There is, however, no way to tell which percentage would be suitable: when a small cell is a secondary suppression for a large, strongly sensitive cell, a protection level even of 100% may be required. On the other hand, when a large cell is a secondary suppression for a very small sensitive cell, even 1% may cause overprotection.

After processing $T_2$ in this fashion, $T_3$ is processed. In addition to the $S_{113}$ cells, here we also have to consider as additional suppressions $S_{213}$ cells, *i.e.* the subset of secondary suppressions selected for Table $T_2$ during the first iteration which are identical to a cell in $T_3$.

In this way, when processing table $T_i,\ (i = 1,2,3)$ in iteration step $k_0$, we consider as additional suppressions $\bigcup\limits_{k < k_{0}}{\left( S_{1\text{ki}} \cup S_{2\text{ki}} \cup S_{3\text{ki}} \right)} \cup \bigcup\limits_{j < i}{S_{\text{jk}_{0}i}}$, while we may try to avoid (by weighting, or by setting cell status to 'protected') that those cells that the tables have in common, *i.e.* the cells of $T_{i_{j}} \quad (j \neq i)$, and that are not already contained in the set of additional suppressions are selected as secondary suppressions. Note that we assign 0% protection levels to cells in $S_{iki}$, the set of secondary suppression which have been assigned to the current table in a previous step $k$ of the iteration. The process is continued until for each of the tables the set of additional suppressions that has to be considered when processing the table in this step is identical to the set considered in the previous step.

The necessary modifications of cell properties like cell status, or cell costs can be achieved relatively easily using the a priori file facility of $\tau$-ARGUS. The general concept of this file is outlined in the following.

\
***A-priori file***\
Through the standard sensitivity rules and other options we can apply all kinds of SDC-methods. Nevertheless there are situations which still require 'manual' intervention. *E.g.* for some other outside reason a cell must be suppressed or contrarily must be excluded from secondary suppression. Examples are the exports of certain sensitive goods (military equipment) to certain countries. These cells must be suppressed. Sometimes a cell value is known already to outsiders (maybe by other publications etc.) and therefore cannot be selected as a secondary suppression.

Also the preferences for the selection of the secondary suppressions via the cost function could be used as an option to influence the secondary suppression pattern. This might be the result of the year-to-year periodical data issue (see below).

Specifying all this information manually during a $\tau$‑ARGUS run for a large table can be rather cumbersome and error-prone. In the batch version of $\tau$‑ARGUS it is even impossible to do this. So the a-priori file has been introduced.

In this file the user can specify for each cell (by giving the values of the spanning variables):

-   cells that cannot be selected as secondary suppressions

-   cells that must be suppressed

-   specify a new value of the cost function for a cell; this can be either high or low. This will influence the chance of a cell as a candidate for sec. suppression.\
    Note that this option is not valid for the hypercube, as the hypercube has its own cost function that cannot be influenced.

Further details of this file can be found in the $\tau$‑ARGUS manual.

\
***Periodical data***\
If tables are provided regularly (say monthly, quarterly or annually) certain problems may be connected to disclosure control: Firstly, it may seem to be too much effort. Secondly, if we do disclosure analysis independently each time, suppression patterns may differ. We will have the situation that we publish a particular cell value for one period, but use it as secondary suppression in the next period. Also with a cell published in one period, but becoming a primary cell in the second period, there is a risk of a very narrow estimate based on the previous period. Especially if the observations tend to be rather constant over time the pre-period value might be a close estimate of the suppressed value for the current period, which then might be used to disclose a current-period sensitive cell.

An alternative might be simply to copy the suppression pattern of the previous period, and add some (primary and secondary -- if required) suppressions for those cases where cells were not sensitive in the last period, but are sensitive in the current period. Another option would be to assign low costs to previous-period suppressions when assigning secondary suppressions to the current data tables. However, both these strategies will cause an increase in information loss, which will be stronger the more changes there are in cell sensitivity between periods.

\
***Tables with negative values***\
In @sec-sensitive-cells-magnitude-tables we already discussed the problem of negative contributions, presenting an idea on how to solve the problem with respect to primary protection. However, when there are negative contributions, it may also happen that some of the table cell values turn out to be negative. The current version of $\tau$-ARGUS does not yet support the processing of such a table, but a little pre-processing may solve the problem:

If the microdata is available, one should simply add another variable to the dataset presenting the absolute values for the variable of interest. Cell suppression should then be based on this (positive) variable. As explained in @sec-sensitive-cells-magnitude-tables, we recommend use of a minimum frequency rule, rather than a concentration rule in such a case. Hence, taking absolute values would not affect the results of primary suppression. As explained in @sec-sensitive-cells-magnitude-tables, we recommend using protection levels of zero in the case of a minimum frequency rule. So the fact that some cell values will be larger for the shadow variable than for the original variable will have no effect on the feasibility checks within the process of secondary cell suppression\index{cell suppression}. It may still have an effect on the selection of secondary suppressions, if we use this shadow variable also as cost variable, because in that way we assign too large costs to cells with many and/or large negative contributions. However, there are options to modify cell costs for individual cells in $\tau$‑ARGUS.

In the more difficult case that the microdata is not available, we could make a new table file, selecting only bottom-level cells. In this new file, we could replace the original cell values by their absolutes, and then use $\tau$‑ARGUS facilities to add up the table again.

### Introductive Example {#sec-introductive-example} 

The German census of horticulture is a decennial survey. The main publication of 2005 data involves about 70 response variables grouped by geography and size class of horticultural area. Many of the response variables are frequencies which were not considered as sensitive. Magnitude variables are horticultural or agricultural area, classified by type of establishment, or by production type, etc., and number of labourers classified in similar ways.

Because the publication presents data on low geographic detail, the experts from agriculture statistics thought it would not be adequate to base disclosure analysis simply on a few lead variables, like horticultural area and copy the resulting suppression pattern to other variables: The argument was that, even if there are many units falling into a particular size class/geography cell, and this cell is not dominated by any large units, it might still happen, for instance, that only a single unit falls into a particular subgroup of, say, the type of production, and this unit could be identifiable by this property (given the size class and geography information). In such a case the subgroup cell should be suppressed to avoid that sensitive information would be published about this unit. Consequently, disclosure risk\index{disclosure risk} assessment had to be carried out for each of the variables separately.

The main approach taken to simplify the original extremely complex linked tables structure resulting from linear relationship between the response variables was to ignore response variable relations like we discussed it in 4.3.3: A typical instance is fruit production. Fruit can be produced by farm companies focussing on fruit production (which was one of the categories for 'type of production'), but also by other companies. The publication presents the overall fruit production area, and also fruit production area of farms focussing on fruit production. In order to avoid disclosure-by-differencing problems, in principal we should have processed fruit production area by both categories (*e.g.* 'farms focussing on fruit production' and 'other farms'). However, as the 'other farms' category corresponds to a much larger group, it seemed to be justified not to include this variable into the disclosure control process, treating 'fruit production area (by size class and geography)' and 'fruit production area of farms focussing on fruit production (by size class and geography)' as separate 2-dimensional tables.

In this way, the disclosure control procedure finally involved 23 tables. 16 were 2-dimensional, the others were 3-dimensional. Most of the tables could be processed independently, but unfortunately, 3 of the 3‑dimensional tables were linked.

The explanatory variables were Geography (at 358 categories and 4 levels, down to the department level), size class (at 9 categories), and various 'type of production' classifications, the largest involving 10 categories with hierarchical substructure.

Data for the tabulations was delivered by the German state statistical offices on an aggregate level. Some SAS procedures were developed to organize data import from those 16 files, *e.g.* to build suitable $\tau$‑ARGUS table format files, and also read the results of disclosure processing (*e.g.* cell status: safe/unsafe) back into the 16 files. Each line of that table format file (corresponding to a particular table cell) contained the respective codes for the spanning variables, the cell value, the cell frequency, the value of the largest and second-largest contributor, and a cell status (safe/unsafe). A special $\tau$‑ARGUS facility was used to build federal-level aggregates from the 16 corresponding state-level cells.

For processing the tables we used $\tau$‑ARGUS. The $p\%$-rule was employed for primary protection. For secondary cell suppression\index{cell suppression} we ran the Modular method of $\tau$‑ARGUS. For the 20 tables that could be protected independently from each other, these were straightforward applications of the software, which all finished successfully after at most a few minutes.

The only problem was the processing of the 3 linked tables. We used some little SAS programs to organise the import of secondary suppressions selected in one table into the others in the fashion described in @sec-efficient-protection-process, making suitable a-priori files for the next $\tau$-ARGUS run. However, it turned out that this procedure did not work well for that instance: the tables were extremely detailed with few non-zero, non-sensitive cells on the lower levels. As a result, quite a lot of secondary suppressions were assigned, also on the higher levels of the table, in particular those parts that were also contained in one of the other 2 tables. The number of suppressions in the overlap sections did not decrease substantially from one iteration step to the next.

After it was explained to the experts from agricultural statistics that the tables were probably too detailed, it became clear that they had not really been interested in publishing data for those tables on the district level. What they were actually interested in for those 3 tables were 3-dimensional tabulations by size class and geography, down to the level 'Region' (just above the district level), and 2-dimensional tabulations by geography down to the district level, but without size classes. Processing the 3 linked, 3-dimensional tables down to the level 'Region' was a straightforward application of the linked tables processing described in 4.3.3, finished after one iteration step. We then copied secondary suppressions of those three 3-dimensional tables into the corresponding three 2-dimensional district level tables and again applied the linked tables processing which also ended successfully after a single iteration.

